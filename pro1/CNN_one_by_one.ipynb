{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PAIRS = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(train_classes.shape)\n",
    "print(train_target.shape)\n",
    "print(test_input.shape)\n",
    "print(test_classes.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据集合\n",
    "tran_train_input=train_input.view([2000, 1, 14, 14])\n",
    "tran_train_classes=train_classes.view([2000])\n",
    "tran_test_input=test_input.view([2000, 1, 14, 14])\n",
    "tran_test_classes=test_classes.view([2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "class CNN_one_by_one_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_one_by_one_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(64,128,kernel_size=2)\n",
    "        self.conv2_drop=nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        #parameters\n",
    "        self.batch_size = 50\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.num_epochs = 25\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 512)))\n",
    "        x = F.dropout(x,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "        # Training Function\n",
    "\n",
    "    def trainer(self, train_input, train_target):\n",
    "        \"\"\"\n",
    "        Train the model on a training set\n",
    "        :param train_input: Training features\n",
    "        :param train_target: Training labels\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch_idx in range(0,train_input.size(0),self.batch_size):\n",
    "                output = self(train_input[batch_idx:batch_idx+self.batch_size]) \n",
    "                loss = self.criterion(output, train_target[batch_idx:batch_idx+self.batch_size])  \n",
    "                self.optimizer.zero_grad()                          #清零梯度(set gradients to zero)\n",
    "                loss.backward()                                #反向求梯度(backpropagate)\n",
    "                self.optimizer.step()\n",
    "#                 每隔50组数据，输出一次loss值(Every 50 data, output loss once)\n",
    "                if not batch_idx % 50:\n",
    "                    print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.6f' \n",
    "                           %(epoch+1, self.num_epochs, batch_idx, \n",
    "                             len(train_input), loss))\n",
    "            print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "        \n",
    "\n",
    "        # Test error\n",
    "\n",
    "    def compute_error(self, input_data, target):\n",
    "        \"\"\"\n",
    "        Compute the number of error of the model on a test set\n",
    "        :param input_data: test features\n",
    "        :param target: test target\n",
    "        :return: error rate of the input data\n",
    "        \"\"\"  \n",
    "    \n",
    "        #test mode\n",
    "        self.eval()     \n",
    "        errors = 0\n",
    "        for idx in range(0,input_data.size(0),self.batch_size):\n",
    "            input_batch=input_data.narrow(0,idx,self.batch_size)\n",
    "            outputs = self(input_batch)\n",
    "            _, predicted = torch.max(outputs, 1)   #返回值和索引\n",
    "            target_labels = target.narrow(0, idx, self.batch_size)\n",
    "            errors += torch.sum(predicted != target_labels)\n",
    "\n",
    "        return float(errors)*100/input_data.size(0)\n",
    "    \n",
    "    def compare_two_digit(self, input_data, comp_targets):\n",
    "        \n",
    "        #test mode\n",
    "        self.eval() \n",
    "        errors = 0\n",
    "        for pairs,comp_target in zip(input_data, comp_targets):\n",
    "            input_num1=pairs[0].view([1,1,14,14])\n",
    "            input_num2=pairs[1].view([1,1,14,14])\n",
    "            output_1 = self(input_num1)\n",
    "            output_2 = self(input_num2)\n",
    "            _, predicted_1 = torch.max(output_1, 1)   #return value and key\n",
    "            _, predicted_2 = torch.max(output_2, 1)   #return value and key\n",
    "            if(predicted_2-predicted_1>0):\n",
    "                result=1\n",
    "            else:\n",
    "                result=0\n",
    "            if(comp_target!=result):\n",
    "                errors=errors+1\n",
    "        return float(errors)*100/input_data.size(0)\n",
    "        \n",
    "    def save_model(self,model_name):\n",
    "        \"\"\"\n",
    "        Save the model to a direction\n",
    "        :param model_name: the model name, e.g. CNN_Net.pth\n",
    "        \"\"\"         \n",
    "        torch.save(self, './model/'+ model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = CNN_one_by_one_Net()\n",
    "my_model.save_model('CNN_one_by_one.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/025 | Batch 000/2000 | Loss: 18.023806\n",
      "Epoch: 001/025 | Batch 050/2000 | Loss: 16.137241\n",
      "Epoch: 001/025 | Batch 100/2000 | Loss: 11.463906\n",
      "Epoch: 001/025 | Batch 150/2000 | Loss: 7.641965\n",
      "Epoch: 001/025 | Batch 200/2000 | Loss: 4.693309\n",
      "Epoch: 001/025 | Batch 250/2000 | Loss: 4.641695\n",
      "Epoch: 001/025 | Batch 300/2000 | Loss: 3.356478\n",
      "Epoch: 001/025 | Batch 350/2000 | Loss: 2.508665\n",
      "Epoch: 001/025 | Batch 400/2000 | Loss: 2.326320\n",
      "Epoch: 001/025 | Batch 450/2000 | Loss: 2.258588\n",
      "Epoch: 001/025 | Batch 500/2000 | Loss: 2.132219\n",
      "Epoch: 001/025 | Batch 550/2000 | Loss: 2.101699\n",
      "Epoch: 001/025 | Batch 600/2000 | Loss: 2.113091\n",
      "Epoch: 001/025 | Batch 650/2000 | Loss: 2.017087\n",
      "Epoch: 001/025 | Batch 700/2000 | Loss: 2.195000\n",
      "Epoch: 001/025 | Batch 750/2000 | Loss: 1.910613\n",
      "Epoch: 001/025 | Batch 800/2000 | Loss: 2.039286\n",
      "Epoch: 001/025 | Batch 850/2000 | Loss: 2.151676\n",
      "Epoch: 001/025 | Batch 900/2000 | Loss: 2.003508\n",
      "Epoch: 001/025 | Batch 950/2000 | Loss: 1.953738\n",
      "Epoch: 001/025 | Batch 1000/2000 | Loss: 2.016550\n",
      "Epoch: 001/025 | Batch 1050/2000 | Loss: 2.075900\n",
      "Epoch: 001/025 | Batch 1100/2000 | Loss: 1.918875\n",
      "Epoch: 001/025 | Batch 1150/2000 | Loss: 1.889060\n",
      "Epoch: 001/025 | Batch 1200/2000 | Loss: 1.874314\n",
      "Epoch: 001/025 | Batch 1250/2000 | Loss: 2.070337\n",
      "Epoch: 001/025 | Batch 1300/2000 | Loss: 1.844888\n",
      "Epoch: 001/025 | Batch 1350/2000 | Loss: 1.871165\n",
      "Epoch: 001/025 | Batch 1400/2000 | Loss: 1.668368\n",
      "Epoch: 001/025 | Batch 1450/2000 | Loss: 1.831710\n",
      "Epoch: 001/025 | Batch 1500/2000 | Loss: 1.818158\n",
      "Epoch: 001/025 | Batch 1550/2000 | Loss: 2.010628\n",
      "Epoch: 001/025 | Batch 1600/2000 | Loss: 1.649341\n",
      "Epoch: 001/025 | Batch 1650/2000 | Loss: 1.470923\n",
      "Epoch: 001/025 | Batch 1700/2000 | Loss: 1.702585\n",
      "Epoch: 001/025 | Batch 1750/2000 | Loss: 1.754100\n",
      "Epoch: 001/025 | Batch 1800/2000 | Loss: 1.563594\n",
      "Epoch: 001/025 | Batch 1850/2000 | Loss: 1.598068\n",
      "Epoch: 001/025 | Batch 1900/2000 | Loss: 1.545708\n",
      "Epoch: 001/025 | Batch 1950/2000 | Loss: 1.277821\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/025 | Batch 000/2000 | Loss: 1.510961\n",
      "Epoch: 002/025 | Batch 050/2000 | Loss: 1.461629\n",
      "Epoch: 002/025 | Batch 100/2000 | Loss: 1.521203\n",
      "Epoch: 002/025 | Batch 150/2000 | Loss: 1.620591\n",
      "Epoch: 002/025 | Batch 200/2000 | Loss: 1.261038\n",
      "Epoch: 002/025 | Batch 250/2000 | Loss: 1.248766\n",
      "Epoch: 002/025 | Batch 300/2000 | Loss: 1.374917\n",
      "Epoch: 002/025 | Batch 350/2000 | Loss: 1.038344\n",
      "Epoch: 002/025 | Batch 400/2000 | Loss: 1.461968\n",
      "Epoch: 002/025 | Batch 450/2000 | Loss: 1.269679\n",
      "Epoch: 002/025 | Batch 500/2000 | Loss: 1.291743\n",
      "Epoch: 002/025 | Batch 550/2000 | Loss: 1.230396\n",
      "Epoch: 002/025 | Batch 600/2000 | Loss: 1.008371\n",
      "Epoch: 002/025 | Batch 650/2000 | Loss: 1.307616\n",
      "Epoch: 002/025 | Batch 700/2000 | Loss: 1.308241\n",
      "Epoch: 002/025 | Batch 750/2000 | Loss: 0.879062\n",
      "Epoch: 002/025 | Batch 800/2000 | Loss: 1.476506\n",
      "Epoch: 002/025 | Batch 850/2000 | Loss: 1.503829\n",
      "Epoch: 002/025 | Batch 900/2000 | Loss: 1.288004\n",
      "Epoch: 002/025 | Batch 950/2000 | Loss: 1.370389\n",
      "Epoch: 002/025 | Batch 1000/2000 | Loss: 1.370187\n",
      "Epoch: 002/025 | Batch 1050/2000 | Loss: 1.013749\n",
      "Epoch: 002/025 | Batch 1100/2000 | Loss: 1.204971\n",
      "Epoch: 002/025 | Batch 1150/2000 | Loss: 0.896145\n",
      "Epoch: 002/025 | Batch 1200/2000 | Loss: 1.228721\n",
      "Epoch: 002/025 | Batch 1250/2000 | Loss: 1.194596\n",
      "Epoch: 002/025 | Batch 1300/2000 | Loss: 1.051584\n",
      "Epoch: 002/025 | Batch 1350/2000 | Loss: 1.206254\n",
      "Epoch: 002/025 | Batch 1400/2000 | Loss: 1.187072\n",
      "Epoch: 002/025 | Batch 1450/2000 | Loss: 1.112079\n",
      "Epoch: 002/025 | Batch 1500/2000 | Loss: 0.923778\n",
      "Epoch: 002/025 | Batch 1550/2000 | Loss: 1.115607\n",
      "Epoch: 002/025 | Batch 1600/2000 | Loss: 1.009616\n",
      "Epoch: 002/025 | Batch 1650/2000 | Loss: 0.809619\n",
      "Epoch: 002/025 | Batch 1700/2000 | Loss: 1.209798\n",
      "Epoch: 002/025 | Batch 1750/2000 | Loss: 1.304083\n",
      "Epoch: 002/025 | Batch 1800/2000 | Loss: 1.012253\n",
      "Epoch: 002/025 | Batch 1850/2000 | Loss: 0.995191\n",
      "Epoch: 002/025 | Batch 1900/2000 | Loss: 1.116684\n",
      "Epoch: 002/025 | Batch 1950/2000 | Loss: 0.853968\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 003/025 | Batch 000/2000 | Loss: 0.954266\n",
      "Epoch: 003/025 | Batch 050/2000 | Loss: 0.863964\n",
      "Epoch: 003/025 | Batch 100/2000 | Loss: 1.122331\n",
      "Epoch: 003/025 | Batch 150/2000 | Loss: 0.792514\n",
      "Epoch: 003/025 | Batch 200/2000 | Loss: 0.793481\n",
      "Epoch: 003/025 | Batch 250/2000 | Loss: 0.832782\n",
      "Epoch: 003/025 | Batch 300/2000 | Loss: 0.755780\n",
      "Epoch: 003/025 | Batch 350/2000 | Loss: 0.752567\n",
      "Epoch: 003/025 | Batch 400/2000 | Loss: 0.766417\n",
      "Epoch: 003/025 | Batch 450/2000 | Loss: 0.592964\n",
      "Epoch: 003/025 | Batch 500/2000 | Loss: 0.718664\n",
      "Epoch: 003/025 | Batch 550/2000 | Loss: 0.979458\n",
      "Epoch: 003/025 | Batch 600/2000 | Loss: 0.767587\n",
      "Epoch: 003/025 | Batch 650/2000 | Loss: 0.771153\n",
      "Epoch: 003/025 | Batch 700/2000 | Loss: 1.292898\n",
      "Epoch: 003/025 | Batch 750/2000 | Loss: 0.539719\n",
      "Epoch: 003/025 | Batch 800/2000 | Loss: 0.902202\n",
      "Epoch: 003/025 | Batch 850/2000 | Loss: 0.872871\n",
      "Epoch: 003/025 | Batch 900/2000 | Loss: 0.762220\n",
      "Epoch: 003/025 | Batch 950/2000 | Loss: 0.685444\n",
      "Epoch: 003/025 | Batch 1000/2000 | Loss: 0.825087\n",
      "Epoch: 003/025 | Batch 1050/2000 | Loss: 0.530989\n",
      "Epoch: 003/025 | Batch 1100/2000 | Loss: 0.690681\n",
      "Epoch: 003/025 | Batch 1150/2000 | Loss: 0.773338\n",
      "Epoch: 003/025 | Batch 1200/2000 | Loss: 0.859674\n",
      "Epoch: 003/025 | Batch 1250/2000 | Loss: 1.065503\n",
      "Epoch: 003/025 | Batch 1300/2000 | Loss: 0.726428\n",
      "Epoch: 003/025 | Batch 1350/2000 | Loss: 0.948793\n",
      "Epoch: 003/025 | Batch 1400/2000 | Loss: 0.767823\n",
      "Epoch: 003/025 | Batch 1450/2000 | Loss: 0.751079\n",
      "Epoch: 003/025 | Batch 1500/2000 | Loss: 0.752516\n",
      "Epoch: 003/025 | Batch 1550/2000 | Loss: 0.727682\n",
      "Epoch: 003/025 | Batch 1600/2000 | Loss: 0.883443\n",
      "Epoch: 003/025 | Batch 1650/2000 | Loss: 0.610975\n",
      "Epoch: 003/025 | Batch 1700/2000 | Loss: 0.605167\n",
      "Epoch: 003/025 | Batch 1750/2000 | Loss: 0.758305\n",
      "Epoch: 003/025 | Batch 1800/2000 | Loss: 0.916010\n",
      "Epoch: 003/025 | Batch 1850/2000 | Loss: 0.797921\n",
      "Epoch: 003/025 | Batch 1900/2000 | Loss: 0.894928\n",
      "Epoch: 003/025 | Batch 1950/2000 | Loss: 0.577593\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 004/025 | Batch 000/2000 | Loss: 0.742905\n",
      "Epoch: 004/025 | Batch 050/2000 | Loss: 0.584634\n",
      "Epoch: 004/025 | Batch 100/2000 | Loss: 0.828960\n",
      "Epoch: 004/025 | Batch 150/2000 | Loss: 0.889003\n",
      "Epoch: 004/025 | Batch 200/2000 | Loss: 0.604402\n",
      "Epoch: 004/025 | Batch 250/2000 | Loss: 0.731056\n",
      "Epoch: 004/025 | Batch 300/2000 | Loss: 0.593494\n",
      "Epoch: 004/025 | Batch 350/2000 | Loss: 0.723591\n",
      "Epoch: 004/025 | Batch 400/2000 | Loss: 0.924307\n",
      "Epoch: 004/025 | Batch 450/2000 | Loss: 0.580929\n",
      "Epoch: 004/025 | Batch 500/2000 | Loss: 0.612527\n",
      "Epoch: 004/025 | Batch 550/2000 | Loss: 0.724558\n",
      "Epoch: 004/025 | Batch 600/2000 | Loss: 0.780492\n",
      "Epoch: 004/025 | Batch 650/2000 | Loss: 0.841805\n",
      "Epoch: 004/025 | Batch 700/2000 | Loss: 0.673592\n",
      "Epoch: 004/025 | Batch 750/2000 | Loss: 0.368557\n",
      "Epoch: 004/025 | Batch 800/2000 | Loss: 0.794230\n",
      "Epoch: 004/025 | Batch 850/2000 | Loss: 1.002826\n",
      "Epoch: 004/025 | Batch 900/2000 | Loss: 0.604579\n",
      "Epoch: 004/025 | Batch 950/2000 | Loss: 0.592177\n",
      "Epoch: 004/025 | Batch 1000/2000 | Loss: 0.855900\n",
      "Epoch: 004/025 | Batch 1050/2000 | Loss: 0.487732\n",
      "Epoch: 004/025 | Batch 1100/2000 | Loss: 0.783197\n",
      "Epoch: 004/025 | Batch 1150/2000 | Loss: 0.731566\n",
      "Epoch: 004/025 | Batch 1200/2000 | Loss: 0.747197\n",
      "Epoch: 004/025 | Batch 1250/2000 | Loss: 0.706099\n",
      "Epoch: 004/025 | Batch 1300/2000 | Loss: 0.670466\n",
      "Epoch: 004/025 | Batch 1350/2000 | Loss: 0.851171\n",
      "Epoch: 004/025 | Batch 1400/2000 | Loss: 0.834979\n",
      "Epoch: 004/025 | Batch 1450/2000 | Loss: 0.870323\n",
      "Epoch: 004/025 | Batch 1500/2000 | Loss: 0.722556\n",
      "Epoch: 004/025 | Batch 1550/2000 | Loss: 0.833423\n",
      "Epoch: 004/025 | Batch 1600/2000 | Loss: 0.621183\n",
      "Epoch: 004/025 | Batch 1650/2000 | Loss: 0.511629\n",
      "Epoch: 004/025 | Batch 1700/2000 | Loss: 0.579803\n",
      "Epoch: 004/025 | Batch 1750/2000 | Loss: 0.873660\n",
      "Epoch: 004/025 | Batch 1800/2000 | Loss: 0.678793\n",
      "Epoch: 004/025 | Batch 1850/2000 | Loss: 0.582170\n",
      "Epoch: 004/025 | Batch 1900/2000 | Loss: 0.782720\n",
      "Epoch: 004/025 | Batch 1950/2000 | Loss: 0.397347\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 005/025 | Batch 000/2000 | Loss: 0.668020\n",
      "Epoch: 005/025 | Batch 050/2000 | Loss: 0.659747\n",
      "Epoch: 005/025 | Batch 100/2000 | Loss: 0.713466\n",
      "Epoch: 005/025 | Batch 150/2000 | Loss: 0.503185\n",
      "Epoch: 005/025 | Batch 200/2000 | Loss: 0.524509\n",
      "Epoch: 005/025 | Batch 250/2000 | Loss: 0.517502\n",
      "Epoch: 005/025 | Batch 300/2000 | Loss: 0.653462\n",
      "Epoch: 005/025 | Batch 350/2000 | Loss: 0.609217\n",
      "Epoch: 005/025 | Batch 400/2000 | Loss: 0.624938\n",
      "Epoch: 005/025 | Batch 450/2000 | Loss: 0.421476\n",
      "Epoch: 005/025 | Batch 500/2000 | Loss: 0.592159\n",
      "Epoch: 005/025 | Batch 550/2000 | Loss: 0.622823\n",
      "Epoch: 005/025 | Batch 600/2000 | Loss: 0.600592\n",
      "Epoch: 005/025 | Batch 650/2000 | Loss: 0.477562\n",
      "Epoch: 005/025 | Batch 700/2000 | Loss: 0.739188\n",
      "Epoch: 005/025 | Batch 750/2000 | Loss: 0.514248\n",
      "Epoch: 005/025 | Batch 800/2000 | Loss: 0.410907\n",
      "Epoch: 005/025 | Batch 850/2000 | Loss: 0.648276\n",
      "Epoch: 005/025 | Batch 900/2000 | Loss: 0.404617\n",
      "Epoch: 005/025 | Batch 950/2000 | Loss: 0.539091\n",
      "Epoch: 005/025 | Batch 1000/2000 | Loss: 0.696700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005/025 | Batch 1050/2000 | Loss: 0.488937\n",
      "Epoch: 005/025 | Batch 1100/2000 | Loss: 0.524890\n",
      "Epoch: 005/025 | Batch 1150/2000 | Loss: 0.280886\n",
      "Epoch: 005/025 | Batch 1200/2000 | Loss: 0.440577\n",
      "Epoch: 005/025 | Batch 1250/2000 | Loss: 0.569734\n",
      "Epoch: 005/025 | Batch 1300/2000 | Loss: 0.522786\n",
      "Epoch: 005/025 | Batch 1350/2000 | Loss: 0.789116\n",
      "Epoch: 005/025 | Batch 1400/2000 | Loss: 0.667226\n",
      "Epoch: 005/025 | Batch 1450/2000 | Loss: 0.321491\n",
      "Epoch: 005/025 | Batch 1500/2000 | Loss: 0.499212\n",
      "Epoch: 005/025 | Batch 1550/2000 | Loss: 0.447713\n",
      "Epoch: 005/025 | Batch 1600/2000 | Loss: 0.642900\n",
      "Epoch: 005/025 | Batch 1650/2000 | Loss: 0.448431\n",
      "Epoch: 005/025 | Batch 1700/2000 | Loss: 0.565500\n",
      "Epoch: 005/025 | Batch 1750/2000 | Loss: 0.831777\n",
      "Epoch: 005/025 | Batch 1800/2000 | Loss: 0.511505\n",
      "Epoch: 005/025 | Batch 1850/2000 | Loss: 0.448998\n",
      "Epoch: 005/025 | Batch 1900/2000 | Loss: 0.658491\n",
      "Epoch: 005/025 | Batch 1950/2000 | Loss: 0.303392\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 006/025 | Batch 000/2000 | Loss: 0.393793\n",
      "Epoch: 006/025 | Batch 050/2000 | Loss: 0.465111\n",
      "Epoch: 006/025 | Batch 100/2000 | Loss: 0.599362\n",
      "Epoch: 006/025 | Batch 150/2000 | Loss: 0.510117\n",
      "Epoch: 006/025 | Batch 200/2000 | Loss: 0.514727\n",
      "Epoch: 006/025 | Batch 250/2000 | Loss: 0.287391\n",
      "Epoch: 006/025 | Batch 300/2000 | Loss: 0.423796\n",
      "Epoch: 006/025 | Batch 350/2000 | Loss: 0.406374\n",
      "Epoch: 006/025 | Batch 400/2000 | Loss: 0.486101\n",
      "Epoch: 006/025 | Batch 450/2000 | Loss: 0.332123\n",
      "Epoch: 006/025 | Batch 500/2000 | Loss: 0.316863\n",
      "Epoch: 006/025 | Batch 550/2000 | Loss: 0.653869\n",
      "Epoch: 006/025 | Batch 600/2000 | Loss: 0.458442\n",
      "Epoch: 006/025 | Batch 650/2000 | Loss: 0.450949\n",
      "Epoch: 006/025 | Batch 700/2000 | Loss: 0.346115\n",
      "Epoch: 006/025 | Batch 750/2000 | Loss: 0.333718\n",
      "Epoch: 006/025 | Batch 800/2000 | Loss: 0.677207\n",
      "Epoch: 006/025 | Batch 850/2000 | Loss: 0.383180\n",
      "Epoch: 006/025 | Batch 900/2000 | Loss: 0.330575\n",
      "Epoch: 006/025 | Batch 950/2000 | Loss: 0.421509\n",
      "Epoch: 006/025 | Batch 1000/2000 | Loss: 0.367010\n",
      "Epoch: 006/025 | Batch 1050/2000 | Loss: 0.472410\n",
      "Epoch: 006/025 | Batch 1100/2000 | Loss: 0.477871\n",
      "Epoch: 006/025 | Batch 1150/2000 | Loss: 0.290216\n",
      "Epoch: 006/025 | Batch 1200/2000 | Loss: 0.462986\n",
      "Epoch: 006/025 | Batch 1250/2000 | Loss: 0.492799\n",
      "Epoch: 006/025 | Batch 1300/2000 | Loss: 0.387830\n",
      "Epoch: 006/025 | Batch 1350/2000 | Loss: 0.677925\n",
      "Epoch: 006/025 | Batch 1400/2000 | Loss: 0.342690\n",
      "Epoch: 006/025 | Batch 1450/2000 | Loss: 0.521385\n",
      "Epoch: 006/025 | Batch 1500/2000 | Loss: 0.453620\n",
      "Epoch: 006/025 | Batch 1550/2000 | Loss: 0.504695\n",
      "Epoch: 006/025 | Batch 1600/2000 | Loss: 0.437498\n",
      "Epoch: 006/025 | Batch 1650/2000 | Loss: 0.363616\n",
      "Epoch: 006/025 | Batch 1700/2000 | Loss: 0.326297\n",
      "Epoch: 006/025 | Batch 1750/2000 | Loss: 0.516730\n",
      "Epoch: 006/025 | Batch 1800/2000 | Loss: 0.416591\n",
      "Epoch: 006/025 | Batch 1850/2000 | Loss: 0.319856\n",
      "Epoch: 006/025 | Batch 1900/2000 | Loss: 0.528940\n",
      "Epoch: 006/025 | Batch 1950/2000 | Loss: 0.266557\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 007/025 | Batch 000/2000 | Loss: 0.398616\n",
      "Epoch: 007/025 | Batch 050/2000 | Loss: 0.366989\n",
      "Epoch: 007/025 | Batch 100/2000 | Loss: 0.596857\n",
      "Epoch: 007/025 | Batch 150/2000 | Loss: 0.332459\n",
      "Epoch: 007/025 | Batch 200/2000 | Loss: 0.343343\n",
      "Epoch: 007/025 | Batch 250/2000 | Loss: 0.156114\n",
      "Epoch: 007/025 | Batch 300/2000 | Loss: 0.308680\n",
      "Epoch: 007/025 | Batch 350/2000 | Loss: 0.261832\n",
      "Epoch: 007/025 | Batch 400/2000 | Loss: 0.313859\n",
      "Epoch: 007/025 | Batch 450/2000 | Loss: 0.339801\n",
      "Epoch: 007/025 | Batch 500/2000 | Loss: 0.187539\n",
      "Epoch: 007/025 | Batch 550/2000 | Loss: 0.258927\n",
      "Epoch: 007/025 | Batch 600/2000 | Loss: 0.335773\n",
      "Epoch: 007/025 | Batch 650/2000 | Loss: 0.317625\n",
      "Epoch: 007/025 | Batch 700/2000 | Loss: 0.360897\n",
      "Epoch: 007/025 | Batch 750/2000 | Loss: 0.187457\n",
      "Epoch: 007/025 | Batch 800/2000 | Loss: 0.498805\n",
      "Epoch: 007/025 | Batch 850/2000 | Loss: 0.374853\n",
      "Epoch: 007/025 | Batch 900/2000 | Loss: 0.282525\n",
      "Epoch: 007/025 | Batch 950/2000 | Loss: 0.265752\n",
      "Epoch: 007/025 | Batch 1000/2000 | Loss: 0.495477\n",
      "Epoch: 007/025 | Batch 1050/2000 | Loss: 0.150526\n",
      "Epoch: 007/025 | Batch 1100/2000 | Loss: 0.342426\n",
      "Epoch: 007/025 | Batch 1150/2000 | Loss: 0.348845\n",
      "Epoch: 007/025 | Batch 1200/2000 | Loss: 0.347926\n",
      "Epoch: 007/025 | Batch 1250/2000 | Loss: 0.318023\n",
      "Epoch: 007/025 | Batch 1300/2000 | Loss: 0.473490\n",
      "Epoch: 007/025 | Batch 1350/2000 | Loss: 0.741545\n",
      "Epoch: 007/025 | Batch 1400/2000 | Loss: 0.278383\n",
      "Epoch: 007/025 | Batch 1450/2000 | Loss: 0.449359\n",
      "Epoch: 007/025 | Batch 1500/2000 | Loss: 0.339647\n",
      "Epoch: 007/025 | Batch 1550/2000 | Loss: 0.587750\n",
      "Epoch: 007/025 | Batch 1600/2000 | Loss: 0.359918\n",
      "Epoch: 007/025 | Batch 1650/2000 | Loss: 0.280275\n",
      "Epoch: 007/025 | Batch 1700/2000 | Loss: 0.267086\n",
      "Epoch: 007/025 | Batch 1750/2000 | Loss: 0.427462\n",
      "Epoch: 007/025 | Batch 1800/2000 | Loss: 0.439782\n",
      "Epoch: 007/025 | Batch 1850/2000 | Loss: 0.439349\n",
      "Epoch: 007/025 | Batch 1900/2000 | Loss: 0.327863\n",
      "Epoch: 007/025 | Batch 1950/2000 | Loss: 0.254853\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 008/025 | Batch 000/2000 | Loss: 0.262647\n",
      "Epoch: 008/025 | Batch 050/2000 | Loss: 0.216659\n",
      "Epoch: 008/025 | Batch 100/2000 | Loss: 0.395855\n",
      "Epoch: 008/025 | Batch 150/2000 | Loss: 0.214827\n",
      "Epoch: 008/025 | Batch 200/2000 | Loss: 0.210764\n",
      "Epoch: 008/025 | Batch 250/2000 | Loss: 0.248001\n",
      "Epoch: 008/025 | Batch 300/2000 | Loss: 0.294692\n",
      "Epoch: 008/025 | Batch 350/2000 | Loss: 0.181692\n",
      "Epoch: 008/025 | Batch 400/2000 | Loss: 0.358103\n",
      "Epoch: 008/025 | Batch 450/2000 | Loss: 0.244143\n",
      "Epoch: 008/025 | Batch 500/2000 | Loss: 0.341118\n",
      "Epoch: 008/025 | Batch 550/2000 | Loss: 0.507922\n",
      "Epoch: 008/025 | Batch 600/2000 | Loss: 0.310983\n",
      "Epoch: 008/025 | Batch 650/2000 | Loss: 0.319341\n",
      "Epoch: 008/025 | Batch 700/2000 | Loss: 0.530479\n",
      "Epoch: 008/025 | Batch 750/2000 | Loss: 0.169441\n",
      "Epoch: 008/025 | Batch 800/2000 | Loss: 0.301705\n",
      "Epoch: 008/025 | Batch 850/2000 | Loss: 0.315092\n",
      "Epoch: 008/025 | Batch 900/2000 | Loss: 0.165078\n",
      "Epoch: 008/025 | Batch 950/2000 | Loss: 0.242240\n",
      "Epoch: 008/025 | Batch 1000/2000 | Loss: 0.181461\n",
      "Epoch: 008/025 | Batch 1050/2000 | Loss: 0.227145\n",
      "Epoch: 008/025 | Batch 1100/2000 | Loss: 0.331072\n",
      "Epoch: 008/025 | Batch 1150/2000 | Loss: 0.300547\n",
      "Epoch: 008/025 | Batch 1200/2000 | Loss: 0.221443\n",
      "Epoch: 008/025 | Batch 1250/2000 | Loss: 0.388509\n",
      "Epoch: 008/025 | Batch 1300/2000 | Loss: 0.216959\n",
      "Epoch: 008/025 | Batch 1350/2000 | Loss: 0.406841\n",
      "Epoch: 008/025 | Batch 1400/2000 | Loss: 0.344685\n",
      "Epoch: 008/025 | Batch 1450/2000 | Loss: 0.484828\n",
      "Epoch: 008/025 | Batch 1500/2000 | Loss: 0.445360\n",
      "Epoch: 008/025 | Batch 1550/2000 | Loss: 0.209205\n",
      "Epoch: 008/025 | Batch 1600/2000 | Loss: 0.349592\n",
      "Epoch: 008/025 | Batch 1650/2000 | Loss: 0.187461\n",
      "Epoch: 008/025 | Batch 1700/2000 | Loss: 0.208362\n",
      "Epoch: 008/025 | Batch 1750/2000 | Loss: 0.342256\n",
      "Epoch: 008/025 | Batch 1800/2000 | Loss: 0.269306\n",
      "Epoch: 008/025 | Batch 1850/2000 | Loss: 0.307179\n",
      "Epoch: 008/025 | Batch 1900/2000 | Loss: 0.292999\n",
      "Epoch: 008/025 | Batch 1950/2000 | Loss: 0.161711\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 009/025 | Batch 000/2000 | Loss: 0.518979\n",
      "Epoch: 009/025 | Batch 050/2000 | Loss: 0.252831\n",
      "Epoch: 009/025 | Batch 100/2000 | Loss: 0.349300\n",
      "Epoch: 009/025 | Batch 150/2000 | Loss: 0.531969\n",
      "Epoch: 009/025 | Batch 200/2000 | Loss: 0.297603\n",
      "Epoch: 009/025 | Batch 250/2000 | Loss: 0.162662\n",
      "Epoch: 009/025 | Batch 300/2000 | Loss: 0.358003\n",
      "Epoch: 009/025 | Batch 350/2000 | Loss: 0.300826\n",
      "Epoch: 009/025 | Batch 400/2000 | Loss: 0.287174\n",
      "Epoch: 009/025 | Batch 450/2000 | Loss: 0.208995\n",
      "Epoch: 009/025 | Batch 500/2000 | Loss: 0.246113\n",
      "Epoch: 009/025 | Batch 550/2000 | Loss: 0.262893\n",
      "Epoch: 009/025 | Batch 600/2000 | Loss: 0.262139\n",
      "Epoch: 009/025 | Batch 650/2000 | Loss: 0.388033\n",
      "Epoch: 009/025 | Batch 700/2000 | Loss: 0.557086\n",
      "Epoch: 009/025 | Batch 750/2000 | Loss: 0.135954\n",
      "Epoch: 009/025 | Batch 800/2000 | Loss: 0.468338\n",
      "Epoch: 009/025 | Batch 850/2000 | Loss: 0.448582\n",
      "Epoch: 009/025 | Batch 900/2000 | Loss: 0.147827\n",
      "Epoch: 009/025 | Batch 950/2000 | Loss: 0.275317\n",
      "Epoch: 009/025 | Batch 1000/2000 | Loss: 0.187386\n",
      "Epoch: 009/025 | Batch 1050/2000 | Loss: 0.220057\n",
      "Epoch: 009/025 | Batch 1100/2000 | Loss: 0.174055\n",
      "Epoch: 009/025 | Batch 1150/2000 | Loss: 0.223932\n",
      "Epoch: 009/025 | Batch 1200/2000 | Loss: 0.273903\n",
      "Epoch: 009/025 | Batch 1250/2000 | Loss: 0.326976\n",
      "Epoch: 009/025 | Batch 1300/2000 | Loss: 0.214726\n",
      "Epoch: 009/025 | Batch 1350/2000 | Loss: 0.371573\n",
      "Epoch: 009/025 | Batch 1400/2000 | Loss: 0.209569\n",
      "Epoch: 009/025 | Batch 1450/2000 | Loss: 0.252630\n",
      "Epoch: 009/025 | Batch 1500/2000 | Loss: 0.263688\n",
      "Epoch: 009/025 | Batch 1550/2000 | Loss: 0.302454\n",
      "Epoch: 009/025 | Batch 1600/2000 | Loss: 0.433358\n",
      "Epoch: 009/025 | Batch 1650/2000 | Loss: 0.177443\n",
      "Epoch: 009/025 | Batch 1700/2000 | Loss: 0.155922\n",
      "Epoch: 009/025 | Batch 1750/2000 | Loss: 0.524407\n",
      "Epoch: 009/025 | Batch 1800/2000 | Loss: 0.386688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009/025 | Batch 1850/2000 | Loss: 0.140820\n",
      "Epoch: 009/025 | Batch 1900/2000 | Loss: 0.236900\n",
      "Epoch: 009/025 | Batch 1950/2000 | Loss: 0.210475\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 010/025 | Batch 000/2000 | Loss: 0.282119\n",
      "Epoch: 010/025 | Batch 050/2000 | Loss: 0.283419\n",
      "Epoch: 010/025 | Batch 100/2000 | Loss: 0.420123\n",
      "Epoch: 010/025 | Batch 150/2000 | Loss: 0.179703\n",
      "Epoch: 010/025 | Batch 200/2000 | Loss: 0.199899\n",
      "Epoch: 010/025 | Batch 250/2000 | Loss: 0.076157\n",
      "Epoch: 010/025 | Batch 300/2000 | Loss: 0.266939\n",
      "Epoch: 010/025 | Batch 350/2000 | Loss: 0.244945\n",
      "Epoch: 010/025 | Batch 400/2000 | Loss: 0.315995\n",
      "Epoch: 010/025 | Batch 450/2000 | Loss: 0.319256\n",
      "Epoch: 010/025 | Batch 500/2000 | Loss: 0.113902\n",
      "Epoch: 010/025 | Batch 550/2000 | Loss: 0.364612\n",
      "Epoch: 010/025 | Batch 600/2000 | Loss: 0.353613\n",
      "Epoch: 010/025 | Batch 650/2000 | Loss: 0.381521\n",
      "Epoch: 010/025 | Batch 700/2000 | Loss: 0.376264\n",
      "Epoch: 010/025 | Batch 750/2000 | Loss: 0.052346\n",
      "Epoch: 010/025 | Batch 800/2000 | Loss: 0.431946\n",
      "Epoch: 010/025 | Batch 850/2000 | Loss: 0.338715\n",
      "Epoch: 010/025 | Batch 900/2000 | Loss: 0.262644\n",
      "Epoch: 010/025 | Batch 950/2000 | Loss: 0.177879\n",
      "Epoch: 010/025 | Batch 1000/2000 | Loss: 0.303224\n",
      "Epoch: 010/025 | Batch 1050/2000 | Loss: 0.162908\n",
      "Epoch: 010/025 | Batch 1100/2000 | Loss: 0.263407\n",
      "Epoch: 010/025 | Batch 1150/2000 | Loss: 0.270935\n",
      "Epoch: 010/025 | Batch 1200/2000 | Loss: 0.315274\n",
      "Epoch: 010/025 | Batch 1250/2000 | Loss: 0.188037\n",
      "Epoch: 010/025 | Batch 1300/2000 | Loss: 0.164533\n",
      "Epoch: 010/025 | Batch 1350/2000 | Loss: 0.343722\n",
      "Epoch: 010/025 | Batch 1400/2000 | Loss: 0.322228\n",
      "Epoch: 010/025 | Batch 1450/2000 | Loss: 0.196548\n",
      "Epoch: 010/025 | Batch 1500/2000 | Loss: 0.241534\n",
      "Epoch: 010/025 | Batch 1550/2000 | Loss: 0.320146\n",
      "Epoch: 010/025 | Batch 1600/2000 | Loss: 0.202616\n",
      "Epoch: 010/025 | Batch 1650/2000 | Loss: 0.155609\n",
      "Epoch: 010/025 | Batch 1700/2000 | Loss: 0.180796\n",
      "Epoch: 010/025 | Batch 1750/2000 | Loss: 0.291642\n",
      "Epoch: 010/025 | Batch 1800/2000 | Loss: 0.329595\n",
      "Epoch: 010/025 | Batch 1850/2000 | Loss: 0.221075\n",
      "Epoch: 010/025 | Batch 1900/2000 | Loss: 0.166164\n",
      "Epoch: 010/025 | Batch 1950/2000 | Loss: 0.099531\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 011/025 | Batch 000/2000 | Loss: 0.127394\n",
      "Epoch: 011/025 | Batch 050/2000 | Loss: 0.312608\n",
      "Epoch: 011/025 | Batch 100/2000 | Loss: 0.220722\n",
      "Epoch: 011/025 | Batch 150/2000 | Loss: 0.147932\n",
      "Epoch: 011/025 | Batch 200/2000 | Loss: 0.176511\n",
      "Epoch: 011/025 | Batch 250/2000 | Loss: 0.157121\n",
      "Epoch: 011/025 | Batch 300/2000 | Loss: 0.369210\n",
      "Epoch: 011/025 | Batch 350/2000 | Loss: 0.129261\n",
      "Epoch: 011/025 | Batch 400/2000 | Loss: 0.202135\n",
      "Epoch: 011/025 | Batch 450/2000 | Loss: 0.262637\n",
      "Epoch: 011/025 | Batch 500/2000 | Loss: 0.153634\n",
      "Epoch: 011/025 | Batch 550/2000 | Loss: 0.201970\n",
      "Epoch: 011/025 | Batch 600/2000 | Loss: 0.163080\n",
      "Epoch: 011/025 | Batch 650/2000 | Loss: 0.168754\n",
      "Epoch: 011/025 | Batch 700/2000 | Loss: 0.288631\n",
      "Epoch: 011/025 | Batch 750/2000 | Loss: 0.119490\n",
      "Epoch: 011/025 | Batch 800/2000 | Loss: 0.365552\n",
      "Epoch: 011/025 | Batch 850/2000 | Loss: 0.507257\n",
      "Epoch: 011/025 | Batch 900/2000 | Loss: 0.144938\n",
      "Epoch: 011/025 | Batch 950/2000 | Loss: 0.185992\n",
      "Epoch: 011/025 | Batch 1000/2000 | Loss: 0.252745\n",
      "Epoch: 011/025 | Batch 1050/2000 | Loss: 0.095438\n",
      "Epoch: 011/025 | Batch 1100/2000 | Loss: 0.143168\n",
      "Epoch: 011/025 | Batch 1150/2000 | Loss: 0.223502\n",
      "Epoch: 011/025 | Batch 1200/2000 | Loss: 0.200732\n",
      "Epoch: 011/025 | Batch 1250/2000 | Loss: 0.115536\n",
      "Epoch: 011/025 | Batch 1300/2000 | Loss: 0.193538\n",
      "Epoch: 011/025 | Batch 1350/2000 | Loss: 0.207750\n",
      "Epoch: 011/025 | Batch 1400/2000 | Loss: 0.201052\n",
      "Epoch: 011/025 | Batch 1450/2000 | Loss: 0.156590\n",
      "Epoch: 011/025 | Batch 1500/2000 | Loss: 0.156300\n",
      "Epoch: 011/025 | Batch 1550/2000 | Loss: 0.214044\n",
      "Epoch: 011/025 | Batch 1600/2000 | Loss: 0.233975\n",
      "Epoch: 011/025 | Batch 1650/2000 | Loss: 0.113604\n",
      "Epoch: 011/025 | Batch 1700/2000 | Loss: 0.087663\n",
      "Epoch: 011/025 | Batch 1750/2000 | Loss: 0.433368\n",
      "Epoch: 011/025 | Batch 1800/2000 | Loss: 0.226684\n",
      "Epoch: 011/025 | Batch 1850/2000 | Loss: 0.210426\n",
      "Epoch: 011/025 | Batch 1900/2000 | Loss: 0.128031\n",
      "Epoch: 011/025 | Batch 1950/2000 | Loss: 0.052940\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 012/025 | Batch 000/2000 | Loss: 0.173019\n",
      "Epoch: 012/025 | Batch 050/2000 | Loss: 0.270587\n",
      "Epoch: 012/025 | Batch 100/2000 | Loss: 0.207057\n",
      "Epoch: 012/025 | Batch 150/2000 | Loss: 0.189401\n",
      "Epoch: 012/025 | Batch 200/2000 | Loss: 0.194131\n",
      "Epoch: 012/025 | Batch 250/2000 | Loss: 0.078701\n",
      "Epoch: 012/025 | Batch 300/2000 | Loss: 0.135380\n",
      "Epoch: 012/025 | Batch 350/2000 | Loss: 0.095790\n",
      "Epoch: 012/025 | Batch 400/2000 | Loss: 0.282887\n",
      "Epoch: 012/025 | Batch 450/2000 | Loss: 0.089144\n",
      "Epoch: 012/025 | Batch 500/2000 | Loss: 0.130993\n",
      "Epoch: 012/025 | Batch 550/2000 | Loss: 0.247115\n",
      "Epoch: 012/025 | Batch 600/2000 | Loss: 0.206286\n",
      "Epoch: 012/025 | Batch 650/2000 | Loss: 0.202715\n",
      "Epoch: 012/025 | Batch 700/2000 | Loss: 0.368788\n",
      "Epoch: 012/025 | Batch 750/2000 | Loss: 0.056035\n",
      "Epoch: 012/025 | Batch 800/2000 | Loss: 0.205622\n",
      "Epoch: 012/025 | Batch 850/2000 | Loss: 0.214983\n",
      "Epoch: 012/025 | Batch 900/2000 | Loss: 0.109191\n",
      "Epoch: 012/025 | Batch 950/2000 | Loss: 0.098488\n",
      "Epoch: 012/025 | Batch 1000/2000 | Loss: 0.205563\n",
      "Epoch: 012/025 | Batch 1050/2000 | Loss: 0.164778\n",
      "Epoch: 012/025 | Batch 1100/2000 | Loss: 0.211462\n",
      "Epoch: 012/025 | Batch 1150/2000 | Loss: 0.135185\n",
      "Epoch: 012/025 | Batch 1200/2000 | Loss: 0.188109\n",
      "Epoch: 012/025 | Batch 1250/2000 | Loss: 0.132191\n",
      "Epoch: 012/025 | Batch 1300/2000 | Loss: 0.156358\n",
      "Epoch: 012/025 | Batch 1350/2000 | Loss: 0.178124\n",
      "Epoch: 012/025 | Batch 1400/2000 | Loss: 0.156323\n",
      "Epoch: 012/025 | Batch 1450/2000 | Loss: 0.171148\n",
      "Epoch: 012/025 | Batch 1500/2000 | Loss: 0.052656\n",
      "Epoch: 012/025 | Batch 1550/2000 | Loss: 0.206821\n",
      "Epoch: 012/025 | Batch 1600/2000 | Loss: 0.166766\n",
      "Epoch: 012/025 | Batch 1650/2000 | Loss: 0.067471\n",
      "Epoch: 012/025 | Batch 1700/2000 | Loss: 0.256768\n",
      "Epoch: 012/025 | Batch 1750/2000 | Loss: 0.182341\n",
      "Epoch: 012/025 | Batch 1800/2000 | Loss: 0.150608\n",
      "Epoch: 012/025 | Batch 1850/2000 | Loss: 0.151537\n",
      "Epoch: 012/025 | Batch 1900/2000 | Loss: 0.252611\n",
      "Epoch: 012/025 | Batch 1950/2000 | Loss: 0.065159\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 013/025 | Batch 000/2000 | Loss: 0.154989\n",
      "Epoch: 013/025 | Batch 050/2000 | Loss: 0.183164\n",
      "Epoch: 013/025 | Batch 100/2000 | Loss: 0.165624\n",
      "Epoch: 013/025 | Batch 150/2000 | Loss: 0.075192\n",
      "Epoch: 013/025 | Batch 200/2000 | Loss: 0.097177\n",
      "Epoch: 013/025 | Batch 250/2000 | Loss: 0.233703\n",
      "Epoch: 013/025 | Batch 300/2000 | Loss: 0.113639\n",
      "Epoch: 013/025 | Batch 350/2000 | Loss: 0.175692\n",
      "Epoch: 013/025 | Batch 400/2000 | Loss: 0.049015\n",
      "Epoch: 013/025 | Batch 450/2000 | Loss: 0.127056\n",
      "Epoch: 013/025 | Batch 500/2000 | Loss: 0.181866\n",
      "Epoch: 013/025 | Batch 550/2000 | Loss: 0.204240\n",
      "Epoch: 013/025 | Batch 600/2000 | Loss: 0.078244\n",
      "Epoch: 013/025 | Batch 650/2000 | Loss: 0.098518\n",
      "Epoch: 013/025 | Batch 700/2000 | Loss: 0.070269\n",
      "Epoch: 013/025 | Batch 750/2000 | Loss: 0.057639\n",
      "Epoch: 013/025 | Batch 800/2000 | Loss: 0.319502\n",
      "Epoch: 013/025 | Batch 850/2000 | Loss: 0.185834\n",
      "Epoch: 013/025 | Batch 900/2000 | Loss: 0.153311\n",
      "Epoch: 013/025 | Batch 950/2000 | Loss: 0.242036\n",
      "Epoch: 013/025 | Batch 1000/2000 | Loss: 0.258956\n",
      "Epoch: 013/025 | Batch 1050/2000 | Loss: 0.085642\n",
      "Epoch: 013/025 | Batch 1100/2000 | Loss: 0.143100\n",
      "Epoch: 013/025 | Batch 1150/2000 | Loss: 0.084737\n",
      "Epoch: 013/025 | Batch 1200/2000 | Loss: 0.065176\n",
      "Epoch: 013/025 | Batch 1250/2000 | Loss: 0.247972\n",
      "Epoch: 013/025 | Batch 1300/2000 | Loss: 0.299721\n",
      "Epoch: 013/025 | Batch 1350/2000 | Loss: 0.122550\n",
      "Epoch: 013/025 | Batch 1400/2000 | Loss: 0.081378\n",
      "Epoch: 013/025 | Batch 1450/2000 | Loss: 0.213420\n",
      "Epoch: 013/025 | Batch 1500/2000 | Loss: 0.217044\n",
      "Epoch: 013/025 | Batch 1550/2000 | Loss: 0.151478\n",
      "Epoch: 013/025 | Batch 1600/2000 | Loss: 0.164560\n",
      "Epoch: 013/025 | Batch 1650/2000 | Loss: 0.152966\n",
      "Epoch: 013/025 | Batch 1700/2000 | Loss: 0.063092\n",
      "Epoch: 013/025 | Batch 1750/2000 | Loss: 0.123385\n",
      "Epoch: 013/025 | Batch 1800/2000 | Loss: 0.277643\n",
      "Epoch: 013/025 | Batch 1850/2000 | Loss: 0.094559\n",
      "Epoch: 013/025 | Batch 1900/2000 | Loss: 0.216618\n",
      "Epoch: 013/025 | Batch 1950/2000 | Loss: 0.047371\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 014/025 | Batch 000/2000 | Loss: 0.179933\n",
      "Epoch: 014/025 | Batch 050/2000 | Loss: 0.105212\n",
      "Epoch: 014/025 | Batch 100/2000 | Loss: 0.215647\n",
      "Epoch: 014/025 | Batch 150/2000 | Loss: 0.166773\n",
      "Epoch: 014/025 | Batch 200/2000 | Loss: 0.103590\n",
      "Epoch: 014/025 | Batch 250/2000 | Loss: 0.116043\n",
      "Epoch: 014/025 | Batch 300/2000 | Loss: 0.118287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014/025 | Batch 350/2000 | Loss: 0.126529\n",
      "Epoch: 014/025 | Batch 400/2000 | Loss: 0.121135\n",
      "Epoch: 014/025 | Batch 450/2000 | Loss: 0.203354\n",
      "Epoch: 014/025 | Batch 500/2000 | Loss: 0.156234\n",
      "Epoch: 014/025 | Batch 550/2000 | Loss: 0.283224\n",
      "Epoch: 014/025 | Batch 600/2000 | Loss: 0.149723\n",
      "Epoch: 014/025 | Batch 650/2000 | Loss: 0.085048\n",
      "Epoch: 014/025 | Batch 700/2000 | Loss: 0.301092\n",
      "Epoch: 014/025 | Batch 750/2000 | Loss: 0.069403\n",
      "Epoch: 014/025 | Batch 800/2000 | Loss: 0.110063\n",
      "Epoch: 014/025 | Batch 850/2000 | Loss: 0.219906\n",
      "Epoch: 014/025 | Batch 900/2000 | Loss: 0.103472\n",
      "Epoch: 014/025 | Batch 950/2000 | Loss: 0.224084\n",
      "Epoch: 014/025 | Batch 1000/2000 | Loss: 0.076041\n",
      "Epoch: 014/025 | Batch 1050/2000 | Loss: 0.123767\n",
      "Epoch: 014/025 | Batch 1100/2000 | Loss: 0.118335\n",
      "Epoch: 014/025 | Batch 1150/2000 | Loss: 0.154276\n",
      "Epoch: 014/025 | Batch 1200/2000 | Loss: 0.084514\n",
      "Epoch: 014/025 | Batch 1250/2000 | Loss: 0.208988\n",
      "Epoch: 014/025 | Batch 1300/2000 | Loss: 0.085546\n",
      "Epoch: 014/025 | Batch 1350/2000 | Loss: 0.339604\n",
      "Epoch: 014/025 | Batch 1400/2000 | Loss: 0.246462\n",
      "Epoch: 014/025 | Batch 1450/2000 | Loss: 0.070088\n",
      "Epoch: 014/025 | Batch 1500/2000 | Loss: 0.044102\n",
      "Epoch: 014/025 | Batch 1550/2000 | Loss: 0.124511\n",
      "Epoch: 014/025 | Batch 1600/2000 | Loss: 0.109027\n",
      "Epoch: 014/025 | Batch 1650/2000 | Loss: 0.187592\n",
      "Epoch: 014/025 | Batch 1700/2000 | Loss: 0.169744\n",
      "Epoch: 014/025 | Batch 1750/2000 | Loss: 0.110965\n",
      "Epoch: 014/025 | Batch 1800/2000 | Loss: 0.152483\n",
      "Epoch: 014/025 | Batch 1850/2000 | Loss: 0.118207\n",
      "Epoch: 014/025 | Batch 1900/2000 | Loss: 0.069282\n",
      "Epoch: 014/025 | Batch 1950/2000 | Loss: 0.032577\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 015/025 | Batch 000/2000 | Loss: 0.116985\n",
      "Epoch: 015/025 | Batch 050/2000 | Loss: 0.234955\n",
      "Epoch: 015/025 | Batch 100/2000 | Loss: 0.161233\n",
      "Epoch: 015/025 | Batch 150/2000 | Loss: 0.177833\n",
      "Epoch: 015/025 | Batch 200/2000 | Loss: 0.068873\n",
      "Epoch: 015/025 | Batch 250/2000 | Loss: 0.150099\n",
      "Epoch: 015/025 | Batch 300/2000 | Loss: 0.171028\n",
      "Epoch: 015/025 | Batch 350/2000 | Loss: 0.123651\n",
      "Epoch: 015/025 | Batch 400/2000 | Loss: 0.117535\n",
      "Epoch: 015/025 | Batch 450/2000 | Loss: 0.066104\n",
      "Epoch: 015/025 | Batch 500/2000 | Loss: 0.128421\n",
      "Epoch: 015/025 | Batch 550/2000 | Loss: 0.058215\n",
      "Epoch: 015/025 | Batch 600/2000 | Loss: 0.164177\n",
      "Epoch: 015/025 | Batch 650/2000 | Loss: 0.301090\n",
      "Epoch: 015/025 | Batch 700/2000 | Loss: 0.194378\n",
      "Epoch: 015/025 | Batch 750/2000 | Loss: 0.074256\n",
      "Epoch: 015/025 | Batch 800/2000 | Loss: 0.151670\n",
      "Epoch: 015/025 | Batch 850/2000 | Loss: 0.114234\n",
      "Epoch: 015/025 | Batch 900/2000 | Loss: 0.051009\n",
      "Epoch: 015/025 | Batch 950/2000 | Loss: 0.197401\n",
      "Epoch: 015/025 | Batch 1000/2000 | Loss: 0.313001\n",
      "Epoch: 015/025 | Batch 1050/2000 | Loss: 0.065392\n",
      "Epoch: 015/025 | Batch 1100/2000 | Loss: 0.139141\n",
      "Epoch: 015/025 | Batch 1150/2000 | Loss: 0.190076\n",
      "Epoch: 015/025 | Batch 1200/2000 | Loss: 0.048200\n",
      "Epoch: 015/025 | Batch 1250/2000 | Loss: 0.093459\n",
      "Epoch: 015/025 | Batch 1300/2000 | Loss: 0.113444\n",
      "Epoch: 015/025 | Batch 1350/2000 | Loss: 0.346494\n",
      "Epoch: 015/025 | Batch 1400/2000 | Loss: 0.170340\n",
      "Epoch: 015/025 | Batch 1450/2000 | Loss: 0.130489\n",
      "Epoch: 015/025 | Batch 1500/2000 | Loss: 0.063375\n",
      "Epoch: 015/025 | Batch 1550/2000 | Loss: 0.184340\n",
      "Epoch: 015/025 | Batch 1600/2000 | Loss: 0.160187\n",
      "Epoch: 015/025 | Batch 1650/2000 | Loss: 0.039311\n",
      "Epoch: 015/025 | Batch 1700/2000 | Loss: 0.023381\n",
      "Epoch: 015/025 | Batch 1750/2000 | Loss: 0.179645\n",
      "Epoch: 015/025 | Batch 1800/2000 | Loss: 0.115832\n",
      "Epoch: 015/025 | Batch 1850/2000 | Loss: 0.092579\n",
      "Epoch: 015/025 | Batch 1900/2000 | Loss: 0.175661\n",
      "Epoch: 015/025 | Batch 1950/2000 | Loss: 0.060864\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 016/025 | Batch 000/2000 | Loss: 0.059536\n",
      "Epoch: 016/025 | Batch 050/2000 | Loss: 0.083735\n",
      "Epoch: 016/025 | Batch 100/2000 | Loss: 0.105740\n",
      "Epoch: 016/025 | Batch 150/2000 | Loss: 0.181444\n",
      "Epoch: 016/025 | Batch 200/2000 | Loss: 0.034355\n",
      "Epoch: 016/025 | Batch 250/2000 | Loss: 0.062297\n",
      "Epoch: 016/025 | Batch 300/2000 | Loss: 0.085667\n",
      "Epoch: 016/025 | Batch 350/2000 | Loss: 0.082457\n",
      "Epoch: 016/025 | Batch 400/2000 | Loss: 0.169662\n",
      "Epoch: 016/025 | Batch 450/2000 | Loss: 0.114754\n",
      "Epoch: 016/025 | Batch 500/2000 | Loss: 0.128593\n",
      "Epoch: 016/025 | Batch 550/2000 | Loss: 0.258703\n",
      "Epoch: 016/025 | Batch 600/2000 | Loss: 0.086789\n",
      "Epoch: 016/025 | Batch 650/2000 | Loss: 0.274219\n",
      "Epoch: 016/025 | Batch 700/2000 | Loss: 0.101587\n",
      "Epoch: 016/025 | Batch 750/2000 | Loss: 0.042602\n",
      "Epoch: 016/025 | Batch 800/2000 | Loss: 0.138083\n",
      "Epoch: 016/025 | Batch 850/2000 | Loss: 0.342818\n",
      "Epoch: 016/025 | Batch 900/2000 | Loss: 0.048844\n",
      "Epoch: 016/025 | Batch 950/2000 | Loss: 0.021326\n",
      "Epoch: 016/025 | Batch 1000/2000 | Loss: 0.076057\n",
      "Epoch: 016/025 | Batch 1050/2000 | Loss: 0.007907\n",
      "Epoch: 016/025 | Batch 1100/2000 | Loss: 0.127586\n",
      "Epoch: 016/025 | Batch 1150/2000 | Loss: 0.076423\n",
      "Epoch: 016/025 | Batch 1200/2000 | Loss: 0.079100\n",
      "Epoch: 016/025 | Batch 1250/2000 | Loss: 0.202477\n",
      "Epoch: 016/025 | Batch 1300/2000 | Loss: 0.108555\n",
      "Epoch: 016/025 | Batch 1350/2000 | Loss: 0.069807\n",
      "Epoch: 016/025 | Batch 1400/2000 | Loss: 0.159889\n",
      "Epoch: 016/025 | Batch 1450/2000 | Loss: 0.077533\n",
      "Epoch: 016/025 | Batch 1500/2000 | Loss: 0.121687\n",
      "Epoch: 016/025 | Batch 1550/2000 | Loss: 0.079497\n",
      "Epoch: 016/025 | Batch 1600/2000 | Loss: 0.124192\n",
      "Epoch: 016/025 | Batch 1650/2000 | Loss: 0.025557\n",
      "Epoch: 016/025 | Batch 1700/2000 | Loss: 0.061773\n",
      "Epoch: 016/025 | Batch 1750/2000 | Loss: 0.085513\n",
      "Epoch: 016/025 | Batch 1800/2000 | Loss: 0.091614\n",
      "Epoch: 016/025 | Batch 1850/2000 | Loss: 0.023458\n",
      "Epoch: 016/025 | Batch 1900/2000 | Loss: 0.114361\n",
      "Epoch: 016/025 | Batch 1950/2000 | Loss: 0.185087\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 017/025 | Batch 000/2000 | Loss: 0.023714\n",
      "Epoch: 017/025 | Batch 050/2000 | Loss: 0.118900\n",
      "Epoch: 017/025 | Batch 100/2000 | Loss: 0.071639\n",
      "Epoch: 017/025 | Batch 150/2000 | Loss: 0.058541\n",
      "Epoch: 017/025 | Batch 200/2000 | Loss: 0.156663\n",
      "Epoch: 017/025 | Batch 250/2000 | Loss: 0.088838\n",
      "Epoch: 017/025 | Batch 300/2000 | Loss: 0.104167\n",
      "Epoch: 017/025 | Batch 350/2000 | Loss: 0.123074\n",
      "Epoch: 017/025 | Batch 400/2000 | Loss: 0.068902\n",
      "Epoch: 017/025 | Batch 450/2000 | Loss: 0.073386\n",
      "Epoch: 017/025 | Batch 500/2000 | Loss: 0.033894\n",
      "Epoch: 017/025 | Batch 550/2000 | Loss: 0.043965\n",
      "Epoch: 017/025 | Batch 600/2000 | Loss: 0.178445\n",
      "Epoch: 017/025 | Batch 650/2000 | Loss: 0.353398\n",
      "Epoch: 017/025 | Batch 700/2000 | Loss: 0.092517\n",
      "Epoch: 017/025 | Batch 750/2000 | Loss: 0.012426\n",
      "Epoch: 017/025 | Batch 800/2000 | Loss: 0.160753\n",
      "Epoch: 017/025 | Batch 850/2000 | Loss: 0.092954\n",
      "Epoch: 017/025 | Batch 900/2000 | Loss: 0.056528\n",
      "Epoch: 017/025 | Batch 950/2000 | Loss: 0.010483\n",
      "Epoch: 017/025 | Batch 1000/2000 | Loss: 0.064132\n",
      "Epoch: 017/025 | Batch 1050/2000 | Loss: 0.079700\n",
      "Epoch: 017/025 | Batch 1100/2000 | Loss: 0.261206\n",
      "Epoch: 017/025 | Batch 1150/2000 | Loss: 0.170571\n",
      "Epoch: 017/025 | Batch 1200/2000 | Loss: 0.044508\n",
      "Epoch: 017/025 | Batch 1250/2000 | Loss: 0.172669\n",
      "Epoch: 017/025 | Batch 1300/2000 | Loss: 0.049484\n",
      "Epoch: 017/025 | Batch 1350/2000 | Loss: 0.102483\n",
      "Epoch: 017/025 | Batch 1400/2000 | Loss: 0.219075\n",
      "Epoch: 017/025 | Batch 1450/2000 | Loss: 0.096531\n",
      "Epoch: 017/025 | Batch 1500/2000 | Loss: 0.197701\n",
      "Epoch: 017/025 | Batch 1550/2000 | Loss: 0.068840\n",
      "Epoch: 017/025 | Batch 1600/2000 | Loss: 0.102670\n",
      "Epoch: 017/025 | Batch 1650/2000 | Loss: 0.091750\n",
      "Epoch: 017/025 | Batch 1700/2000 | Loss: 0.034455\n",
      "Epoch: 017/025 | Batch 1750/2000 | Loss: 0.240017\n",
      "Epoch: 017/025 | Batch 1800/2000 | Loss: 0.055107\n",
      "Epoch: 017/025 | Batch 1850/2000 | Loss: 0.041164\n",
      "Epoch: 017/025 | Batch 1900/2000 | Loss: 0.155134\n",
      "Epoch: 017/025 | Batch 1950/2000 | Loss: 0.042770\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 018/025 | Batch 000/2000 | Loss: 0.102618\n",
      "Epoch: 018/025 | Batch 050/2000 | Loss: 0.122879\n",
      "Epoch: 018/025 | Batch 100/2000 | Loss: 0.133039\n",
      "Epoch: 018/025 | Batch 150/2000 | Loss: 0.049792\n",
      "Epoch: 018/025 | Batch 200/2000 | Loss: 0.187168\n",
      "Epoch: 018/025 | Batch 250/2000 | Loss: 0.134536\n",
      "Epoch: 018/025 | Batch 300/2000 | Loss: 0.102000\n",
      "Epoch: 018/025 | Batch 350/2000 | Loss: 0.113904\n",
      "Epoch: 018/025 | Batch 400/2000 | Loss: 0.269531\n",
      "Epoch: 018/025 | Batch 450/2000 | Loss: 0.087829\n",
      "Epoch: 018/025 | Batch 500/2000 | Loss: 0.122965\n",
      "Epoch: 018/025 | Batch 550/2000 | Loss: 0.059335\n",
      "Epoch: 018/025 | Batch 600/2000 | Loss: 0.096301\n",
      "Epoch: 018/025 | Batch 650/2000 | Loss: 0.363596\n",
      "Epoch: 018/025 | Batch 700/2000 | Loss: 0.156493\n",
      "Epoch: 018/025 | Batch 750/2000 | Loss: 0.045447\n",
      "Epoch: 018/025 | Batch 800/2000 | Loss: 0.246614\n",
      "Epoch: 018/025 | Batch 850/2000 | Loss: 0.069169\n",
      "Epoch: 018/025 | Batch 900/2000 | Loss: 0.043768\n",
      "Epoch: 018/025 | Batch 950/2000 | Loss: 0.081364\n",
      "Epoch: 018/025 | Batch 1000/2000 | Loss: 0.151075\n",
      "Epoch: 018/025 | Batch 1050/2000 | Loss: 0.036373\n",
      "Epoch: 018/025 | Batch 1100/2000 | Loss: 0.029456\n",
      "Epoch: 018/025 | Batch 1150/2000 | Loss: 0.048819\n",
      "Epoch: 018/025 | Batch 1200/2000 | Loss: 0.227911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018/025 | Batch 1250/2000 | Loss: 0.045240\n",
      "Epoch: 018/025 | Batch 1300/2000 | Loss: 0.191820\n",
      "Epoch: 018/025 | Batch 1350/2000 | Loss: 0.173458\n",
      "Epoch: 018/025 | Batch 1400/2000 | Loss: 0.207420\n",
      "Epoch: 018/025 | Batch 1450/2000 | Loss: 0.114630\n",
      "Epoch: 018/025 | Batch 1500/2000 | Loss: 0.087928\n",
      "Epoch: 018/025 | Batch 1550/2000 | Loss: 0.062894\n",
      "Epoch: 018/025 | Batch 1600/2000 | Loss: 0.081232\n",
      "Epoch: 018/025 | Batch 1650/2000 | Loss: 0.137118\n",
      "Epoch: 018/025 | Batch 1700/2000 | Loss: 0.068093\n",
      "Epoch: 018/025 | Batch 1750/2000 | Loss: 0.117140\n",
      "Epoch: 018/025 | Batch 1800/2000 | Loss: 0.058127\n",
      "Epoch: 018/025 | Batch 1850/2000 | Loss: 0.073985\n",
      "Epoch: 018/025 | Batch 1900/2000 | Loss: 0.097633\n",
      "Epoch: 018/025 | Batch 1950/2000 | Loss: 0.113918\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 019/025 | Batch 000/2000 | Loss: 0.202075\n",
      "Epoch: 019/025 | Batch 050/2000 | Loss: 0.094120\n",
      "Epoch: 019/025 | Batch 100/2000 | Loss: 0.128343\n",
      "Epoch: 019/025 | Batch 150/2000 | Loss: 0.166892\n",
      "Epoch: 019/025 | Batch 200/2000 | Loss: 0.064861\n",
      "Epoch: 019/025 | Batch 250/2000 | Loss: 0.027414\n",
      "Epoch: 019/025 | Batch 300/2000 | Loss: 0.272453\n",
      "Epoch: 019/025 | Batch 350/2000 | Loss: 0.059147\n",
      "Epoch: 019/025 | Batch 400/2000 | Loss: 0.115095\n",
      "Epoch: 019/025 | Batch 450/2000 | Loss: 0.223009\n",
      "Epoch: 019/025 | Batch 500/2000 | Loss: 0.030175\n",
      "Epoch: 019/025 | Batch 550/2000 | Loss: 0.170914\n",
      "Epoch: 019/025 | Batch 600/2000 | Loss: 0.042166\n",
      "Epoch: 019/025 | Batch 650/2000 | Loss: 0.101665\n",
      "Epoch: 019/025 | Batch 700/2000 | Loss: 0.058301\n",
      "Epoch: 019/025 | Batch 750/2000 | Loss: 0.070375\n",
      "Epoch: 019/025 | Batch 800/2000 | Loss: 0.106169\n",
      "Epoch: 019/025 | Batch 850/2000 | Loss: 0.050550\n",
      "Epoch: 019/025 | Batch 900/2000 | Loss: 0.101711\n",
      "Epoch: 019/025 | Batch 950/2000 | Loss: 0.088704\n",
      "Epoch: 019/025 | Batch 1000/2000 | Loss: 0.124716\n",
      "Epoch: 019/025 | Batch 1050/2000 | Loss: 0.079113\n",
      "Epoch: 019/025 | Batch 1100/2000 | Loss: 0.062122\n",
      "Epoch: 019/025 | Batch 1150/2000 | Loss: 0.119467\n",
      "Epoch: 019/025 | Batch 1200/2000 | Loss: 0.180382\n",
      "Epoch: 019/025 | Batch 1250/2000 | Loss: 0.063719\n",
      "Epoch: 019/025 | Batch 1300/2000 | Loss: 0.208792\n",
      "Epoch: 019/025 | Batch 1350/2000 | Loss: 0.114889\n",
      "Epoch: 019/025 | Batch 1400/2000 | Loss: 0.148370\n",
      "Epoch: 019/025 | Batch 1450/2000 | Loss: 0.148596\n",
      "Epoch: 019/025 | Batch 1500/2000 | Loss: 0.126720\n",
      "Epoch: 019/025 | Batch 1550/2000 | Loss: 0.066244\n",
      "Epoch: 019/025 | Batch 1600/2000 | Loss: 0.120354\n",
      "Epoch: 019/025 | Batch 1650/2000 | Loss: 0.055412\n",
      "Epoch: 019/025 | Batch 1700/2000 | Loss: 0.037516\n",
      "Epoch: 019/025 | Batch 1750/2000 | Loss: 0.078005\n",
      "Epoch: 019/025 | Batch 1800/2000 | Loss: 0.072445\n",
      "Epoch: 019/025 | Batch 1850/2000 | Loss: 0.087713\n",
      "Epoch: 019/025 | Batch 1900/2000 | Loss: 0.022297\n",
      "Epoch: 019/025 | Batch 1950/2000 | Loss: 0.012025\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 020/025 | Batch 000/2000 | Loss: 0.097802\n",
      "Epoch: 020/025 | Batch 050/2000 | Loss: 0.137847\n",
      "Epoch: 020/025 | Batch 100/2000 | Loss: 0.111712\n",
      "Epoch: 020/025 | Batch 150/2000 | Loss: 0.070159\n",
      "Epoch: 020/025 | Batch 200/2000 | Loss: 0.042322\n",
      "Epoch: 020/025 | Batch 250/2000 | Loss: 0.005682\n",
      "Epoch: 020/025 | Batch 300/2000 | Loss: 0.162895\n",
      "Epoch: 020/025 | Batch 350/2000 | Loss: 0.077776\n",
      "Epoch: 020/025 | Batch 400/2000 | Loss: 0.102814\n",
      "Epoch: 020/025 | Batch 450/2000 | Loss: 0.061699\n",
      "Epoch: 020/025 | Batch 500/2000 | Loss: 0.050525\n",
      "Epoch: 020/025 | Batch 550/2000 | Loss: 0.103211\n",
      "Epoch: 020/025 | Batch 600/2000 | Loss: 0.065798\n",
      "Epoch: 020/025 | Batch 650/2000 | Loss: 0.235882\n",
      "Epoch: 020/025 | Batch 700/2000 | Loss: 0.044309\n",
      "Epoch: 020/025 | Batch 750/2000 | Loss: 0.028346\n",
      "Epoch: 020/025 | Batch 800/2000 | Loss: 0.092126\n",
      "Epoch: 020/025 | Batch 850/2000 | Loss: 0.050981\n",
      "Epoch: 020/025 | Batch 900/2000 | Loss: 0.123734\n",
      "Epoch: 020/025 | Batch 950/2000 | Loss: 0.078526\n",
      "Epoch: 020/025 | Batch 1000/2000 | Loss: 0.099776\n",
      "Epoch: 020/025 | Batch 1050/2000 | Loss: 0.018713\n",
      "Epoch: 020/025 | Batch 1100/2000 | Loss: 0.050612\n",
      "Epoch: 020/025 | Batch 1150/2000 | Loss: 0.040530\n",
      "Epoch: 020/025 | Batch 1200/2000 | Loss: 0.079523\n",
      "Epoch: 020/025 | Batch 1250/2000 | Loss: 0.116701\n",
      "Epoch: 020/025 | Batch 1300/2000 | Loss: 0.031238\n",
      "Epoch: 020/025 | Batch 1350/2000 | Loss: 0.190283\n",
      "Epoch: 020/025 | Batch 1400/2000 | Loss: 0.140161\n",
      "Epoch: 020/025 | Batch 1450/2000 | Loss: 0.080963\n",
      "Epoch: 020/025 | Batch 1500/2000 | Loss: 0.042080\n",
      "Epoch: 020/025 | Batch 1550/2000 | Loss: 0.047817\n",
      "Epoch: 020/025 | Batch 1600/2000 | Loss: 0.236290\n",
      "Epoch: 020/025 | Batch 1650/2000 | Loss: 0.036674\n",
      "Epoch: 020/025 | Batch 1700/2000 | Loss: 0.005317\n",
      "Epoch: 020/025 | Batch 1750/2000 | Loss: 0.100381\n",
      "Epoch: 020/025 | Batch 1800/2000 | Loss: 0.034133\n",
      "Epoch: 020/025 | Batch 1850/2000 | Loss: 0.032536\n",
      "Epoch: 020/025 | Batch 1900/2000 | Loss: 0.252719\n",
      "Epoch: 020/025 | Batch 1950/2000 | Loss: 0.048240\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 021/025 | Batch 000/2000 | Loss: 0.100820\n",
      "Epoch: 021/025 | Batch 050/2000 | Loss: 0.073830\n",
      "Epoch: 021/025 | Batch 100/2000 | Loss: 0.100691\n",
      "Epoch: 021/025 | Batch 150/2000 | Loss: 0.018344\n",
      "Epoch: 021/025 | Batch 200/2000 | Loss: 0.102124\n",
      "Epoch: 021/025 | Batch 250/2000 | Loss: 0.009631\n",
      "Epoch: 021/025 | Batch 300/2000 | Loss: 0.061468\n",
      "Epoch: 021/025 | Batch 350/2000 | Loss: 0.027476\n",
      "Epoch: 021/025 | Batch 400/2000 | Loss: 0.079522\n",
      "Epoch: 021/025 | Batch 450/2000 | Loss: 0.163436\n",
      "Epoch: 021/025 | Batch 500/2000 | Loss: 0.038226\n",
      "Epoch: 021/025 | Batch 550/2000 | Loss: 0.136543\n",
      "Epoch: 021/025 | Batch 600/2000 | Loss: 0.048407\n",
      "Epoch: 021/025 | Batch 650/2000 | Loss: 0.112644\n",
      "Epoch: 021/025 | Batch 700/2000 | Loss: 0.144842\n",
      "Epoch: 021/025 | Batch 750/2000 | Loss: 0.010024\n",
      "Epoch: 021/025 | Batch 800/2000 | Loss: 0.065630\n",
      "Epoch: 021/025 | Batch 850/2000 | Loss: 0.072693\n",
      "Epoch: 021/025 | Batch 900/2000 | Loss: 0.041029\n",
      "Epoch: 021/025 | Batch 950/2000 | Loss: 0.072605\n",
      "Epoch: 021/025 | Batch 1000/2000 | Loss: 0.017789\n",
      "Epoch: 021/025 | Batch 1050/2000 | Loss: 0.069577\n",
      "Epoch: 021/025 | Batch 1100/2000 | Loss: 0.066848\n",
      "Epoch: 021/025 | Batch 1150/2000 | Loss: 0.029976\n",
      "Epoch: 021/025 | Batch 1200/2000 | Loss: 0.038856\n",
      "Epoch: 021/025 | Batch 1250/2000 | Loss: 0.073314\n",
      "Epoch: 021/025 | Batch 1300/2000 | Loss: 0.053985\n",
      "Epoch: 021/025 | Batch 1350/2000 | Loss: 0.014795\n",
      "Epoch: 021/025 | Batch 1400/2000 | Loss: 0.062407\n",
      "Epoch: 021/025 | Batch 1450/2000 | Loss: 0.109360\n",
      "Epoch: 021/025 | Batch 1500/2000 | Loss: 0.030241\n",
      "Epoch: 021/025 | Batch 1550/2000 | Loss: 0.060768\n",
      "Epoch: 021/025 | Batch 1600/2000 | Loss: 0.011843\n",
      "Epoch: 021/025 | Batch 1650/2000 | Loss: 0.027379\n",
      "Epoch: 021/025 | Batch 1700/2000 | Loss: 0.100225\n",
      "Epoch: 021/025 | Batch 1750/2000 | Loss: 0.042368\n",
      "Epoch: 021/025 | Batch 1800/2000 | Loss: 0.075367\n",
      "Epoch: 021/025 | Batch 1850/2000 | Loss: 0.054728\n",
      "Epoch: 021/025 | Batch 1900/2000 | Loss: 0.089317\n",
      "Epoch: 021/025 | Batch 1950/2000 | Loss: 0.096129\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 022/025 | Batch 000/2000 | Loss: 0.203368\n",
      "Epoch: 022/025 | Batch 050/2000 | Loss: 0.125176\n",
      "Epoch: 022/025 | Batch 100/2000 | Loss: 0.095557\n",
      "Epoch: 022/025 | Batch 150/2000 | Loss: 0.048509\n",
      "Epoch: 022/025 | Batch 200/2000 | Loss: 0.044833\n",
      "Epoch: 022/025 | Batch 250/2000 | Loss: 0.027686\n",
      "Epoch: 022/025 | Batch 300/2000 | Loss: 0.025025\n",
      "Epoch: 022/025 | Batch 350/2000 | Loss: 0.100530\n",
      "Epoch: 022/025 | Batch 400/2000 | Loss: 0.086249\n",
      "Epoch: 022/025 | Batch 450/2000 | Loss: 0.089418\n",
      "Epoch: 022/025 | Batch 500/2000 | Loss: 0.078295\n",
      "Epoch: 022/025 | Batch 550/2000 | Loss: 0.019363\n",
      "Epoch: 022/025 | Batch 600/2000 | Loss: 0.055416\n",
      "Epoch: 022/025 | Batch 650/2000 | Loss: 0.111006\n",
      "Epoch: 022/025 | Batch 700/2000 | Loss: 0.052841\n",
      "Epoch: 022/025 | Batch 750/2000 | Loss: 0.066632\n",
      "Epoch: 022/025 | Batch 800/2000 | Loss: 0.118077\n",
      "Epoch: 022/025 | Batch 850/2000 | Loss: 0.014165\n",
      "Epoch: 022/025 | Batch 900/2000 | Loss: 0.016399\n",
      "Epoch: 022/025 | Batch 950/2000 | Loss: 0.042953\n",
      "Epoch: 022/025 | Batch 1000/2000 | Loss: 0.024292\n",
      "Epoch: 022/025 | Batch 1050/2000 | Loss: 0.040995\n",
      "Epoch: 022/025 | Batch 1100/2000 | Loss: 0.074588\n",
      "Epoch: 022/025 | Batch 1150/2000 | Loss: 0.059394\n",
      "Epoch: 022/025 | Batch 1200/2000 | Loss: 0.112698\n",
      "Epoch: 022/025 | Batch 1250/2000 | Loss: 0.070844\n",
      "Epoch: 022/025 | Batch 1300/2000 | Loss: 0.060635\n",
      "Epoch: 022/025 | Batch 1350/2000 | Loss: 0.088690\n",
      "Epoch: 022/025 | Batch 1400/2000 | Loss: 0.061443\n",
      "Epoch: 022/025 | Batch 1450/2000 | Loss: 0.090834\n",
      "Epoch: 022/025 | Batch 1500/2000 | Loss: 0.143582\n",
      "Epoch: 022/025 | Batch 1550/2000 | Loss: 0.126478\n",
      "Epoch: 022/025 | Batch 1600/2000 | Loss: 0.127547\n",
      "Epoch: 022/025 | Batch 1650/2000 | Loss: 0.047735\n",
      "Epoch: 022/025 | Batch 1700/2000 | Loss: 0.080194\n",
      "Epoch: 022/025 | Batch 1750/2000 | Loss: 0.093473\n",
      "Epoch: 022/025 | Batch 1800/2000 | Loss: 0.048740\n",
      "Epoch: 022/025 | Batch 1850/2000 | Loss: 0.033903\n",
      "Epoch: 022/025 | Batch 1900/2000 | Loss: 0.040135\n",
      "Epoch: 022/025 | Batch 1950/2000 | Loss: 0.062753\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 023/025 | Batch 000/2000 | Loss: 0.089905\n",
      "Epoch: 023/025 | Batch 050/2000 | Loss: 0.068567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023/025 | Batch 100/2000 | Loss: 0.102002\n",
      "Epoch: 023/025 | Batch 150/2000 | Loss: 0.113289\n",
      "Epoch: 023/025 | Batch 200/2000 | Loss: 0.025732\n",
      "Epoch: 023/025 | Batch 250/2000 | Loss: 0.115393\n",
      "Epoch: 023/025 | Batch 300/2000 | Loss: 0.160562\n",
      "Epoch: 023/025 | Batch 350/2000 | Loss: 0.032659\n",
      "Epoch: 023/025 | Batch 400/2000 | Loss: 0.040091\n",
      "Epoch: 023/025 | Batch 450/2000 | Loss: 0.044041\n",
      "Epoch: 023/025 | Batch 500/2000 | Loss: 0.018382\n",
      "Epoch: 023/025 | Batch 550/2000 | Loss: 0.090279\n",
      "Epoch: 023/025 | Batch 600/2000 | Loss: 0.098186\n",
      "Epoch: 023/025 | Batch 650/2000 | Loss: 0.025286\n",
      "Epoch: 023/025 | Batch 700/2000 | Loss: 0.097253\n",
      "Epoch: 023/025 | Batch 750/2000 | Loss: 0.081296\n",
      "Epoch: 023/025 | Batch 800/2000 | Loss: 0.098267\n",
      "Epoch: 023/025 | Batch 850/2000 | Loss: 0.099129\n",
      "Epoch: 023/025 | Batch 900/2000 | Loss: 0.057075\n",
      "Epoch: 023/025 | Batch 950/2000 | Loss: 0.071073\n",
      "Epoch: 023/025 | Batch 1000/2000 | Loss: 0.067098\n",
      "Epoch: 023/025 | Batch 1050/2000 | Loss: 0.041091\n",
      "Epoch: 023/025 | Batch 1100/2000 | Loss: 0.033707\n",
      "Epoch: 023/025 | Batch 1150/2000 | Loss: 0.150630\n",
      "Epoch: 023/025 | Batch 1200/2000 | Loss: 0.060312\n",
      "Epoch: 023/025 | Batch 1250/2000 | Loss: 0.104359\n",
      "Epoch: 023/025 | Batch 1300/2000 | Loss: 0.027332\n",
      "Epoch: 023/025 | Batch 1350/2000 | Loss: 0.055367\n",
      "Epoch: 023/025 | Batch 1400/2000 | Loss: 0.046858\n",
      "Epoch: 023/025 | Batch 1450/2000 | Loss: 0.037372\n",
      "Epoch: 023/025 | Batch 1500/2000 | Loss: 0.111913\n",
      "Epoch: 023/025 | Batch 1550/2000 | Loss: 0.051097\n",
      "Epoch: 023/025 | Batch 1600/2000 | Loss: 0.135136\n",
      "Epoch: 023/025 | Batch 1650/2000 | Loss: 0.016128\n",
      "Epoch: 023/025 | Batch 1700/2000 | Loss: 0.117912\n",
      "Epoch: 023/025 | Batch 1750/2000 | Loss: 0.050255\n",
      "Epoch: 023/025 | Batch 1800/2000 | Loss: 0.024349\n",
      "Epoch: 023/025 | Batch 1850/2000 | Loss: 0.022798\n",
      "Epoch: 023/025 | Batch 1900/2000 | Loss: 0.052376\n",
      "Epoch: 023/025 | Batch 1950/2000 | Loss: 0.025792\n",
      "Time elapsed: 0.19 min\n",
      "Epoch: 024/025 | Batch 000/2000 | Loss: 0.192717\n",
      "Epoch: 024/025 | Batch 050/2000 | Loss: 0.042314\n",
      "Epoch: 024/025 | Batch 100/2000 | Loss: 0.036773\n",
      "Epoch: 024/025 | Batch 150/2000 | Loss: 0.024442\n",
      "Epoch: 024/025 | Batch 200/2000 | Loss: 0.067821\n",
      "Epoch: 024/025 | Batch 250/2000 | Loss: 0.024209\n",
      "Epoch: 024/025 | Batch 300/2000 | Loss: 0.068845\n",
      "Epoch: 024/025 | Batch 350/2000 | Loss: 0.049161\n",
      "Epoch: 024/025 | Batch 400/2000 | Loss: 0.049099\n",
      "Epoch: 024/025 | Batch 450/2000 | Loss: 0.034535\n",
      "Epoch: 024/025 | Batch 500/2000 | Loss: 0.025926\n",
      "Epoch: 024/025 | Batch 550/2000 | Loss: 0.026252\n",
      "Epoch: 024/025 | Batch 600/2000 | Loss: 0.073336\n",
      "Epoch: 024/025 | Batch 650/2000 | Loss: 0.072598\n",
      "Epoch: 024/025 | Batch 700/2000 | Loss: 0.032707\n",
      "Epoch: 024/025 | Batch 750/2000 | Loss: 0.011953\n",
      "Epoch: 024/025 | Batch 800/2000 | Loss: 0.115444\n",
      "Epoch: 024/025 | Batch 850/2000 | Loss: 0.245494\n",
      "Epoch: 024/025 | Batch 900/2000 | Loss: 0.130603\n",
      "Epoch: 024/025 | Batch 950/2000 | Loss: 0.004151\n",
      "Epoch: 024/025 | Batch 1000/2000 | Loss: 0.020491\n",
      "Epoch: 024/025 | Batch 1050/2000 | Loss: 0.016959\n",
      "Epoch: 024/025 | Batch 1100/2000 | Loss: 0.084061\n",
      "Epoch: 024/025 | Batch 1150/2000 | Loss: 0.088607\n",
      "Epoch: 024/025 | Batch 1200/2000 | Loss: 0.039082\n",
      "Epoch: 024/025 | Batch 1250/2000 | Loss: 0.070841\n",
      "Epoch: 024/025 | Batch 1300/2000 | Loss: 0.053698\n",
      "Epoch: 024/025 | Batch 1350/2000 | Loss: 0.102146\n",
      "Epoch: 024/025 | Batch 1400/2000 | Loss: 0.132355\n",
      "Epoch: 024/025 | Batch 1450/2000 | Loss: 0.251290\n",
      "Epoch: 024/025 | Batch 1500/2000 | Loss: 0.087843\n",
      "Epoch: 024/025 | Batch 1550/2000 | Loss: 0.014496\n",
      "Epoch: 024/025 | Batch 1600/2000 | Loss: 0.074043\n",
      "Epoch: 024/025 | Batch 1650/2000 | Loss: 0.136195\n",
      "Epoch: 024/025 | Batch 1700/2000 | Loss: 0.076022\n",
      "Epoch: 024/025 | Batch 1750/2000 | Loss: 0.173745\n",
      "Epoch: 024/025 | Batch 1800/2000 | Loss: 0.021034\n",
      "Epoch: 024/025 | Batch 1850/2000 | Loss: 0.016831\n",
      "Epoch: 024/025 | Batch 1900/2000 | Loss: 0.134504\n",
      "Epoch: 024/025 | Batch 1950/2000 | Loss: 0.051355\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 025/025 | Batch 000/2000 | Loss: 0.028349\n",
      "Epoch: 025/025 | Batch 050/2000 | Loss: 0.019593\n",
      "Epoch: 025/025 | Batch 100/2000 | Loss: 0.107380\n",
      "Epoch: 025/025 | Batch 150/2000 | Loss: 0.030461\n",
      "Epoch: 025/025 | Batch 200/2000 | Loss: 0.013350\n",
      "Epoch: 025/025 | Batch 250/2000 | Loss: 0.055919\n",
      "Epoch: 025/025 | Batch 300/2000 | Loss: 0.019055\n",
      "Epoch: 025/025 | Batch 350/2000 | Loss: 0.057215\n",
      "Epoch: 025/025 | Batch 400/2000 | Loss: 0.076152\n",
      "Epoch: 025/025 | Batch 450/2000 | Loss: 0.022211\n",
      "Epoch: 025/025 | Batch 500/2000 | Loss: 0.030757\n",
      "Epoch: 025/025 | Batch 550/2000 | Loss: 0.071491\n",
      "Epoch: 025/025 | Batch 600/2000 | Loss: 0.068550\n",
      "Epoch: 025/025 | Batch 650/2000 | Loss: 0.047068\n",
      "Epoch: 025/025 | Batch 700/2000 | Loss: 0.099643\n",
      "Epoch: 025/025 | Batch 750/2000 | Loss: 0.062295\n",
      "Epoch: 025/025 | Batch 800/2000 | Loss: 0.048840\n",
      "Epoch: 025/025 | Batch 850/2000 | Loss: 0.021780\n",
      "Epoch: 025/025 | Batch 900/2000 | Loss: 0.039924\n",
      "Epoch: 025/025 | Batch 950/2000 | Loss: 0.066296\n",
      "Epoch: 025/025 | Batch 1000/2000 | Loss: 0.020962\n",
      "Epoch: 025/025 | Batch 1050/2000 | Loss: 0.005809\n",
      "Epoch: 025/025 | Batch 1100/2000 | Loss: 0.011394\n",
      "Epoch: 025/025 | Batch 1150/2000 | Loss: 0.011755\n",
      "Epoch: 025/025 | Batch 1200/2000 | Loss: 0.016949\n",
      "Epoch: 025/025 | Batch 1250/2000 | Loss: 0.037581\n",
      "Epoch: 025/025 | Batch 1300/2000 | Loss: 0.025580\n",
      "Epoch: 025/025 | Batch 1350/2000 | Loss: 0.095498\n",
      "Epoch: 025/025 | Batch 1400/2000 | Loss: 0.095694\n",
      "Epoch: 025/025 | Batch 1450/2000 | Loss: 0.010161\n",
      "Epoch: 025/025 | Batch 1500/2000 | Loss: 0.105566\n",
      "Epoch: 025/025 | Batch 1550/2000 | Loss: 0.053553\n",
      "Epoch: 025/025 | Batch 1600/2000 | Loss: 0.043189\n",
      "Epoch: 025/025 | Batch 1650/2000 | Loss: 0.087287\n",
      "Epoch: 025/025 | Batch 1700/2000 | Loss: 0.007502\n",
      "Epoch: 025/025 | Batch 1750/2000 | Loss: 0.221379\n",
      "Epoch: 025/025 | Batch 1800/2000 | Loss: 0.035596\n",
      "Epoch: 025/025 | Batch 1850/2000 | Loss: 0.029847\n",
      "Epoch: 025/025 | Batch 1900/2000 | Loss: 0.049136\n",
      "Epoch: 025/025 | Batch 1950/2000 | Loss: 0.009581\n",
      "Time elapsed: 0.21 min\n",
      "Total Training Time: 0.21 min\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "my_model.trainer(tran_train_input, tran_train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error : 0.0% \n",
      "Test error : 3.0%\n",
      "The total number of the parameters is: 101514\n"
     ]
    }
   ],
   "source": [
    "# output the train error and test error when figuring out what the number is\n",
    "print(\"Train error : %.1f%% \\nTest error : %.1f%%\" %\n",
    "      (my_model.compute_error(tran_train_input, tran_train_classes),\n",
    "       my_model.compute_error(tran_test_input, tran_test_classes)))\n",
    "\n",
    "print(\"The total number of the parameters is: %d\" % (sum(p.numel() for p in my_model.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have trained a model to figure out what a number is, then I can apply this model to compare the number pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error : 9.2% \n",
      "Test error : 14.0%\n"
     ]
    }
   ],
   "source": [
    "# output the train error and test error when comparing the two numbers\n",
    "print(\"Train error : %.1f%% \\nTest error : %.1f%%\" %\n",
    "      (my_model.compare_two_digit(train_input,train_target),\n",
    "      my_model.compare_two_digit(test_input,test_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this method is we can split the 1000 pairs of data into 2000 data. Written digital number recognition has acquired very high accuracy, so we are going to firstly recognize the numbers and then then compare them. Finally, we compare the results with the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
