{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_practical_prologue as prologue\n",
    "#直接导入出现http403错误\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable是对Tensor的封装，操作与tensor基本一致，不同的是，每一个Variable被构建的时候，都包含三个属性：\n",
    "# 1）Variable中所包含的tensor\n",
    "# 2）tensor的梯度 .grad\n",
    "# 3）以何种方式得到这种梯度 .grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接导入出现http403错误\n",
    "# have to add a header to your urllib request (due to that site moving to Cloudflare protection)\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "#*********************** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "N_PAIRS = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2, 14, 14])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_input[0][0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_input[0][1],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.float32\n",
      "torch.Size([1000])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(train_input.dtype)\n",
    "print(train_target.shape)\n",
    "print(train_target.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input=Variable(train_input, requires_grad=True) \n",
    "train_target=Variable(train_target)\n",
    "test_input=Variable(train_input, requires_grad=True)\n",
    "test_target=Variable(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32,64,kernel_size=2)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "        self.batch_size = 50\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.num_epochs = 25\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "        # Training Function\n",
    "\n",
    "    def trainer(self, train_input, train_target):\n",
    "        \"\"\"\n",
    "        Train the model on a training set\n",
    "        :param train_input: Training features\n",
    "        :param train_target: Training labels\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch_idx in range(0,train_input.size(0),self.batch_size):\n",
    "                output = self(train_input[batch_idx:batch_idx+self.batch_size]) \n",
    "                loss = self.criterion(output, train_target[batch_idx:batch_idx+self.batch_size])  \n",
    "                self.optimizer.zero_grad()                          #清零梯度\n",
    "                loss.backward()                                #反向求梯度\n",
    "                self.optimizer.step()\n",
    "\n",
    "                if not batch_idx % 50:\n",
    "                    print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.6f' \n",
    "                           %(epoch+1, self.num_epochs, batch_idx, \n",
    "                             len(train_input), loss))\n",
    "            print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "        \n",
    "\n",
    "        # Test error\n",
    "\n",
    "    def compute_error(self, input_data, target):\n",
    "        \"\"\"\n",
    "        Compute the number of error of the model on a test set\n",
    "        :param input_data: test features\n",
    "        :param target: test target\n",
    "        :return: error rate of the input data\n",
    "        \"\"\"  \n",
    "    \n",
    "        #测试模型\n",
    "        self.eval()      #测试模式，关闭正则化\n",
    "        errors = 0\n",
    "        for idx in range(0,input_data.size(0),self.batch_size):\n",
    "            input_batch=input_data.narrow(0,idx,self.batch_size)\n",
    "            outputs = self(input_batch)\n",
    "            _, predicted = torch.max(outputs, 1)   #返回值和索引\n",
    "            target_labels = target.narrow(0, idx, self.batch_size)\n",
    "            errors += torch.sum(predicted != target_labels)\n",
    "\n",
    "        return float(errors)*100/input_data.size(0)\n",
    "    def save_model(self,model_name):\n",
    "        \"\"\"\n",
    "        Save the model to a direction\n",
    "        :param model_name: the model name, e.g. CNN_Net.pth\n",
    "        \"\"\"         \n",
    "        torch.save(self, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=CNN_Net()\n",
    "my_model.save_model('CNN_Net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/025 | Batch 000/1000 | Loss: 4.469487\n",
      "Epoch: 001/025 | Batch 050/1000 | Loss: 24.257231\n",
      "Epoch: 001/025 | Batch 100/1000 | Loss: 16.048353\n",
      "Epoch: 001/025 | Batch 150/1000 | Loss: 2.079017\n",
      "Epoch: 001/025 | Batch 200/1000 | Loss: 6.801905\n",
      "Epoch: 001/025 | Batch 250/1000 | Loss: 10.917764\n",
      "Epoch: 001/025 | Batch 300/1000 | Loss: 11.342573\n",
      "Epoch: 001/025 | Batch 350/1000 | Loss: 6.238704\n",
      "Epoch: 001/025 | Batch 400/1000 | Loss: 4.412317\n",
      "Epoch: 001/025 | Batch 450/1000 | Loss: 2.786123\n",
      "Epoch: 001/025 | Batch 500/1000 | Loss: 1.082032\n",
      "Epoch: 001/025 | Batch 550/1000 | Loss: 3.331166\n",
      "Epoch: 001/025 | Batch 600/1000 | Loss: 3.899317\n",
      "Epoch: 001/025 | Batch 650/1000 | Loss: 2.412399\n",
      "Epoch: 001/025 | Batch 700/1000 | Loss: 1.301060\n",
      "Epoch: 001/025 | Batch 750/1000 | Loss: 0.948079\n",
      "Epoch: 001/025 | Batch 800/1000 | Loss: 1.965607\n",
      "Epoch: 001/025 | Batch 850/1000 | Loss: 1.583278\n",
      "Epoch: 001/025 | Batch 900/1000 | Loss: 2.013511\n",
      "Epoch: 001/025 | Batch 950/1000 | Loss: 1.272594\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/025 | Batch 000/1000 | Loss: 0.867233\n",
      "Epoch: 002/025 | Batch 050/1000 | Loss: 0.712927\n",
      "Epoch: 002/025 | Batch 100/1000 | Loss: 0.641454\n",
      "Epoch: 002/025 | Batch 150/1000 | Loss: 0.938449\n",
      "Epoch: 002/025 | Batch 200/1000 | Loss: 1.203526\n",
      "Epoch: 002/025 | Batch 250/1000 | Loss: 1.119480\n",
      "Epoch: 002/025 | Batch 300/1000 | Loss: 0.705401\n",
      "Epoch: 002/025 | Batch 350/1000 | Loss: 0.637007\n",
      "Epoch: 002/025 | Batch 400/1000 | Loss: 0.611315\n",
      "Epoch: 002/025 | Batch 450/1000 | Loss: 0.651420\n",
      "Epoch: 002/025 | Batch 500/1000 | Loss: 0.646490\n",
      "Epoch: 002/025 | Batch 550/1000 | Loss: 0.931135\n",
      "Epoch: 002/025 | Batch 600/1000 | Loss: 0.615037\n",
      "Epoch: 002/025 | Batch 650/1000 | Loss: 0.747921\n",
      "Epoch: 002/025 | Batch 700/1000 | Loss: 0.867187\n",
      "Epoch: 002/025 | Batch 750/1000 | Loss: 0.719516\n",
      "Epoch: 002/025 | Batch 800/1000 | Loss: 0.682197\n",
      "Epoch: 002/025 | Batch 850/1000 | Loss: 0.619065\n",
      "Epoch: 002/025 | Batch 900/1000 | Loss: 0.662493\n",
      "Epoch: 002/025 | Batch 950/1000 | Loss: 0.612593\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/025 | Batch 000/1000 | Loss: 0.632163\n",
      "Epoch: 003/025 | Batch 050/1000 | Loss: 0.542065\n",
      "Epoch: 003/025 | Batch 100/1000 | Loss: 0.606038\n",
      "Epoch: 003/025 | Batch 150/1000 | Loss: 0.581873\n",
      "Epoch: 003/025 | Batch 200/1000 | Loss: 0.614878\n",
      "Epoch: 003/025 | Batch 250/1000 | Loss: 0.577534\n",
      "Epoch: 003/025 | Batch 300/1000 | Loss: 0.494356\n",
      "Epoch: 003/025 | Batch 350/1000 | Loss: 0.456611\n",
      "Epoch: 003/025 | Batch 400/1000 | Loss: 0.515645\n",
      "Epoch: 003/025 | Batch 450/1000 | Loss: 0.579113\n",
      "Epoch: 003/025 | Batch 500/1000 | Loss: 0.508293\n",
      "Epoch: 003/025 | Batch 550/1000 | Loss: 0.634743\n",
      "Epoch: 003/025 | Batch 600/1000 | Loss: 0.416564\n",
      "Epoch: 003/025 | Batch 650/1000 | Loss: 0.452434\n",
      "Epoch: 003/025 | Batch 700/1000 | Loss: 0.652013\n",
      "Epoch: 003/025 | Batch 750/1000 | Loss: 0.573207\n",
      "Epoch: 003/025 | Batch 800/1000 | Loss: 0.482342\n",
      "Epoch: 003/025 | Batch 850/1000 | Loss: 0.536135\n",
      "Epoch: 003/025 | Batch 900/1000 | Loss: 0.531380\n",
      "Epoch: 003/025 | Batch 950/1000 | Loss: 0.477852\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/025 | Batch 000/1000 | Loss: 0.466956\n",
      "Epoch: 004/025 | Batch 050/1000 | Loss: 0.357520\n",
      "Epoch: 004/025 | Batch 100/1000 | Loss: 0.470957\n",
      "Epoch: 004/025 | Batch 150/1000 | Loss: 0.480325\n",
      "Epoch: 004/025 | Batch 200/1000 | Loss: 0.490115\n",
      "Epoch: 004/025 | Batch 250/1000 | Loss: 0.493425\n",
      "Epoch: 004/025 | Batch 300/1000 | Loss: 0.407418\n",
      "Epoch: 004/025 | Batch 350/1000 | Loss: 0.386304\n",
      "Epoch: 004/025 | Batch 400/1000 | Loss: 0.447211\n",
      "Epoch: 004/025 | Batch 450/1000 | Loss: 0.487509\n",
      "Epoch: 004/025 | Batch 500/1000 | Loss: 0.366204\n",
      "Epoch: 004/025 | Batch 550/1000 | Loss: 0.558482\n",
      "Epoch: 004/025 | Batch 600/1000 | Loss: 0.356274\n",
      "Epoch: 004/025 | Batch 650/1000 | Loss: 0.381187\n",
      "Epoch: 004/025 | Batch 700/1000 | Loss: 0.561090\n",
      "Epoch: 004/025 | Batch 750/1000 | Loss: 0.504601\n",
      "Epoch: 004/025 | Batch 800/1000 | Loss: 0.458629\n",
      "Epoch: 004/025 | Batch 850/1000 | Loss: 0.438831\n",
      "Epoch: 004/025 | Batch 900/1000 | Loss: 0.471822\n",
      "Epoch: 004/025 | Batch 950/1000 | Loss: 0.390560\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/025 | Batch 000/1000 | Loss: 0.392417\n",
      "Epoch: 005/025 | Batch 050/1000 | Loss: 0.289172\n",
      "Epoch: 005/025 | Batch 100/1000 | Loss: 0.394722\n",
      "Epoch: 005/025 | Batch 150/1000 | Loss: 0.415300\n",
      "Epoch: 005/025 | Batch 200/1000 | Loss: 0.413788\n",
      "Epoch: 005/025 | Batch 250/1000 | Loss: 0.407128\n",
      "Epoch: 005/025 | Batch 300/1000 | Loss: 0.366695\n",
      "Epoch: 005/025 | Batch 350/1000 | Loss: 0.319703\n",
      "Epoch: 005/025 | Batch 400/1000 | Loss: 0.406817\n",
      "Epoch: 005/025 | Batch 450/1000 | Loss: 0.418015\n",
      "Epoch: 005/025 | Batch 500/1000 | Loss: 0.336294\n",
      "Epoch: 005/025 | Batch 550/1000 | Loss: 0.555549\n",
      "Epoch: 005/025 | Batch 600/1000 | Loss: 0.291181\n",
      "Epoch: 005/025 | Batch 650/1000 | Loss: 0.322338\n",
      "Epoch: 005/025 | Batch 700/1000 | Loss: 0.491991\n",
      "Epoch: 005/025 | Batch 750/1000 | Loss: 0.437812\n",
      "Epoch: 005/025 | Batch 800/1000 | Loss: 0.368763\n",
      "Epoch: 005/025 | Batch 850/1000 | Loss: 0.414484\n",
      "Epoch: 005/025 | Batch 900/1000 | Loss: 0.396299\n",
      "Epoch: 005/025 | Batch 950/1000 | Loss: 0.372876\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 006/025 | Batch 000/1000 | Loss: 0.351649\n",
      "Epoch: 006/025 | Batch 050/1000 | Loss: 0.243050\n",
      "Epoch: 006/025 | Batch 100/1000 | Loss: 0.341339\n",
      "Epoch: 006/025 | Batch 150/1000 | Loss: 0.375603\n",
      "Epoch: 006/025 | Batch 200/1000 | Loss: 0.389024\n",
      "Epoch: 006/025 | Batch 250/1000 | Loss: 0.348161\n",
      "Epoch: 006/025 | Batch 300/1000 | Loss: 0.303938\n",
      "Epoch: 006/025 | Batch 350/1000 | Loss: 0.273378\n",
      "Epoch: 006/025 | Batch 400/1000 | Loss: 0.327876\n",
      "Epoch: 006/025 | Batch 450/1000 | Loss: 0.340495\n",
      "Epoch: 006/025 | Batch 500/1000 | Loss: 0.272324\n",
      "Epoch: 006/025 | Batch 550/1000 | Loss: 0.475649\n",
      "Epoch: 006/025 | Batch 600/1000 | Loss: 0.250315\n",
      "Epoch: 006/025 | Batch 650/1000 | Loss: 0.263588\n",
      "Epoch: 006/025 | Batch 700/1000 | Loss: 0.404901\n",
      "Epoch: 006/025 | Batch 750/1000 | Loss: 0.377612\n",
      "Epoch: 006/025 | Batch 800/1000 | Loss: 0.286383\n",
      "Epoch: 006/025 | Batch 850/1000 | Loss: 0.331950\n",
      "Epoch: 006/025 | Batch 900/1000 | Loss: 0.324956\n",
      "Epoch: 006/025 | Batch 950/1000 | Loss: 0.301074\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 007/025 | Batch 000/1000 | Loss: 0.293303\n",
      "Epoch: 007/025 | Batch 050/1000 | Loss: 0.204908\n",
      "Epoch: 007/025 | Batch 100/1000 | Loss: 0.280146\n",
      "Epoch: 007/025 | Batch 150/1000 | Loss: 0.324689\n",
      "Epoch: 007/025 | Batch 200/1000 | Loss: 0.326435\n",
      "Epoch: 007/025 | Batch 250/1000 | Loss: 0.256956\n",
      "Epoch: 007/025 | Batch 300/1000 | Loss: 0.258089\n",
      "Epoch: 007/025 | Batch 350/1000 | Loss: 0.213382\n",
      "Epoch: 007/025 | Batch 400/1000 | Loss: 0.279237\n",
      "Epoch: 007/025 | Batch 450/1000 | Loss: 0.261956\n",
      "Epoch: 007/025 | Batch 500/1000 | Loss: 0.213373\n",
      "Epoch: 007/025 | Batch 550/1000 | Loss: 0.416439\n",
      "Epoch: 007/025 | Batch 600/1000 | Loss: 0.193018\n",
      "Epoch: 007/025 | Batch 650/1000 | Loss: 0.199021\n",
      "Epoch: 007/025 | Batch 700/1000 | Loss: 0.318546\n",
      "Epoch: 007/025 | Batch 750/1000 | Loss: 0.304215\n",
      "Epoch: 007/025 | Batch 800/1000 | Loss: 0.215963\n",
      "Epoch: 007/025 | Batch 850/1000 | Loss: 0.294572\n",
      "Epoch: 007/025 | Batch 900/1000 | Loss: 0.291684\n",
      "Epoch: 007/025 | Batch 950/1000 | Loss: 0.241345\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 008/025 | Batch 000/1000 | Loss: 0.319820\n",
      "Epoch: 008/025 | Batch 050/1000 | Loss: 0.161982\n",
      "Epoch: 008/025 | Batch 100/1000 | Loss: 0.224436\n",
      "Epoch: 008/025 | Batch 150/1000 | Loss: 0.274450\n",
      "Epoch: 008/025 | Batch 200/1000 | Loss: 0.267851\n",
      "Epoch: 008/025 | Batch 250/1000 | Loss: 0.196116\n",
      "Epoch: 008/025 | Batch 300/1000 | Loss: 0.203558\n",
      "Epoch: 008/025 | Batch 350/1000 | Loss: 0.159392\n",
      "Epoch: 008/025 | Batch 400/1000 | Loss: 0.253630\n",
      "Epoch: 008/025 | Batch 450/1000 | Loss: 0.244958\n",
      "Epoch: 008/025 | Batch 500/1000 | Loss: 0.165274\n",
      "Epoch: 008/025 | Batch 550/1000 | Loss: 0.330680\n",
      "Epoch: 008/025 | Batch 600/1000 | Loss: 0.172069\n",
      "Epoch: 008/025 | Batch 650/1000 | Loss: 0.165298\n",
      "Epoch: 008/025 | Batch 700/1000 | Loss: 0.231737\n",
      "Epoch: 008/025 | Batch 750/1000 | Loss: 0.185181\n",
      "Epoch: 008/025 | Batch 800/1000 | Loss: 0.166499\n",
      "Epoch: 008/025 | Batch 850/1000 | Loss: 0.224165\n",
      "Epoch: 008/025 | Batch 900/1000 | Loss: 0.248961\n",
      "Epoch: 008/025 | Batch 950/1000 | Loss: 0.202309\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 009/025 | Batch 000/1000 | Loss: 0.196652\n",
      "Epoch: 009/025 | Batch 050/1000 | Loss: 0.119434\n",
      "Epoch: 009/025 | Batch 100/1000 | Loss: 0.200723\n",
      "Epoch: 009/025 | Batch 150/1000 | Loss: 0.216939\n",
      "Epoch: 009/025 | Batch 200/1000 | Loss: 0.185240\n",
      "Epoch: 009/025 | Batch 250/1000 | Loss: 0.184794\n",
      "Epoch: 009/025 | Batch 300/1000 | Loss: 0.154314\n",
      "Epoch: 009/025 | Batch 350/1000 | Loss: 0.137957\n",
      "Epoch: 009/025 | Batch 400/1000 | Loss: 0.213426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009/025 | Batch 450/1000 | Loss: 0.194555\n",
      "Epoch: 009/025 | Batch 500/1000 | Loss: 0.153906\n",
      "Epoch: 009/025 | Batch 550/1000 | Loss: 0.236931\n",
      "Epoch: 009/025 | Batch 600/1000 | Loss: 0.119789\n",
      "Epoch: 009/025 | Batch 650/1000 | Loss: 0.125927\n",
      "Epoch: 009/025 | Batch 700/1000 | Loss: 0.162735\n",
      "Epoch: 009/025 | Batch 750/1000 | Loss: 0.146838\n",
      "Epoch: 009/025 | Batch 800/1000 | Loss: 0.111956\n",
      "Epoch: 009/025 | Batch 850/1000 | Loss: 0.186944\n",
      "Epoch: 009/025 | Batch 900/1000 | Loss: 0.199765\n",
      "Epoch: 009/025 | Batch 950/1000 | Loss: 0.158631\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 010/025 | Batch 000/1000 | Loss: 0.120475\n",
      "Epoch: 010/025 | Batch 050/1000 | Loss: 0.112982\n",
      "Epoch: 010/025 | Batch 100/1000 | Loss: 0.148654\n",
      "Epoch: 010/025 | Batch 150/1000 | Loss: 0.219998\n",
      "Epoch: 010/025 | Batch 200/1000 | Loss: 0.177494\n",
      "Epoch: 010/025 | Batch 250/1000 | Loss: 0.110282\n",
      "Epoch: 010/025 | Batch 300/1000 | Loss: 0.120411\n",
      "Epoch: 010/025 | Batch 350/1000 | Loss: 0.107254\n",
      "Epoch: 010/025 | Batch 400/1000 | Loss: 0.108158\n",
      "Epoch: 010/025 | Batch 450/1000 | Loss: 0.141480\n",
      "Epoch: 010/025 | Batch 500/1000 | Loss: 0.093895\n",
      "Epoch: 010/025 | Batch 550/1000 | Loss: 0.137426\n",
      "Epoch: 010/025 | Batch 600/1000 | Loss: 0.082067\n",
      "Epoch: 010/025 | Batch 650/1000 | Loss: 0.076077\n",
      "Epoch: 010/025 | Batch 700/1000 | Loss: 0.131443\n",
      "Epoch: 010/025 | Batch 750/1000 | Loss: 0.091255\n",
      "Epoch: 010/025 | Batch 800/1000 | Loss: 0.088300\n",
      "Epoch: 010/025 | Batch 850/1000 | Loss: 0.100068\n",
      "Epoch: 010/025 | Batch 900/1000 | Loss: 0.129908\n",
      "Epoch: 010/025 | Batch 950/1000 | Loss: 0.088328\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 011/025 | Batch 000/1000 | Loss: 0.088758\n",
      "Epoch: 011/025 | Batch 050/1000 | Loss: 0.066214\n",
      "Epoch: 011/025 | Batch 100/1000 | Loss: 0.097494\n",
      "Epoch: 011/025 | Batch 150/1000 | Loss: 0.108764\n",
      "Epoch: 011/025 | Batch 200/1000 | Loss: 0.075730\n",
      "Epoch: 011/025 | Batch 250/1000 | Loss: 0.068539\n",
      "Epoch: 011/025 | Batch 300/1000 | Loss: 0.070299\n",
      "Epoch: 011/025 | Batch 350/1000 | Loss: 0.058716\n",
      "Epoch: 011/025 | Batch 400/1000 | Loss: 0.074056\n",
      "Epoch: 011/025 | Batch 450/1000 | Loss: 0.084451\n",
      "Epoch: 011/025 | Batch 500/1000 | Loss: 0.058235\n",
      "Epoch: 011/025 | Batch 550/1000 | Loss: 0.122718\n",
      "Epoch: 011/025 | Batch 600/1000 | Loss: 0.051881\n",
      "Epoch: 011/025 | Batch 650/1000 | Loss: 0.052619\n",
      "Epoch: 011/025 | Batch 700/1000 | Loss: 0.090375\n",
      "Epoch: 011/025 | Batch 750/1000 | Loss: 0.036838\n",
      "Epoch: 011/025 | Batch 800/1000 | Loss: 0.054225\n",
      "Epoch: 011/025 | Batch 850/1000 | Loss: 0.069509\n",
      "Epoch: 011/025 | Batch 900/1000 | Loss: 0.069349\n",
      "Epoch: 011/025 | Batch 950/1000 | Loss: 0.047980\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 012/025 | Batch 000/1000 | Loss: 0.065311\n",
      "Epoch: 012/025 | Batch 050/1000 | Loss: 0.041041\n",
      "Epoch: 012/025 | Batch 100/1000 | Loss: 0.062123\n",
      "Epoch: 012/025 | Batch 150/1000 | Loss: 0.052044\n",
      "Epoch: 012/025 | Batch 200/1000 | Loss: 0.054670\n",
      "Epoch: 012/025 | Batch 250/1000 | Loss: 0.054611\n",
      "Epoch: 012/025 | Batch 300/1000 | Loss: 0.052215\n",
      "Epoch: 012/025 | Batch 350/1000 | Loss: 0.036563\n",
      "Epoch: 012/025 | Batch 400/1000 | Loss: 0.038346\n",
      "Epoch: 012/025 | Batch 450/1000 | Loss: 0.070532\n",
      "Epoch: 012/025 | Batch 500/1000 | Loss: 0.050640\n",
      "Epoch: 012/025 | Batch 550/1000 | Loss: 0.107595\n",
      "Epoch: 012/025 | Batch 600/1000 | Loss: 0.023067\n",
      "Epoch: 012/025 | Batch 650/1000 | Loss: 0.038802\n",
      "Epoch: 012/025 | Batch 700/1000 | Loss: 0.081034\n",
      "Epoch: 012/025 | Batch 750/1000 | Loss: 0.042311\n",
      "Epoch: 012/025 | Batch 800/1000 | Loss: 0.037266\n",
      "Epoch: 012/025 | Batch 850/1000 | Loss: 0.054812\n",
      "Epoch: 012/025 | Batch 900/1000 | Loss: 0.065635\n",
      "Epoch: 012/025 | Batch 950/1000 | Loss: 0.041565\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 013/025 | Batch 000/1000 | Loss: 0.064620\n",
      "Epoch: 013/025 | Batch 050/1000 | Loss: 0.030754\n",
      "Epoch: 013/025 | Batch 100/1000 | Loss: 0.050357\n",
      "Epoch: 013/025 | Batch 150/1000 | Loss: 0.057416\n",
      "Epoch: 013/025 | Batch 200/1000 | Loss: 0.063070\n",
      "Epoch: 013/025 | Batch 250/1000 | Loss: 0.083370\n",
      "Epoch: 013/025 | Batch 300/1000 | Loss: 0.037066\n",
      "Epoch: 013/025 | Batch 350/1000 | Loss: 0.028118\n",
      "Epoch: 013/025 | Batch 400/1000 | Loss: 0.029763\n",
      "Epoch: 013/025 | Batch 450/1000 | Loss: 0.056816\n",
      "Epoch: 013/025 | Batch 500/1000 | Loss: 0.050452\n",
      "Epoch: 013/025 | Batch 550/1000 | Loss: 0.096262\n",
      "Epoch: 013/025 | Batch 600/1000 | Loss: 0.025072\n",
      "Epoch: 013/025 | Batch 650/1000 | Loss: 0.062902\n",
      "Epoch: 013/025 | Batch 700/1000 | Loss: 0.042482\n",
      "Epoch: 013/025 | Batch 750/1000 | Loss: 0.059230\n",
      "Epoch: 013/025 | Batch 800/1000 | Loss: 0.083133\n",
      "Epoch: 013/025 | Batch 850/1000 | Loss: 0.102210\n",
      "Epoch: 013/025 | Batch 900/1000 | Loss: 0.056670\n",
      "Epoch: 013/025 | Batch 950/1000 | Loss: 0.052336\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 014/025 | Batch 000/1000 | Loss: 0.165624\n",
      "Epoch: 014/025 | Batch 050/1000 | Loss: 0.073564\n",
      "Epoch: 014/025 | Batch 100/1000 | Loss: 0.054213\n",
      "Epoch: 014/025 | Batch 150/1000 | Loss: 0.032373\n",
      "Epoch: 014/025 | Batch 200/1000 | Loss: 0.097831\n",
      "Epoch: 014/025 | Batch 250/1000 | Loss: 0.131485\n",
      "Epoch: 014/025 | Batch 300/1000 | Loss: 0.071614\n",
      "Epoch: 014/025 | Batch 350/1000 | Loss: 0.088079\n",
      "Epoch: 014/025 | Batch 400/1000 | Loss: 0.033352\n",
      "Epoch: 014/025 | Batch 450/1000 | Loss: 0.064057\n",
      "Epoch: 014/025 | Batch 500/1000 | Loss: 0.172896\n",
      "Epoch: 014/025 | Batch 550/1000 | Loss: 0.119453\n",
      "Epoch: 014/025 | Batch 600/1000 | Loss: 0.029862\n",
      "Epoch: 014/025 | Batch 650/1000 | Loss: 0.038959\n",
      "Epoch: 014/025 | Batch 700/1000 | Loss: 0.046092\n",
      "Epoch: 014/025 | Batch 750/1000 | Loss: 0.047288\n",
      "Epoch: 014/025 | Batch 800/1000 | Loss: 0.029591\n",
      "Epoch: 014/025 | Batch 850/1000 | Loss: 0.191980\n",
      "Epoch: 014/025 | Batch 900/1000 | Loss: 0.045581\n",
      "Epoch: 014/025 | Batch 950/1000 | Loss: 0.082316\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 015/025 | Batch 000/1000 | Loss: 0.052485\n",
      "Epoch: 015/025 | Batch 050/1000 | Loss: 0.084776\n",
      "Epoch: 015/025 | Batch 100/1000 | Loss: 0.102713\n",
      "Epoch: 015/025 | Batch 150/1000 | Loss: 0.118343\n",
      "Epoch: 015/025 | Batch 200/1000 | Loss: 0.033921\n",
      "Epoch: 015/025 | Batch 250/1000 | Loss: 0.080397\n",
      "Epoch: 015/025 | Batch 300/1000 | Loss: 0.193462\n",
      "Epoch: 015/025 | Batch 350/1000 | Loss: 0.125579\n",
      "Epoch: 015/025 | Batch 400/1000 | Loss: 0.124769\n",
      "Epoch: 015/025 | Batch 450/1000 | Loss: 0.079549\n",
      "Epoch: 015/025 | Batch 500/1000 | Loss: 0.058437\n",
      "Epoch: 015/025 | Batch 550/1000 | Loss: 0.198121\n",
      "Epoch: 015/025 | Batch 600/1000 | Loss: 0.141651\n",
      "Epoch: 015/025 | Batch 650/1000 | Loss: 0.151551\n",
      "Epoch: 015/025 | Batch 700/1000 | Loss: 0.175232\n",
      "Epoch: 015/025 | Batch 750/1000 | Loss: 0.029616\n",
      "Epoch: 015/025 | Batch 800/1000 | Loss: 0.057699\n",
      "Epoch: 015/025 | Batch 850/1000 | Loss: 0.242459\n",
      "Epoch: 015/025 | Batch 900/1000 | Loss: 0.207569\n",
      "Epoch: 015/025 | Batch 950/1000 | Loss: 0.125479\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 016/025 | Batch 000/1000 | Loss: 0.045964\n",
      "Epoch: 016/025 | Batch 050/1000 | Loss: 0.066138\n",
      "Epoch: 016/025 | Batch 100/1000 | Loss: 0.401715\n",
      "Epoch: 016/025 | Batch 150/1000 | Loss: 0.229349\n",
      "Epoch: 016/025 | Batch 200/1000 | Loss: 0.097482\n",
      "Epoch: 016/025 | Batch 250/1000 | Loss: 0.069744\n",
      "Epoch: 016/025 | Batch 300/1000 | Loss: 0.071920\n",
      "Epoch: 016/025 | Batch 350/1000 | Loss: 0.157881\n",
      "Epoch: 016/025 | Batch 400/1000 | Loss: 0.212399\n",
      "Epoch: 016/025 | Batch 450/1000 | Loss: 0.108076\n",
      "Epoch: 016/025 | Batch 500/1000 | Loss: 0.093388\n",
      "Epoch: 016/025 | Batch 550/1000 | Loss: 0.277422\n",
      "Epoch: 016/025 | Batch 600/1000 | Loss: 0.178820\n",
      "Epoch: 016/025 | Batch 650/1000 | Loss: 0.147358\n",
      "Epoch: 016/025 | Batch 700/1000 | Loss: 0.212851\n",
      "Epoch: 016/025 | Batch 750/1000 | Loss: 0.066359\n",
      "Epoch: 016/025 | Batch 800/1000 | Loss: 0.153533\n",
      "Epoch: 016/025 | Batch 850/1000 | Loss: 0.091051\n",
      "Epoch: 016/025 | Batch 900/1000 | Loss: 0.151686\n",
      "Epoch: 016/025 | Batch 950/1000 | Loss: 0.244079\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 017/025 | Batch 000/1000 | Loss: 0.145214\n",
      "Epoch: 017/025 | Batch 050/1000 | Loss: 0.166802\n",
      "Epoch: 017/025 | Batch 100/1000 | Loss: 0.075951\n",
      "Epoch: 017/025 | Batch 150/1000 | Loss: 0.134567\n",
      "Epoch: 017/025 | Batch 200/1000 | Loss: 0.138647\n",
      "Epoch: 017/025 | Batch 250/1000 | Loss: 0.106875\n",
      "Epoch: 017/025 | Batch 300/1000 | Loss: 0.186777\n",
      "Epoch: 017/025 | Batch 350/1000 | Loss: 0.080077\n",
      "Epoch: 017/025 | Batch 400/1000 | Loss: 0.118141\n",
      "Epoch: 017/025 | Batch 450/1000 | Loss: 0.314828\n",
      "Epoch: 017/025 | Batch 500/1000 | Loss: 0.211164\n",
      "Epoch: 017/025 | Batch 550/1000 | Loss: 0.187892\n",
      "Epoch: 017/025 | Batch 600/1000 | Loss: 0.039859\n",
      "Epoch: 017/025 | Batch 650/1000 | Loss: 0.151803\n",
      "Epoch: 017/025 | Batch 700/1000 | Loss: 0.642177\n",
      "Epoch: 017/025 | Batch 750/1000 | Loss: 0.345807\n",
      "Epoch: 017/025 | Batch 800/1000 | Loss: 0.182915\n",
      "Epoch: 017/025 | Batch 850/1000 | Loss: 0.030602\n",
      "Epoch: 017/025 | Batch 900/1000 | Loss: 0.117774\n",
      "Epoch: 017/025 | Batch 950/1000 | Loss: 0.183346\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 018/025 | Batch 000/1000 | Loss: 0.416977\n",
      "Epoch: 018/025 | Batch 050/1000 | Loss: 0.135221\n",
      "Epoch: 018/025 | Batch 100/1000 | Loss: 0.213860\n",
      "Epoch: 018/025 | Batch 150/1000 | Loss: 0.154614\n",
      "Epoch: 018/025 | Batch 200/1000 | Loss: 0.149409\n",
      "Epoch: 018/025 | Batch 250/1000 | Loss: 0.276462\n",
      "Epoch: 018/025 | Batch 300/1000 | Loss: 0.371965\n",
      "Epoch: 018/025 | Batch 350/1000 | Loss: 0.044254\n",
      "Epoch: 018/025 | Batch 400/1000 | Loss: 0.107198\n",
      "Epoch: 018/025 | Batch 450/1000 | Loss: 0.248488\n",
      "Epoch: 018/025 | Batch 500/1000 | Loss: 0.111895\n",
      "Epoch: 018/025 | Batch 550/1000 | Loss: 0.136689\n",
      "Epoch: 018/025 | Batch 600/1000 | Loss: 0.078538\n",
      "Epoch: 018/025 | Batch 650/1000 | Loss: 0.115582\n",
      "Epoch: 018/025 | Batch 700/1000 | Loss: 0.167577\n",
      "Epoch: 018/025 | Batch 750/1000 | Loss: 0.216116\n",
      "Epoch: 018/025 | Batch 800/1000 | Loss: 0.222654\n",
      "Epoch: 018/025 | Batch 850/1000 | Loss: 0.067241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018/025 | Batch 900/1000 | Loss: 0.162826\n",
      "Epoch: 018/025 | Batch 950/1000 | Loss: 0.058535\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 019/025 | Batch 000/1000 | Loss: 0.090196\n",
      "Epoch: 019/025 | Batch 050/1000 | Loss: 0.180054\n",
      "Epoch: 019/025 | Batch 100/1000 | Loss: 0.187399\n",
      "Epoch: 019/025 | Batch 150/1000 | Loss: 0.268780\n",
      "Epoch: 019/025 | Batch 200/1000 | Loss: 0.153037\n",
      "Epoch: 019/025 | Batch 250/1000 | Loss: 0.057923\n",
      "Epoch: 019/025 | Batch 300/1000 | Loss: 0.088001\n",
      "Epoch: 019/025 | Batch 350/1000 | Loss: 0.029622\n",
      "Epoch: 019/025 | Batch 400/1000 | Loss: 0.166660\n",
      "Epoch: 019/025 | Batch 450/1000 | Loss: 0.121422\n",
      "Epoch: 019/025 | Batch 500/1000 | Loss: 0.031776\n",
      "Epoch: 019/025 | Batch 550/1000 | Loss: 0.063452\n",
      "Epoch: 019/025 | Batch 600/1000 | Loss: 0.044235\n",
      "Epoch: 019/025 | Batch 650/1000 | Loss: 0.070451\n",
      "Epoch: 019/025 | Batch 700/1000 | Loss: 0.093860\n",
      "Epoch: 019/025 | Batch 750/1000 | Loss: 0.035969\n",
      "Epoch: 019/025 | Batch 800/1000 | Loss: 0.094352\n",
      "Epoch: 019/025 | Batch 850/1000 | Loss: 0.035463\n",
      "Epoch: 019/025 | Batch 900/1000 | Loss: 0.123830\n",
      "Epoch: 019/025 | Batch 950/1000 | Loss: 0.140657\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 020/025 | Batch 000/1000 | Loss: 0.054567\n",
      "Epoch: 020/025 | Batch 050/1000 | Loss: 0.022433\n",
      "Epoch: 020/025 | Batch 100/1000 | Loss: 0.132545\n",
      "Epoch: 020/025 | Batch 150/1000 | Loss: 0.090221\n",
      "Epoch: 020/025 | Batch 200/1000 | Loss: 0.097058\n",
      "Epoch: 020/025 | Batch 250/1000 | Loss: 0.074770\n",
      "Epoch: 020/025 | Batch 300/1000 | Loss: 0.046175\n",
      "Epoch: 020/025 | Batch 350/1000 | Loss: 0.077064\n",
      "Epoch: 020/025 | Batch 400/1000 | Loss: 0.041025\n",
      "Epoch: 020/025 | Batch 450/1000 | Loss: 0.015499\n",
      "Epoch: 020/025 | Batch 500/1000 | Loss: 0.014712\n",
      "Epoch: 020/025 | Batch 550/1000 | Loss: 0.065826\n",
      "Epoch: 020/025 | Batch 600/1000 | Loss: 0.034465\n",
      "Epoch: 020/025 | Batch 650/1000 | Loss: 0.073215\n",
      "Epoch: 020/025 | Batch 700/1000 | Loss: 0.025742\n",
      "Epoch: 020/025 | Batch 750/1000 | Loss: 0.057161\n",
      "Epoch: 020/025 | Batch 800/1000 | Loss: 0.053496\n",
      "Epoch: 020/025 | Batch 850/1000 | Loss: 0.026796\n",
      "Epoch: 020/025 | Batch 900/1000 | Loss: 0.042596\n",
      "Epoch: 020/025 | Batch 950/1000 | Loss: 0.035207\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 021/025 | Batch 000/1000 | Loss: 0.020357\n",
      "Epoch: 021/025 | Batch 050/1000 | Loss: 0.028428\n",
      "Epoch: 021/025 | Batch 100/1000 | Loss: 0.023698\n",
      "Epoch: 021/025 | Batch 150/1000 | Loss: 0.031509\n",
      "Epoch: 021/025 | Batch 200/1000 | Loss: 0.049355\n",
      "Epoch: 021/025 | Batch 250/1000 | Loss: 0.073042\n",
      "Epoch: 021/025 | Batch 300/1000 | Loss: 0.028604\n",
      "Epoch: 021/025 | Batch 350/1000 | Loss: 0.048127\n",
      "Epoch: 021/025 | Batch 400/1000 | Loss: 0.087174\n",
      "Epoch: 021/025 | Batch 450/1000 | Loss: 0.071884\n",
      "Epoch: 021/025 | Batch 500/1000 | Loss: 0.008970\n",
      "Epoch: 021/025 | Batch 550/1000 | Loss: 0.011933\n",
      "Epoch: 021/025 | Batch 600/1000 | Loss: 0.014329\n",
      "Epoch: 021/025 | Batch 650/1000 | Loss: 0.060123\n",
      "Epoch: 021/025 | Batch 700/1000 | Loss: 0.060993\n",
      "Epoch: 021/025 | Batch 750/1000 | Loss: 0.027395\n",
      "Epoch: 021/025 | Batch 800/1000 | Loss: 0.041260\n",
      "Epoch: 021/025 | Batch 850/1000 | Loss: 0.029449\n",
      "Epoch: 021/025 | Batch 900/1000 | Loss: 0.044510\n",
      "Epoch: 021/025 | Batch 950/1000 | Loss: 0.031423\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 022/025 | Batch 000/1000 | Loss: 0.037999\n",
      "Epoch: 022/025 | Batch 050/1000 | Loss: 0.013211\n",
      "Epoch: 022/025 | Batch 100/1000 | Loss: 0.013181\n",
      "Epoch: 022/025 | Batch 150/1000 | Loss: 0.006774\n",
      "Epoch: 022/025 | Batch 200/1000 | Loss: 0.024284\n",
      "Epoch: 022/025 | Batch 250/1000 | Loss: 0.032863\n",
      "Epoch: 022/025 | Batch 300/1000 | Loss: 0.101154\n",
      "Epoch: 022/025 | Batch 350/1000 | Loss: 0.061842\n",
      "Epoch: 022/025 | Batch 400/1000 | Loss: 0.105646\n",
      "Epoch: 022/025 | Batch 450/1000 | Loss: 0.064166\n",
      "Epoch: 022/025 | Batch 500/1000 | Loss: 0.069960\n",
      "Epoch: 022/025 | Batch 550/1000 | Loss: 0.054505\n",
      "Epoch: 022/025 | Batch 600/1000 | Loss: 0.018958\n",
      "Epoch: 022/025 | Batch 650/1000 | Loss: 0.027998\n",
      "Epoch: 022/025 | Batch 700/1000 | Loss: 0.053168\n",
      "Epoch: 022/025 | Batch 750/1000 | Loss: 0.048263\n",
      "Epoch: 022/025 | Batch 800/1000 | Loss: 0.250849\n",
      "Epoch: 022/025 | Batch 850/1000 | Loss: 0.072096\n",
      "Epoch: 022/025 | Batch 900/1000 | Loss: 0.130097\n",
      "Epoch: 022/025 | Batch 950/1000 | Loss: 0.034708\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 023/025 | Batch 000/1000 | Loss: 0.048956\n",
      "Epoch: 023/025 | Batch 050/1000 | Loss: 0.029982\n",
      "Epoch: 023/025 | Batch 100/1000 | Loss: 0.011836\n",
      "Epoch: 023/025 | Batch 150/1000 | Loss: 0.015804\n",
      "Epoch: 023/025 | Batch 200/1000 | Loss: 0.037873\n",
      "Epoch: 023/025 | Batch 250/1000 | Loss: 0.028915\n",
      "Epoch: 023/025 | Batch 300/1000 | Loss: 0.029752\n",
      "Epoch: 023/025 | Batch 350/1000 | Loss: 0.103660\n",
      "Epoch: 023/025 | Batch 400/1000 | Loss: 0.031211\n",
      "Epoch: 023/025 | Batch 450/1000 | Loss: 0.038806\n",
      "Epoch: 023/025 | Batch 500/1000 | Loss: 0.085135\n",
      "Epoch: 023/025 | Batch 550/1000 | Loss: 0.115954\n",
      "Epoch: 023/025 | Batch 600/1000 | Loss: 0.062154\n",
      "Epoch: 023/025 | Batch 650/1000 | Loss: 0.015021\n",
      "Epoch: 023/025 | Batch 700/1000 | Loss: 0.035201\n",
      "Epoch: 023/025 | Batch 750/1000 | Loss: 0.012899\n",
      "Epoch: 023/025 | Batch 800/1000 | Loss: 0.059036\n",
      "Epoch: 023/025 | Batch 850/1000 | Loss: 0.042190\n",
      "Epoch: 023/025 | Batch 900/1000 | Loss: 0.085736\n",
      "Epoch: 023/025 | Batch 950/1000 | Loss: 0.165093\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 024/025 | Batch 000/1000 | Loss: 0.079419\n",
      "Epoch: 024/025 | Batch 050/1000 | Loss: 0.013619\n",
      "Epoch: 024/025 | Batch 100/1000 | Loss: 0.017601\n",
      "Epoch: 024/025 | Batch 150/1000 | Loss: 0.021249\n",
      "Epoch: 024/025 | Batch 200/1000 | Loss: 0.081338\n",
      "Epoch: 024/025 | Batch 250/1000 | Loss: 0.081096\n",
      "Epoch: 024/025 | Batch 300/1000 | Loss: 0.064317\n",
      "Epoch: 024/025 | Batch 350/1000 | Loss: 0.013470\n",
      "Epoch: 024/025 | Batch 400/1000 | Loss: 0.027072\n",
      "Epoch: 024/025 | Batch 450/1000 | Loss: 0.027200\n",
      "Epoch: 024/025 | Batch 500/1000 | Loss: 0.034155\n",
      "Epoch: 024/025 | Batch 550/1000 | Loss: 0.033715\n",
      "Epoch: 024/025 | Batch 600/1000 | Loss: 0.061834\n",
      "Epoch: 024/025 | Batch 650/1000 | Loss: 0.010300\n",
      "Epoch: 024/025 | Batch 700/1000 | Loss: 0.029271\n",
      "Epoch: 024/025 | Batch 750/1000 | Loss: 0.006871\n",
      "Epoch: 024/025 | Batch 800/1000 | Loss: 0.027336\n",
      "Epoch: 024/025 | Batch 850/1000 | Loss: 0.006723\n",
      "Epoch: 024/025 | Batch 900/1000 | Loss: 0.050669\n",
      "Epoch: 024/025 | Batch 950/1000 | Loss: 0.081496\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 025/025 | Batch 000/1000 | Loss: 0.035144\n",
      "Epoch: 025/025 | Batch 050/1000 | Loss: 0.093892\n",
      "Epoch: 025/025 | Batch 100/1000 | Loss: 0.130786\n",
      "Epoch: 025/025 | Batch 150/1000 | Loss: 0.042816\n",
      "Epoch: 025/025 | Batch 200/1000 | Loss: 0.012884\n",
      "Epoch: 025/025 | Batch 250/1000 | Loss: 0.005995\n",
      "Epoch: 025/025 | Batch 300/1000 | Loss: 0.005840\n",
      "Epoch: 025/025 | Batch 350/1000 | Loss: 0.041691\n",
      "Epoch: 025/025 | Batch 400/1000 | Loss: 0.104026\n",
      "Epoch: 025/025 | Batch 450/1000 | Loss: 0.073024\n",
      "Epoch: 025/025 | Batch 500/1000 | Loss: 0.066557\n",
      "Epoch: 025/025 | Batch 550/1000 | Loss: 0.041940\n",
      "Epoch: 025/025 | Batch 600/1000 | Loss: 0.055422\n",
      "Epoch: 025/025 | Batch 650/1000 | Loss: 0.023181\n",
      "Epoch: 025/025 | Batch 700/1000 | Loss: 0.026126\n",
      "Epoch: 025/025 | Batch 750/1000 | Loss: 0.008593\n",
      "Epoch: 025/025 | Batch 800/1000 | Loss: 0.005768\n",
      "Epoch: 025/025 | Batch 850/1000 | Loss: 0.005140\n",
      "Epoch: 025/025 | Batch 900/1000 | Loss: 0.055166\n",
      "Epoch: 025/025 | Batch 950/1000 | Loss: 0.027716\n",
      "Time elapsed: 0.06 min\n",
      "Total Training Time: 0.06 min\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "my_model.trainer(train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error : 6.0% \n",
      "Test error : 47.8%\n",
      "The total number of the parameters is: 76194\n"
     ]
    }
   ],
   "source": [
    "# output the train error and test error\n",
    "print(\"Train error : %.1f%% \\nTest error : %.1f%%\" %\n",
    "      (my_model.compute_error(train_input, train_target),\n",
    "       my_model.compute_error(test_input, test_target)))\n",
    "\n",
    "print(\"The total number of the parameters is: %d\" % (sum(p.numel() for p in my_model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
