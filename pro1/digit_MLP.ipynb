{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#直接导入出现http403错误\n",
    "from six.moves import urllib\n",
    "# 直接导入出现http403错误\n",
    "# have to add a header to your urllib request (due to that site moving to Cloudflare protection)\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "#*********************** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PAIRS = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(train_classes.shape)\n",
    "print(train_target.shape)\n",
    "print(test_input.shape)\n",
    "print(test_classes.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 14])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_input[0][0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_input[0][1],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000, 392])\n"
     ]
    }
   ],
   "source": [
    "# 将两个图片拼接成一个1*392的tensor，为进入MLP网络进行准备。\n",
    "print(train_input.shape)\n",
    "tran_train_input=train_input.view(-1,2*14*14)\n",
    "print(tran_train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 392])\n"
     ]
    }
   ],
   "source": [
    "#同理对测试数据集进行相同的操作\n",
    "tran_test_input=test_input.view(-1,2*14*14)\n",
    "print(tran_test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP_Net(nn.Module):\n",
    "#     def __init__(self, num_hidden_1,num_hidden_2,num_hidden_3,num_hidden_4):\n",
    "#         super(Test_MLPmodel, self).__init__()\n",
    "#         self.linear1 = nn.Linear(2*14*14, 300)\n",
    "#         self.linear2 = nn.Linear(300, 200)\n",
    "#         self.linear3 = nn.Linear (200, 100)\n",
    "#         self.linear4 = nn.Linear (100, 50)\n",
    "#         self.linear_out = nn.Linear(num_hidden_4,2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.linear1(x.view(-1,2*14*14)))\n",
    "#         x = F.relu(self.linear2(x))\n",
    "#         x = F.relu(self.linear3(x))\n",
    "#         x = F.relu(self.linear4(x))\n",
    "#         x = self.linear_out(x)       \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Net, self).__init__()\n",
    "        self.linear1 = nn.Linear(2*14*14, 300)\n",
    "        self.linear2 = nn.Linear(300, 200)\n",
    "        self.linear3 = nn.Linear (200, 100)\n",
    "        self.linear4 = nn.Linear (100, 50)\n",
    "        self.linear5 = nn.Linear (50, 20)\n",
    "        self.linear_out = nn.Linear(20,2)\n",
    "        # training parameter\n",
    "        self.batch_size = 20\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.num_epochs = 25\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x.view(-1,2*14*14)))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = F.relu(self.linear5(x))\n",
    "        x = self.linear_out(x)       \n",
    "        return x\n",
    "     # Training Function\n",
    "\n",
    "    def trainer(self, train_input, train_target):\n",
    "        \"\"\"\n",
    "        Train the model on a training set\n",
    "        :param train_input: Training features\n",
    "        :param train_target: Training labels\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch_idx in range(0,train_input.size(0),self.batch_size):\n",
    "                output = self(train_input[batch_idx:batch_idx+self.batch_size]) \n",
    "                loss = self.criterion(output, train_target[batch_idx:batch_idx+self.batch_size])  \n",
    "                self.optimizer.zero_grad()                          #清零梯度\n",
    "                loss.backward()                                #反向求梯度\n",
    "                self.optimizer.step()\n",
    "\n",
    "                if not batch_idx % 50:\n",
    "                    print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.6f' \n",
    "                           %(epoch+1, self.num_epochs, batch_idx, \n",
    "                             len(train_input), loss))\n",
    "            print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "         # Test error\n",
    "\n",
    "    def compute_error(self, input_data, target):\n",
    "        \"\"\"\n",
    "        Compute the number of error of the model on a test set\n",
    "        :param input_data: test features\n",
    "        :param target: test target\n",
    "        :return: error rate of the input data\n",
    "        \"\"\"  \n",
    "    \n",
    "        #测试模型\n",
    "        self.eval()      #测试模式，关闭正则化\n",
    "        errors = 0\n",
    "        for idx in range(0,input_data.size(0),self.batch_size):\n",
    "            input_batch=input_data.narrow(0,idx,self.batch_size)\n",
    "            outputs = self(input_batch)\n",
    "            _, predicted = torch.max(outputs, 1)   #返回值和索引\n",
    "            target_labels = target.narrow(0, idx, self.batch_size)\n",
    "            errors += torch.sum(predicted != target_labels)\n",
    "\n",
    "        return float(errors)*100/input_data.size(0)\n",
    "    def save_model(self,model_name):\n",
    "        \"\"\"\n",
    "        Save the model to this folder\n",
    "        :param model_name: the model name, e.g. CNN_Net.pth\n",
    "        \"\"\"         \n",
    "        torch.save(self, './model/'+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MLP_Net()\n",
    "my_model.save_model('MLP_Net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/025 | Batch 000/1000 | Loss: 0.627910\n",
      "Epoch: 001/025 | Batch 100/1000 | Loss: 0.665862\n",
      "Epoch: 001/025 | Batch 200/1000 | Loss: 0.547835\n",
      "Epoch: 001/025 | Batch 300/1000 | Loss: 0.445861\n",
      "Epoch: 001/025 | Batch 400/1000 | Loss: 0.457809\n",
      "Epoch: 001/025 | Batch 500/1000 | Loss: 0.501272\n",
      "Epoch: 001/025 | Batch 600/1000 | Loss: 0.610885\n",
      "Epoch: 001/025 | Batch 700/1000 | Loss: 0.630210\n",
      "Epoch: 001/025 | Batch 800/1000 | Loss: 0.478961\n",
      "Epoch: 001/025 | Batch 900/1000 | Loss: 0.437284\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/025 | Batch 000/1000 | Loss: 0.344189\n",
      "Epoch: 002/025 | Batch 100/1000 | Loss: 0.331104\n",
      "Epoch: 002/025 | Batch 200/1000 | Loss: 0.342556\n",
      "Epoch: 002/025 | Batch 300/1000 | Loss: 0.400370\n",
      "Epoch: 002/025 | Batch 400/1000 | Loss: 0.274455\n",
      "Epoch: 002/025 | Batch 500/1000 | Loss: 0.286864\n",
      "Epoch: 002/025 | Batch 600/1000 | Loss: 0.429386\n",
      "Epoch: 002/025 | Batch 700/1000 | Loss: 0.682405\n",
      "Epoch: 002/025 | Batch 800/1000 | Loss: 0.324944\n",
      "Epoch: 002/025 | Batch 900/1000 | Loss: 0.270531\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 003/025 | Batch 000/1000 | Loss: 0.175593\n",
      "Epoch: 003/025 | Batch 100/1000 | Loss: 0.317450\n",
      "Epoch: 003/025 | Batch 200/1000 | Loss: 0.189727\n",
      "Epoch: 003/025 | Batch 300/1000 | Loss: 0.384621\n",
      "Epoch: 003/025 | Batch 400/1000 | Loss: 0.238240\n",
      "Epoch: 003/025 | Batch 500/1000 | Loss: 0.196804\n",
      "Epoch: 003/025 | Batch 600/1000 | Loss: 0.291297\n",
      "Epoch: 003/025 | Batch 700/1000 | Loss: 0.303372\n",
      "Epoch: 003/025 | Batch 800/1000 | Loss: 0.358711\n",
      "Epoch: 003/025 | Batch 900/1000 | Loss: 0.179952\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 004/025 | Batch 000/1000 | Loss: 0.186701\n",
      "Epoch: 004/025 | Batch 100/1000 | Loss: 0.237898\n",
      "Epoch: 004/025 | Batch 200/1000 | Loss: 0.186182\n",
      "Epoch: 004/025 | Batch 300/1000 | Loss: 0.105152\n",
      "Epoch: 004/025 | Batch 400/1000 | Loss: 0.210105\n",
      "Epoch: 004/025 | Batch 500/1000 | Loss: 0.087094\n",
      "Epoch: 004/025 | Batch 600/1000 | Loss: 0.223753\n",
      "Epoch: 004/025 | Batch 700/1000 | Loss: 0.167106\n",
      "Epoch: 004/025 | Batch 800/1000 | Loss: 0.211649\n",
      "Epoch: 004/025 | Batch 900/1000 | Loss: 0.168775\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/025 | Batch 000/1000 | Loss: 0.178504\n",
      "Epoch: 005/025 | Batch 100/1000 | Loss: 0.117339\n",
      "Epoch: 005/025 | Batch 200/1000 | Loss: 0.110892\n",
      "Epoch: 005/025 | Batch 300/1000 | Loss: 0.218429\n",
      "Epoch: 005/025 | Batch 400/1000 | Loss: 0.102889\n",
      "Epoch: 005/025 | Batch 500/1000 | Loss: 0.082681\n",
      "Epoch: 005/025 | Batch 600/1000 | Loss: 0.148122\n",
      "Epoch: 005/025 | Batch 700/1000 | Loss: 0.195327\n",
      "Epoch: 005/025 | Batch 800/1000 | Loss: 0.464924\n",
      "Epoch: 005/025 | Batch 900/1000 | Loss: 0.438722\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 006/025 | Batch 000/1000 | Loss: 0.043978\n",
      "Epoch: 006/025 | Batch 100/1000 | Loss: 0.096440\n",
      "Epoch: 006/025 | Batch 200/1000 | Loss: 0.154344\n",
      "Epoch: 006/025 | Batch 300/1000 | Loss: 0.065740\n",
      "Epoch: 006/025 | Batch 400/1000 | Loss: 0.034959\n",
      "Epoch: 006/025 | Batch 500/1000 | Loss: 0.014766\n",
      "Epoch: 006/025 | Batch 600/1000 | Loss: 0.094517\n",
      "Epoch: 006/025 | Batch 700/1000 | Loss: 0.056929\n",
      "Epoch: 006/025 | Batch 800/1000 | Loss: 0.133879\n",
      "Epoch: 006/025 | Batch 900/1000 | Loss: 0.146255\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 007/025 | Batch 000/1000 | Loss: 0.009057\n",
      "Epoch: 007/025 | Batch 100/1000 | Loss: 0.018314\n",
      "Epoch: 007/025 | Batch 200/1000 | Loss: 0.026608\n",
      "Epoch: 007/025 | Batch 300/1000 | Loss: 0.146165\n",
      "Epoch: 007/025 | Batch 400/1000 | Loss: 0.038322\n",
      "Epoch: 007/025 | Batch 500/1000 | Loss: 0.005953\n",
      "Epoch: 007/025 | Batch 600/1000 | Loss: 0.056987\n",
      "Epoch: 007/025 | Batch 700/1000 | Loss: 0.016795\n",
      "Epoch: 007/025 | Batch 800/1000 | Loss: 0.039554\n",
      "Epoch: 007/025 | Batch 900/1000 | Loss: 0.200735\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 008/025 | Batch 000/1000 | Loss: 0.044073\n",
      "Epoch: 008/025 | Batch 100/1000 | Loss: 0.434867\n",
      "Epoch: 008/025 | Batch 200/1000 | Loss: 0.019764\n",
      "Epoch: 008/025 | Batch 300/1000 | Loss: 0.177850\n",
      "Epoch: 008/025 | Batch 400/1000 | Loss: 0.059898\n",
      "Epoch: 008/025 | Batch 500/1000 | Loss: 0.140680\n",
      "Epoch: 008/025 | Batch 600/1000 | Loss: 0.178527\n",
      "Epoch: 008/025 | Batch 700/1000 | Loss: 0.084732\n",
      "Epoch: 008/025 | Batch 800/1000 | Loss: 0.271373\n",
      "Epoch: 008/025 | Batch 900/1000 | Loss: 0.062263\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 009/025 | Batch 000/1000 | Loss: 0.009758\n",
      "Epoch: 009/025 | Batch 100/1000 | Loss: 0.078551\n",
      "Epoch: 009/025 | Batch 200/1000 | Loss: 0.054337\n",
      "Epoch: 009/025 | Batch 300/1000 | Loss: 0.049168\n",
      "Epoch: 009/025 | Batch 400/1000 | Loss: 0.100998\n",
      "Epoch: 009/025 | Batch 500/1000 | Loss: 0.221566\n",
      "Epoch: 009/025 | Batch 600/1000 | Loss: 0.024828\n",
      "Epoch: 009/025 | Batch 700/1000 | Loss: 0.489402\n",
      "Epoch: 009/025 | Batch 800/1000 | Loss: 0.170702\n",
      "Epoch: 009/025 | Batch 900/1000 | Loss: 0.033120\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 010/025 | Batch 000/1000 | Loss: 0.289323\n",
      "Epoch: 010/025 | Batch 100/1000 | Loss: 0.057761\n",
      "Epoch: 010/025 | Batch 200/1000 | Loss: 0.092355\n",
      "Epoch: 010/025 | Batch 300/1000 | Loss: 0.017246\n",
      "Epoch: 010/025 | Batch 400/1000 | Loss: 0.168489\n",
      "Epoch: 010/025 | Batch 500/1000 | Loss: 0.099800\n",
      "Epoch: 010/025 | Batch 600/1000 | Loss: 0.318467\n",
      "Epoch: 010/025 | Batch 700/1000 | Loss: 0.065080\n",
      "Epoch: 010/025 | Batch 800/1000 | Loss: 0.042902\n",
      "Epoch: 010/025 | Batch 900/1000 | Loss: 0.048822\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 011/025 | Batch 000/1000 | Loss: 0.009104\n",
      "Epoch: 011/025 | Batch 100/1000 | Loss: 0.017835\n",
      "Epoch: 011/025 | Batch 200/1000 | Loss: 0.017528\n",
      "Epoch: 011/025 | Batch 300/1000 | Loss: 0.027059\n",
      "Epoch: 011/025 | Batch 400/1000 | Loss: 0.002676\n",
      "Epoch: 011/025 | Batch 500/1000 | Loss: 0.000204\n",
      "Epoch: 011/025 | Batch 600/1000 | Loss: 0.083316\n",
      "Epoch: 011/025 | Batch 700/1000 | Loss: 0.181091\n",
      "Epoch: 011/025 | Batch 800/1000 | Loss: 0.039878\n",
      "Epoch: 011/025 | Batch 900/1000 | Loss: 0.027688\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 012/025 | Batch 000/1000 | Loss: 0.000385\n",
      "Epoch: 012/025 | Batch 100/1000 | Loss: 0.006424\n",
      "Epoch: 012/025 | Batch 200/1000 | Loss: 0.002006\n",
      "Epoch: 012/025 | Batch 300/1000 | Loss: 0.058834\n",
      "Epoch: 012/025 | Batch 400/1000 | Loss: 0.330082\n",
      "Epoch: 012/025 | Batch 500/1000 | Loss: 0.048211\n",
      "Epoch: 012/025 | Batch 600/1000 | Loss: 0.065289\n",
      "Epoch: 012/025 | Batch 700/1000 | Loss: 0.012973\n",
      "Epoch: 012/025 | Batch 800/1000 | Loss: 0.015184\n",
      "Epoch: 012/025 | Batch 900/1000 | Loss: 0.078434\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 013/025 | Batch 000/1000 | Loss: 0.000252\n",
      "Epoch: 013/025 | Batch 100/1000 | Loss: 0.007449\n",
      "Epoch: 013/025 | Batch 200/1000 | Loss: 0.004303\n",
      "Epoch: 013/025 | Batch 300/1000 | Loss: 0.003300\n",
      "Epoch: 013/025 | Batch 400/1000 | Loss: 0.013426\n",
      "Epoch: 013/025 | Batch 500/1000 | Loss: 0.009661\n",
      "Epoch: 013/025 | Batch 600/1000 | Loss: 0.013363\n",
      "Epoch: 013/025 | Batch 700/1000 | Loss: 0.029606\n",
      "Epoch: 013/025 | Batch 800/1000 | Loss: 0.002747\n",
      "Epoch: 013/025 | Batch 900/1000 | Loss: 0.014888\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 014/025 | Batch 000/1000 | Loss: 0.000110\n",
      "Epoch: 014/025 | Batch 100/1000 | Loss: 0.001555\n",
      "Epoch: 014/025 | Batch 200/1000 | Loss: 0.023997\n",
      "Epoch: 014/025 | Batch 300/1000 | Loss: 0.015204\n",
      "Epoch: 014/025 | Batch 400/1000 | Loss: 0.012744\n",
      "Epoch: 014/025 | Batch 500/1000 | Loss: 0.035305\n",
      "Epoch: 014/025 | Batch 600/1000 | Loss: 0.010234\n",
      "Epoch: 014/025 | Batch 700/1000 | Loss: 0.008463\n",
      "Epoch: 014/025 | Batch 800/1000 | Loss: 0.004366\n",
      "Epoch: 014/025 | Batch 900/1000 | Loss: 0.210236\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 015/025 | Batch 000/1000 | Loss: 0.000065\n",
      "Epoch: 015/025 | Batch 100/1000 | Loss: 0.002681\n",
      "Epoch: 015/025 | Batch 200/1000 | Loss: 0.006440\n",
      "Epoch: 015/025 | Batch 300/1000 | Loss: 0.003068\n",
      "Epoch: 015/025 | Batch 400/1000 | Loss: 0.027831\n",
      "Epoch: 015/025 | Batch 500/1000 | Loss: 0.092627\n",
      "Epoch: 015/025 | Batch 600/1000 | Loss: 0.029777\n",
      "Epoch: 015/025 | Batch 700/1000 | Loss: 0.003350\n",
      "Epoch: 015/025 | Batch 800/1000 | Loss: 0.007074\n",
      "Epoch: 015/025 | Batch 900/1000 | Loss: 0.043567\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 016/025 | Batch 000/1000 | Loss: 0.000000\n",
      "Epoch: 016/025 | Batch 100/1000 | Loss: 0.000929\n",
      "Epoch: 016/025 | Batch 200/1000 | Loss: 0.000192\n",
      "Epoch: 016/025 | Batch 300/1000 | Loss: 0.000024\n",
      "Epoch: 016/025 | Batch 400/1000 | Loss: 0.000163\n",
      "Epoch: 016/025 | Batch 500/1000 | Loss: 0.109406\n",
      "Epoch: 016/025 | Batch 600/1000 | Loss: 0.000307\n",
      "Epoch: 016/025 | Batch 700/1000 | Loss: 0.021699\n",
      "Epoch: 016/025 | Batch 800/1000 | Loss: 0.052340\n",
      "Epoch: 016/025 | Batch 900/1000 | Loss: 0.002476\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 017/025 | Batch 000/1000 | Loss: 0.000003\n",
      "Epoch: 017/025 | Batch 100/1000 | Loss: 0.003270\n",
      "Epoch: 017/025 | Batch 200/1000 | Loss: 0.012983\n",
      "Epoch: 017/025 | Batch 300/1000 | Loss: 0.002292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017/025 | Batch 400/1000 | Loss: 0.000986\n",
      "Epoch: 017/025 | Batch 500/1000 | Loss: 0.002208\n",
      "Epoch: 017/025 | Batch 600/1000 | Loss: 0.000938\n",
      "Epoch: 017/025 | Batch 700/1000 | Loss: 0.002532\n",
      "Epoch: 017/025 | Batch 800/1000 | Loss: 0.006642\n",
      "Epoch: 017/025 | Batch 900/1000 | Loss: 0.000069\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 018/025 | Batch 000/1000 | Loss: 0.000067\n",
      "Epoch: 018/025 | Batch 100/1000 | Loss: 0.001301\n",
      "Epoch: 018/025 | Batch 200/1000 | Loss: 0.000424\n",
      "Epoch: 018/025 | Batch 300/1000 | Loss: 0.000138\n",
      "Epoch: 018/025 | Batch 400/1000 | Loss: 0.000046\n",
      "Epoch: 018/025 | Batch 500/1000 | Loss: 0.000062\n",
      "Epoch: 018/025 | Batch 600/1000 | Loss: 0.000433\n",
      "Epoch: 018/025 | Batch 700/1000 | Loss: 0.000094\n",
      "Epoch: 018/025 | Batch 800/1000 | Loss: 0.000403\n",
      "Epoch: 018/025 | Batch 900/1000 | Loss: 0.000032\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 019/025 | Batch 000/1000 | Loss: 0.000040\n",
      "Epoch: 019/025 | Batch 100/1000 | Loss: 0.000605\n",
      "Epoch: 019/025 | Batch 200/1000 | Loss: 0.000149\n",
      "Epoch: 019/025 | Batch 300/1000 | Loss: 0.000076\n",
      "Epoch: 019/025 | Batch 400/1000 | Loss: 0.000029\n",
      "Epoch: 019/025 | Batch 500/1000 | Loss: 0.000035\n",
      "Epoch: 019/025 | Batch 600/1000 | Loss: 0.000237\n",
      "Epoch: 019/025 | Batch 700/1000 | Loss: 0.000071\n",
      "Epoch: 019/025 | Batch 800/1000 | Loss: 0.000203\n",
      "Epoch: 019/025 | Batch 900/1000 | Loss: 0.000017\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 020/025 | Batch 000/1000 | Loss: 0.000023\n",
      "Epoch: 020/025 | Batch 100/1000 | Loss: 0.000353\n",
      "Epoch: 020/025 | Batch 200/1000 | Loss: 0.000103\n",
      "Epoch: 020/025 | Batch 300/1000 | Loss: 0.000059\n",
      "Epoch: 020/025 | Batch 400/1000 | Loss: 0.000020\n",
      "Epoch: 020/025 | Batch 500/1000 | Loss: 0.000024\n",
      "Epoch: 020/025 | Batch 600/1000 | Loss: 0.000160\n",
      "Epoch: 020/025 | Batch 700/1000 | Loss: 0.000054\n",
      "Epoch: 020/025 | Batch 800/1000 | Loss: 0.000131\n",
      "Epoch: 020/025 | Batch 900/1000 | Loss: 0.000012\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 021/025 | Batch 000/1000 | Loss: 0.000016\n",
      "Epoch: 021/025 | Batch 100/1000 | Loss: 0.000251\n",
      "Epoch: 021/025 | Batch 200/1000 | Loss: 0.000078\n",
      "Epoch: 021/025 | Batch 300/1000 | Loss: 0.000046\n",
      "Epoch: 021/025 | Batch 400/1000 | Loss: 0.000015\n",
      "Epoch: 021/025 | Batch 500/1000 | Loss: 0.000018\n",
      "Epoch: 021/025 | Batch 600/1000 | Loss: 0.000119\n",
      "Epoch: 021/025 | Batch 700/1000 | Loss: 0.000042\n",
      "Epoch: 021/025 | Batch 800/1000 | Loss: 0.000094\n",
      "Epoch: 021/025 | Batch 900/1000 | Loss: 0.000008\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 022/025 | Batch 000/1000 | Loss: 0.000012\n",
      "Epoch: 022/025 | Batch 100/1000 | Loss: 0.000191\n",
      "Epoch: 022/025 | Batch 200/1000 | Loss: 0.000063\n",
      "Epoch: 022/025 | Batch 300/1000 | Loss: 0.000037\n",
      "Epoch: 022/025 | Batch 400/1000 | Loss: 0.000012\n",
      "Epoch: 022/025 | Batch 500/1000 | Loss: 0.000014\n",
      "Epoch: 022/025 | Batch 600/1000 | Loss: 0.000094\n",
      "Epoch: 022/025 | Batch 700/1000 | Loss: 0.000034\n",
      "Epoch: 022/025 | Batch 800/1000 | Loss: 0.000072\n",
      "Epoch: 022/025 | Batch 900/1000 | Loss: 0.000006\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 023/025 | Batch 000/1000 | Loss: 0.000009\n",
      "Epoch: 023/025 | Batch 100/1000 | Loss: 0.000152\n",
      "Epoch: 023/025 | Batch 200/1000 | Loss: 0.000052\n",
      "Epoch: 023/025 | Batch 300/1000 | Loss: 0.000030\n",
      "Epoch: 023/025 | Batch 400/1000 | Loss: 0.000010\n",
      "Epoch: 023/025 | Batch 500/1000 | Loss: 0.000012\n",
      "Epoch: 023/025 | Batch 600/1000 | Loss: 0.000076\n",
      "Epoch: 023/025 | Batch 700/1000 | Loss: 0.000028\n",
      "Epoch: 023/025 | Batch 800/1000 | Loss: 0.000057\n",
      "Epoch: 023/025 | Batch 900/1000 | Loss: 0.000005\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 024/025 | Batch 000/1000 | Loss: 0.000008\n",
      "Epoch: 024/025 | Batch 100/1000 | Loss: 0.000125\n",
      "Epoch: 024/025 | Batch 200/1000 | Loss: 0.000045\n",
      "Epoch: 024/025 | Batch 300/1000 | Loss: 0.000025\n",
      "Epoch: 024/025 | Batch 400/1000 | Loss: 0.000009\n",
      "Epoch: 024/025 | Batch 500/1000 | Loss: 0.000010\n",
      "Epoch: 024/025 | Batch 600/1000 | Loss: 0.000064\n",
      "Epoch: 024/025 | Batch 700/1000 | Loss: 0.000023\n",
      "Epoch: 024/025 | Batch 800/1000 | Loss: 0.000047\n",
      "Epoch: 024/025 | Batch 900/1000 | Loss: 0.000004\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 025/025 | Batch 000/1000 | Loss: 0.000006\n",
      "Epoch: 025/025 | Batch 100/1000 | Loss: 0.000105\n",
      "Epoch: 025/025 | Batch 200/1000 | Loss: 0.000039\n",
      "Epoch: 025/025 | Batch 300/1000 | Loss: 0.000021\n",
      "Epoch: 025/025 | Batch 400/1000 | Loss: 0.000008\n",
      "Epoch: 025/025 | Batch 500/1000 | Loss: 0.000008\n",
      "Epoch: 025/025 | Batch 600/1000 | Loss: 0.000054\n",
      "Epoch: 025/025 | Batch 700/1000 | Loss: 0.000019\n",
      "Epoch: 025/025 | Batch 800/1000 | Loss: 0.000039\n",
      "Epoch: 025/025 | Batch 900/1000 | Loss: 0.000003\n",
      "Time elapsed: 0.05 min\n",
      "Total Training Time: 0.05 min\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "my_model.trainer(train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error : 0.0% \n",
      "Test error : 18.0%\n",
      "The total number of the parameters is: 204312\n"
     ]
    }
   ],
   "source": [
    "# output the train error and test error\n",
    "print(\"Train error : %.1f%% \\nTest error : %.1f%%\" %\n",
    "      (my_model.compute_error(train_input, train_target),\n",
    "       my_model.compute_error(test_input, test_target)))\n",
    "\n",
    "print(\"The total number of the parameters is: %d\" % (sum(p.numel() for p in my_model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_accuracy(net, test_input,test_target):\n",
    "# # 在训练模型的时候前面加上model.train()，在测试模型的时候前面加上model.eval(),以切换到测试模式\n",
    "#     net.eval()\n",
    "#     correct_pred, num_examples = 0, 0\n",
    "# # 而with torch.no_grad()则主要是用于停止autograd模块的工作，\n",
    "# # 以起到加速和节省显存的作用，具体行为就是停止gradient计算，\n",
    "# # 从而节省了GPU算力和显存，但是并不会影响dropout和batchnorm层的行为。\n",
    "#     with torch.no_grad():\n",
    "# #         for features, targets in data_loader:\n",
    "#         for idx in range(test_input.size(0)):\n",
    "# #             features = features.view(-1, 14*14)\n",
    "#             features= test_input[idx]\n",
    "#             targets = test_target[idx]\n",
    "#             logits, probas = net(features)\n",
    "#             _, predicted_labels = torch.max(probas, 1)\n",
    "#             num_examples += targets.size(0)\n",
    "#             correct_pred += (predicted_labels == targets).sum()\n",
    "#         return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
