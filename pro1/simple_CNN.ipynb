{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_practical_prologue as prologue\n",
    "#直接导入出现http403错误\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接导入出现http403错误\n",
    "# have to add a header to your urllib request (due to that site moving to Cloudflare protection)\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "#*********************** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "N_PAIRS = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2, 14, 14])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying samples of data\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.imshow(test_input[i][0], cmap='gray')\n",
    "  plt.title(\"Value: {}\".format(train_classes[i][0]))  \n",
    "  plt.tight_layout()\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_input[0][1],cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.float32\n",
      "torch.Size([1000])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(train_input.dtype)\n",
    "print(train_target.shape)\n",
    "print(train_target.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64,128,kernel_size=2)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv2_drop=nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        #parameters\n",
    "        self.batch_size = 50\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.num_epochs = 25\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.bn2(self.conv2(x)), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 512)))\n",
    "        x = F.dropout(x,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "        # Training Function\n",
    "\n",
    "    def trainer(self, train_input, train_target):\n",
    "        \"\"\"\n",
    "        Train the model on a training set\n",
    "        :param train_input: Training features\n",
    "        :param train_target: Training labels\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch_idx in range(0,train_input.size(0),self.batch_size):\n",
    "                output = self(train_input[batch_idx:batch_idx+self.batch_size]) \n",
    "                loss = self.criterion(output, train_target[batch_idx:batch_idx+self.batch_size])  \n",
    "                self.optimizer.zero_grad()                          #清零梯度\n",
    "                loss.backward()                                #反向求梯度\n",
    "                self.optimizer.step()\n",
    "#                 每隔50组数据，输出一次loss值\n",
    "                if not batch_idx % 50:\n",
    "                    print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.6f' \n",
    "                           %(epoch+1, self.num_epochs, batch_idx, \n",
    "                             len(train_input), loss))\n",
    "            print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "        \n",
    "\n",
    "        # Test error\n",
    "\n",
    "    def compute_error(self, input_data, target):\n",
    "        \"\"\"\n",
    "        Compute the number of error of the model on a test set\n",
    "        :param input_data: test features\n",
    "        :param target: test target\n",
    "        :return: error rate of the input data\n",
    "        \"\"\"  \n",
    "    \n",
    "        #测试模型\n",
    "        self.eval()      #测试模式，关闭正则化\n",
    "        errors = 0\n",
    "        for idx in range(0,input_data.size(0),self.batch_size):\n",
    "            input_batch=input_data.narrow(0,idx,self.batch_size)\n",
    "            outputs = self(input_batch)\n",
    "            _, predicted = torch.max(outputs, 1)   #返回值和索引\n",
    "            target_labels = target.narrow(0, idx, self.batch_size)\n",
    "            errors += torch.sum(predicted != target_labels)\n",
    "\n",
    "        return float(errors)*100/input_data.size(0)\n",
    "    def save_model(self,model_name):\n",
    "        \"\"\"\n",
    "        Save the model to a direction\n",
    "        :param model_name: the model name, e.g. CNN_Net.pth\n",
    "        \"\"\"         \n",
    "        torch.save(self, './model/'+ model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=CNN_Net()\n",
    "my_model.save_model('CNN_Net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/025 | Batch 000/1000 | Loss: 0.718260\n",
      "Epoch: 001/025 | Batch 050/1000 | Loss: 0.727345\n",
      "Epoch: 001/025 | Batch 100/1000 | Loss: 0.784436\n",
      "Epoch: 001/025 | Batch 150/1000 | Loss: 0.640804\n",
      "Epoch: 001/025 | Batch 200/1000 | Loss: 0.683480\n",
      "Epoch: 001/025 | Batch 250/1000 | Loss: 0.639555\n",
      "Epoch: 001/025 | Batch 300/1000 | Loss: 0.686323\n",
      "Epoch: 001/025 | Batch 350/1000 | Loss: 0.757853\n",
      "Epoch: 001/025 | Batch 400/1000 | Loss: 0.639212\n",
      "Epoch: 001/025 | Batch 450/1000 | Loss: 0.695754\n",
      "Epoch: 001/025 | Batch 500/1000 | Loss: 0.653220\n",
      "Epoch: 001/025 | Batch 550/1000 | Loss: 0.626454\n",
      "Epoch: 001/025 | Batch 600/1000 | Loss: 0.556833\n",
      "Epoch: 001/025 | Batch 650/1000 | Loss: 0.643307\n",
      "Epoch: 001/025 | Batch 700/1000 | Loss: 0.552047\n",
      "Epoch: 001/025 | Batch 750/1000 | Loss: 0.627101\n",
      "Epoch: 001/025 | Batch 800/1000 | Loss: 0.590212\n",
      "Epoch: 001/025 | Batch 850/1000 | Loss: 0.553420\n",
      "Epoch: 001/025 | Batch 900/1000 | Loss: 0.567891\n",
      "Epoch: 001/025 | Batch 950/1000 | Loss: 0.551793\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 002/025 | Batch 000/1000 | Loss: 0.447675\n",
      "Epoch: 002/025 | Batch 050/1000 | Loss: 0.535820\n",
      "Epoch: 002/025 | Batch 100/1000 | Loss: 0.522819\n",
      "Epoch: 002/025 | Batch 150/1000 | Loss: 0.508083\n",
      "Epoch: 002/025 | Batch 200/1000 | Loss: 0.508982\n",
      "Epoch: 002/025 | Batch 250/1000 | Loss: 0.522431\n",
      "Epoch: 002/025 | Batch 300/1000 | Loss: 0.479984\n",
      "Epoch: 002/025 | Batch 350/1000 | Loss: 0.520499\n",
      "Epoch: 002/025 | Batch 400/1000 | Loss: 0.567155\n",
      "Epoch: 002/025 | Batch 450/1000 | Loss: 0.434609\n",
      "Epoch: 002/025 | Batch 500/1000 | Loss: 0.481253\n",
      "Epoch: 002/025 | Batch 550/1000 | Loss: 0.551669\n",
      "Epoch: 002/025 | Batch 600/1000 | Loss: 0.453629\n",
      "Epoch: 002/025 | Batch 650/1000 | Loss: 0.552798\n",
      "Epoch: 002/025 | Batch 700/1000 | Loss: 0.459550\n",
      "Epoch: 002/025 | Batch 750/1000 | Loss: 0.487504\n",
      "Epoch: 002/025 | Batch 800/1000 | Loss: 0.452732\n",
      "Epoch: 002/025 | Batch 850/1000 | Loss: 0.358373\n",
      "Epoch: 002/025 | Batch 900/1000 | Loss: 0.375633\n",
      "Epoch: 002/025 | Batch 950/1000 | Loss: 0.436907\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/025 | Batch 000/1000 | Loss: 0.324737\n",
      "Epoch: 003/025 | Batch 050/1000 | Loss: 0.408965\n",
      "Epoch: 003/025 | Batch 100/1000 | Loss: 0.344982\n",
      "Epoch: 003/025 | Batch 150/1000 | Loss: 0.495461\n",
      "Epoch: 003/025 | Batch 200/1000 | Loss: 0.402006\n",
      "Epoch: 003/025 | Batch 250/1000 | Loss: 0.418323\n",
      "Epoch: 003/025 | Batch 300/1000 | Loss: 0.404800\n",
      "Epoch: 003/025 | Batch 350/1000 | Loss: 0.397342\n",
      "Epoch: 003/025 | Batch 400/1000 | Loss: 0.293988\n",
      "Epoch: 003/025 | Batch 450/1000 | Loss: 0.363986\n",
      "Epoch: 003/025 | Batch 500/1000 | Loss: 0.329291\n",
      "Epoch: 003/025 | Batch 550/1000 | Loss: 0.360494\n",
      "Epoch: 003/025 | Batch 600/1000 | Loss: 0.320714\n",
      "Epoch: 003/025 | Batch 650/1000 | Loss: 0.418612\n",
      "Epoch: 003/025 | Batch 700/1000 | Loss: 0.319512\n",
      "Epoch: 003/025 | Batch 750/1000 | Loss: 0.373179\n",
      "Epoch: 003/025 | Batch 800/1000 | Loss: 0.321245\n",
      "Epoch: 003/025 | Batch 850/1000 | Loss: 0.256533\n",
      "Epoch: 003/025 | Batch 900/1000 | Loss: 0.239532\n",
      "Epoch: 003/025 | Batch 950/1000 | Loss: 0.247453\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 004/025 | Batch 000/1000 | Loss: 0.154913\n",
      "Epoch: 004/025 | Batch 050/1000 | Loss: 0.230700\n",
      "Epoch: 004/025 | Batch 100/1000 | Loss: 0.253849\n",
      "Epoch: 004/025 | Batch 150/1000 | Loss: 0.316577\n",
      "Epoch: 004/025 | Batch 200/1000 | Loss: 0.335783\n",
      "Epoch: 004/025 | Batch 250/1000 | Loss: 0.295857\n",
      "Epoch: 004/025 | Batch 300/1000 | Loss: 0.280564\n",
      "Epoch: 004/025 | Batch 350/1000 | Loss: 0.286259\n",
      "Epoch: 004/025 | Batch 400/1000 | Loss: 0.176278\n",
      "Epoch: 004/025 | Batch 450/1000 | Loss: 0.257901\n",
      "Epoch: 004/025 | Batch 500/1000 | Loss: 0.330655\n",
      "Epoch: 004/025 | Batch 550/1000 | Loss: 0.302547\n",
      "Epoch: 004/025 | Batch 600/1000 | Loss: 0.224449\n",
      "Epoch: 004/025 | Batch 650/1000 | Loss: 0.324944\n",
      "Epoch: 004/025 | Batch 700/1000 | Loss: 0.217417\n",
      "Epoch: 004/025 | Batch 750/1000 | Loss: 0.214944\n",
      "Epoch: 004/025 | Batch 800/1000 | Loss: 0.218695\n",
      "Epoch: 004/025 | Batch 850/1000 | Loss: 0.181814\n",
      "Epoch: 004/025 | Batch 900/1000 | Loss: 0.188785\n",
      "Epoch: 004/025 | Batch 950/1000 | Loss: 0.138080\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 005/025 | Batch 000/1000 | Loss: 0.114627\n",
      "Epoch: 005/025 | Batch 050/1000 | Loss: 0.175112\n",
      "Epoch: 005/025 | Batch 100/1000 | Loss: 0.143604\n",
      "Epoch: 005/025 | Batch 150/1000 | Loss: 0.212779\n",
      "Epoch: 005/025 | Batch 200/1000 | Loss: 0.296906\n",
      "Epoch: 005/025 | Batch 250/1000 | Loss: 0.168878\n",
      "Epoch: 005/025 | Batch 300/1000 | Loss: 0.173060\n",
      "Epoch: 005/025 | Batch 350/1000 | Loss: 0.163787\n",
      "Epoch: 005/025 | Batch 400/1000 | Loss: 0.130679\n",
      "Epoch: 005/025 | Batch 450/1000 | Loss: 0.180073\n",
      "Epoch: 005/025 | Batch 500/1000 | Loss: 0.149736\n",
      "Epoch: 005/025 | Batch 550/1000 | Loss: 0.181842\n",
      "Epoch: 005/025 | Batch 600/1000 | Loss: 0.185155\n",
      "Epoch: 005/025 | Batch 650/1000 | Loss: 0.148573\n",
      "Epoch: 005/025 | Batch 700/1000 | Loss: 0.139283\n",
      "Epoch: 005/025 | Batch 750/1000 | Loss: 0.113853\n",
      "Epoch: 005/025 | Batch 800/1000 | Loss: 0.114907\n",
      "Epoch: 005/025 | Batch 850/1000 | Loss: 0.083637\n",
      "Epoch: 005/025 | Batch 900/1000 | Loss: 0.090579\n",
      "Epoch: 005/025 | Batch 950/1000 | Loss: 0.139230\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 006/025 | Batch 000/1000 | Loss: 0.085867\n",
      "Epoch: 006/025 | Batch 050/1000 | Loss: 0.105735\n",
      "Epoch: 006/025 | Batch 100/1000 | Loss: 0.069138\n",
      "Epoch: 006/025 | Batch 150/1000 | Loss: 0.160502\n",
      "Epoch: 006/025 | Batch 200/1000 | Loss: 0.095158\n",
      "Epoch: 006/025 | Batch 250/1000 | Loss: 0.123962\n",
      "Epoch: 006/025 | Batch 300/1000 | Loss: 0.082793\n",
      "Epoch: 006/025 | Batch 350/1000 | Loss: 0.093855\n",
      "Epoch: 006/025 | Batch 400/1000 | Loss: 0.120352\n",
      "Epoch: 006/025 | Batch 450/1000 | Loss: 0.104596\n",
      "Epoch: 006/025 | Batch 500/1000 | Loss: 0.091654\n",
      "Epoch: 006/025 | Batch 550/1000 | Loss: 0.109635\n",
      "Epoch: 006/025 | Batch 600/1000 | Loss: 0.205681\n",
      "Epoch: 006/025 | Batch 650/1000 | Loss: 0.116134\n",
      "Epoch: 006/025 | Batch 700/1000 | Loss: 0.066698\n",
      "Epoch: 006/025 | Batch 750/1000 | Loss: 0.047956\n",
      "Epoch: 006/025 | Batch 800/1000 | Loss: 0.050698\n",
      "Epoch: 006/025 | Batch 850/1000 | Loss: 0.093977\n",
      "Epoch: 006/025 | Batch 900/1000 | Loss: 0.035290\n",
      "Epoch: 006/025 | Batch 950/1000 | Loss: 0.069565\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 007/025 | Batch 000/1000 | Loss: 0.038898\n",
      "Epoch: 007/025 | Batch 050/1000 | Loss: 0.043562\n",
      "Epoch: 007/025 | Batch 100/1000 | Loss: 0.044795\n",
      "Epoch: 007/025 | Batch 150/1000 | Loss: 0.099507\n",
      "Epoch: 007/025 | Batch 200/1000 | Loss: 0.084536\n",
      "Epoch: 007/025 | Batch 250/1000 | Loss: 0.081693\n",
      "Epoch: 007/025 | Batch 300/1000 | Loss: 0.077507\n",
      "Epoch: 007/025 | Batch 350/1000 | Loss: 0.042269\n",
      "Epoch: 007/025 | Batch 400/1000 | Loss: 0.031709\n",
      "Epoch: 007/025 | Batch 450/1000 | Loss: 0.060199\n",
      "Epoch: 007/025 | Batch 500/1000 | Loss: 0.064169\n",
      "Epoch: 007/025 | Batch 550/1000 | Loss: 0.098273\n",
      "Epoch: 007/025 | Batch 600/1000 | Loss: 0.040560\n",
      "Epoch: 007/025 | Batch 650/1000 | Loss: 0.038453\n",
      "Epoch: 007/025 | Batch 700/1000 | Loss: 0.085533\n",
      "Epoch: 007/025 | Batch 750/1000 | Loss: 0.115376\n",
      "Epoch: 007/025 | Batch 800/1000 | Loss: 0.042119\n",
      "Epoch: 007/025 | Batch 850/1000 | Loss: 0.049123\n",
      "Epoch: 007/025 | Batch 900/1000 | Loss: 0.025526\n",
      "Epoch: 007/025 | Batch 950/1000 | Loss: 0.027031\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 008/025 | Batch 000/1000 | Loss: 0.086780\n",
      "Epoch: 008/025 | Batch 050/1000 | Loss: 0.172063\n",
      "Epoch: 008/025 | Batch 100/1000 | Loss: 0.074519\n",
      "Epoch: 008/025 | Batch 150/1000 | Loss: 0.041860\n",
      "Epoch: 008/025 | Batch 200/1000 | Loss: 0.055793\n",
      "Epoch: 008/025 | Batch 250/1000 | Loss: 0.334878\n",
      "Epoch: 008/025 | Batch 300/1000 | Loss: 0.128061\n",
      "Epoch: 008/025 | Batch 350/1000 | Loss: 0.061447\n",
      "Epoch: 008/025 | Batch 400/1000 | Loss: 0.080748\n",
      "Epoch: 008/025 | Batch 450/1000 | Loss: 0.164463\n",
      "Epoch: 008/025 | Batch 500/1000 | Loss: 0.234140\n",
      "Epoch: 008/025 | Batch 550/1000 | Loss: 0.360013\n",
      "Epoch: 008/025 | Batch 600/1000 | Loss: 0.047536\n",
      "Epoch: 008/025 | Batch 650/1000 | Loss: 0.062459\n",
      "Epoch: 008/025 | Batch 700/1000 | Loss: 0.136133\n",
      "Epoch: 008/025 | Batch 750/1000 | Loss: 0.207042\n",
      "Epoch: 008/025 | Batch 800/1000 | Loss: 0.272848\n",
      "Epoch: 008/025 | Batch 850/1000 | Loss: 0.122569\n",
      "Epoch: 008/025 | Batch 900/1000 | Loss: 0.031567\n",
      "Epoch: 008/025 | Batch 950/1000 | Loss: 0.042098\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 009/025 | Batch 000/1000 | Loss: 0.096027\n",
      "Epoch: 009/025 | Batch 050/1000 | Loss: 0.100399\n",
      "Epoch: 009/025 | Batch 100/1000 | Loss: 0.283024\n",
      "Epoch: 009/025 | Batch 150/1000 | Loss: 0.079440\n",
      "Epoch: 009/025 | Batch 200/1000 | Loss: 0.188421\n",
      "Epoch: 009/025 | Batch 250/1000 | Loss: 0.039113\n",
      "Epoch: 009/025 | Batch 300/1000 | Loss: 0.132006\n",
      "Epoch: 009/025 | Batch 350/1000 | Loss: 0.336199\n",
      "Epoch: 009/025 | Batch 400/1000 | Loss: 0.390262\n",
      "Epoch: 009/025 | Batch 450/1000 | Loss: 0.182082\n",
      "Epoch: 009/025 | Batch 500/1000 | Loss: 0.067047\n",
      "Epoch: 009/025 | Batch 550/1000 | Loss: 0.055151\n",
      "Epoch: 009/025 | Batch 600/1000 | Loss: 0.191851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009/025 | Batch 650/1000 | Loss: 0.379087\n",
      "Epoch: 009/025 | Batch 700/1000 | Loss: 0.391804\n",
      "Epoch: 009/025 | Batch 750/1000 | Loss: 0.116681\n",
      "Epoch: 009/025 | Batch 800/1000 | Loss: 0.035463\n",
      "Epoch: 009/025 | Batch 850/1000 | Loss: 0.074433\n",
      "Epoch: 009/025 | Batch 900/1000 | Loss: 0.251370\n",
      "Epoch: 009/025 | Batch 950/1000 | Loss: 0.265512\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 010/025 | Batch 000/1000 | Loss: 0.141251\n",
      "Epoch: 010/025 | Batch 050/1000 | Loss: 0.114770\n",
      "Epoch: 010/025 | Batch 100/1000 | Loss: 0.066596\n",
      "Epoch: 010/025 | Batch 150/1000 | Loss: 0.054471\n",
      "Epoch: 010/025 | Batch 200/1000 | Loss: 0.180847\n",
      "Epoch: 010/025 | Batch 250/1000 | Loss: 0.154173\n",
      "Epoch: 010/025 | Batch 300/1000 | Loss: 0.181106\n",
      "Epoch: 010/025 | Batch 350/1000 | Loss: 0.050076\n",
      "Epoch: 010/025 | Batch 400/1000 | Loss: 0.025160\n",
      "Epoch: 010/025 | Batch 450/1000 | Loss: 0.026014\n",
      "Epoch: 010/025 | Batch 500/1000 | Loss: 0.061199\n",
      "Epoch: 010/025 | Batch 550/1000 | Loss: 0.191244\n",
      "Epoch: 010/025 | Batch 600/1000 | Loss: 0.084290\n",
      "Epoch: 010/025 | Batch 650/1000 | Loss: 0.083567\n",
      "Epoch: 010/025 | Batch 700/1000 | Loss: 0.013993\n",
      "Epoch: 010/025 | Batch 750/1000 | Loss: 0.038706\n",
      "Epoch: 010/025 | Batch 800/1000 | Loss: 0.082532\n",
      "Epoch: 010/025 | Batch 850/1000 | Loss: 0.062278\n",
      "Epoch: 010/025 | Batch 900/1000 | Loss: 0.021896\n",
      "Epoch: 010/025 | Batch 950/1000 | Loss: 0.053953\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 011/025 | Batch 000/1000 | Loss: 0.011820\n",
      "Epoch: 011/025 | Batch 050/1000 | Loss: 0.016204\n",
      "Epoch: 011/025 | Batch 100/1000 | Loss: 0.012064\n",
      "Epoch: 011/025 | Batch 150/1000 | Loss: 0.026547\n",
      "Epoch: 011/025 | Batch 200/1000 | Loss: 0.055931\n",
      "Epoch: 011/025 | Batch 250/1000 | Loss: 0.019864\n",
      "Epoch: 011/025 | Batch 300/1000 | Loss: 0.030095\n",
      "Epoch: 011/025 | Batch 350/1000 | Loss: 0.028395\n",
      "Epoch: 011/025 | Batch 400/1000 | Loss: 0.009414\n",
      "Epoch: 011/025 | Batch 450/1000 | Loss: 0.020093\n",
      "Epoch: 011/025 | Batch 500/1000 | Loss: 0.028390\n",
      "Epoch: 011/025 | Batch 550/1000 | Loss: 0.028871\n",
      "Epoch: 011/025 | Batch 600/1000 | Loss: 0.014974\n",
      "Epoch: 011/025 | Batch 650/1000 | Loss: 0.008322\n",
      "Epoch: 011/025 | Batch 700/1000 | Loss: 0.007887\n",
      "Epoch: 011/025 | Batch 750/1000 | Loss: 0.012577\n",
      "Epoch: 011/025 | Batch 800/1000 | Loss: 0.013021\n",
      "Epoch: 011/025 | Batch 850/1000 | Loss: 0.059893\n",
      "Epoch: 011/025 | Batch 900/1000 | Loss: 0.023373\n",
      "Epoch: 011/025 | Batch 950/1000 | Loss: 0.007744\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 012/025 | Batch 000/1000 | Loss: 0.011887\n",
      "Epoch: 012/025 | Batch 050/1000 | Loss: 0.012250\n",
      "Epoch: 012/025 | Batch 100/1000 | Loss: 0.011202\n",
      "Epoch: 012/025 | Batch 150/1000 | Loss: 0.029733\n",
      "Epoch: 012/025 | Batch 200/1000 | Loss: 0.035318\n",
      "Epoch: 012/025 | Batch 250/1000 | Loss: 0.018592\n",
      "Epoch: 012/025 | Batch 300/1000 | Loss: 0.008246\n",
      "Epoch: 012/025 | Batch 350/1000 | Loss: 0.006804\n",
      "Epoch: 012/025 | Batch 400/1000 | Loss: 0.019416\n",
      "Epoch: 012/025 | Batch 450/1000 | Loss: 0.030670\n",
      "Epoch: 012/025 | Batch 500/1000 | Loss: 0.010177\n",
      "Epoch: 012/025 | Batch 550/1000 | Loss: 0.010228\n",
      "Epoch: 012/025 | Batch 600/1000 | Loss: 0.009260\n",
      "Epoch: 012/025 | Batch 650/1000 | Loss: 0.005940\n",
      "Epoch: 012/025 | Batch 700/1000 | Loss: 0.006960\n",
      "Epoch: 012/025 | Batch 750/1000 | Loss: 0.011632\n",
      "Epoch: 012/025 | Batch 800/1000 | Loss: 0.005148\n",
      "Epoch: 012/025 | Batch 850/1000 | Loss: 0.003691\n",
      "Epoch: 012/025 | Batch 900/1000 | Loss: 0.002940\n",
      "Epoch: 012/025 | Batch 950/1000 | Loss: 0.005179\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 013/025 | Batch 000/1000 | Loss: 0.003762\n",
      "Epoch: 013/025 | Batch 050/1000 | Loss: 0.005293\n",
      "Epoch: 013/025 | Batch 100/1000 | Loss: 0.005147\n",
      "Epoch: 013/025 | Batch 150/1000 | Loss: 0.025146\n",
      "Epoch: 013/025 | Batch 200/1000 | Loss: 0.011414\n",
      "Epoch: 013/025 | Batch 250/1000 | Loss: 0.019370\n",
      "Epoch: 013/025 | Batch 300/1000 | Loss: 0.005063\n",
      "Epoch: 013/025 | Batch 350/1000 | Loss: 0.014016\n",
      "Epoch: 013/025 | Batch 400/1000 | Loss: 0.004675\n",
      "Epoch: 013/025 | Batch 450/1000 | Loss: 0.007643\n",
      "Epoch: 013/025 | Batch 500/1000 | Loss: 0.004841\n",
      "Epoch: 013/025 | Batch 550/1000 | Loss: 0.006430\n",
      "Epoch: 013/025 | Batch 600/1000 | Loss: 0.006411\n",
      "Epoch: 013/025 | Batch 650/1000 | Loss: 0.002102\n",
      "Epoch: 013/025 | Batch 700/1000 | Loss: 0.003574\n",
      "Epoch: 013/025 | Batch 750/1000 | Loss: 0.003770\n",
      "Epoch: 013/025 | Batch 800/1000 | Loss: 0.006242\n",
      "Epoch: 013/025 | Batch 850/1000 | Loss: 0.003089\n",
      "Epoch: 013/025 | Batch 900/1000 | Loss: 0.002866\n",
      "Epoch: 013/025 | Batch 950/1000 | Loss: 0.003607\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 014/025 | Batch 000/1000 | Loss: 0.001794\n",
      "Epoch: 014/025 | Batch 050/1000 | Loss: 0.004774\n",
      "Epoch: 014/025 | Batch 100/1000 | Loss: 0.004632\n",
      "Epoch: 014/025 | Batch 150/1000 | Loss: 0.004591\n",
      "Epoch: 014/025 | Batch 200/1000 | Loss: 0.009023\n",
      "Epoch: 014/025 | Batch 250/1000 | Loss: 0.002574\n",
      "Epoch: 014/025 | Batch 300/1000 | Loss: 0.001609\n",
      "Epoch: 014/025 | Batch 350/1000 | Loss: 0.002695\n",
      "Epoch: 014/025 | Batch 400/1000 | Loss: 0.004698\n",
      "Epoch: 014/025 | Batch 450/1000 | Loss: 0.007851\n",
      "Epoch: 014/025 | Batch 500/1000 | Loss: 0.003215\n",
      "Epoch: 014/025 | Batch 550/1000 | Loss: 0.007926\n",
      "Epoch: 014/025 | Batch 600/1000 | Loss: 0.006366\n",
      "Epoch: 014/025 | Batch 650/1000 | Loss: 0.006221\n",
      "Epoch: 014/025 | Batch 700/1000 | Loss: 0.002320\n",
      "Epoch: 014/025 | Batch 750/1000 | Loss: 0.005739\n",
      "Epoch: 014/025 | Batch 800/1000 | Loss: 0.002875\n",
      "Epoch: 014/025 | Batch 850/1000 | Loss: 0.004136\n",
      "Epoch: 014/025 | Batch 900/1000 | Loss: 0.003156\n",
      "Epoch: 014/025 | Batch 950/1000 | Loss: 0.001717\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 015/025 | Batch 000/1000 | Loss: 0.002918\n",
      "Epoch: 015/025 | Batch 050/1000 | Loss: 0.002888\n",
      "Epoch: 015/025 | Batch 100/1000 | Loss: 0.002498\n",
      "Epoch: 015/025 | Batch 150/1000 | Loss: 0.004901\n",
      "Epoch: 015/025 | Batch 200/1000 | Loss: 0.004126\n",
      "Epoch: 015/025 | Batch 250/1000 | Loss: 0.005036\n",
      "Epoch: 015/025 | Batch 300/1000 | Loss: 0.002887\n",
      "Epoch: 015/025 | Batch 350/1000 | Loss: 0.002495\n",
      "Epoch: 015/025 | Batch 400/1000 | Loss: 0.002615\n",
      "Epoch: 015/025 | Batch 450/1000 | Loss: 0.004590\n",
      "Epoch: 015/025 | Batch 500/1000 | Loss: 0.003284\n",
      "Epoch: 015/025 | Batch 550/1000 | Loss: 0.003642\n",
      "Epoch: 015/025 | Batch 600/1000 | Loss: 0.003961\n",
      "Epoch: 015/025 | Batch 650/1000 | Loss: 0.002083\n",
      "Epoch: 015/025 | Batch 700/1000 | Loss: 0.002022\n",
      "Epoch: 015/025 | Batch 750/1000 | Loss: 0.009273\n",
      "Epoch: 015/025 | Batch 800/1000 | Loss: 0.005552\n",
      "Epoch: 015/025 | Batch 850/1000 | Loss: 0.002939\n",
      "Epoch: 015/025 | Batch 900/1000 | Loss: 0.002680\n",
      "Epoch: 015/025 | Batch 950/1000 | Loss: 0.002314\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 016/025 | Batch 000/1000 | Loss: 0.004261\n",
      "Epoch: 016/025 | Batch 050/1000 | Loss: 0.002759\n",
      "Epoch: 016/025 | Batch 100/1000 | Loss: 0.005045\n",
      "Epoch: 016/025 | Batch 150/1000 | Loss: 0.004845\n",
      "Epoch: 016/025 | Batch 200/1000 | Loss: 0.002831\n",
      "Epoch: 016/025 | Batch 250/1000 | Loss: 0.001772\n",
      "Epoch: 016/025 | Batch 300/1000 | Loss: 0.006466\n",
      "Epoch: 016/025 | Batch 350/1000 | Loss: 0.002378\n",
      "Epoch: 016/025 | Batch 400/1000 | Loss: 0.002097\n",
      "Epoch: 016/025 | Batch 450/1000 | Loss: 0.002993\n",
      "Epoch: 016/025 | Batch 500/1000 | Loss: 0.002991\n",
      "Epoch: 016/025 | Batch 550/1000 | Loss: 0.002262\n",
      "Epoch: 016/025 | Batch 600/1000 | Loss: 0.002080\n",
      "Epoch: 016/025 | Batch 650/1000 | Loss: 0.002496\n",
      "Epoch: 016/025 | Batch 700/1000 | Loss: 0.002435\n",
      "Epoch: 016/025 | Batch 750/1000 | Loss: 0.001980\n",
      "Epoch: 016/025 | Batch 800/1000 | Loss: 0.001604\n",
      "Epoch: 016/025 | Batch 850/1000 | Loss: 0.000771\n",
      "Epoch: 016/025 | Batch 900/1000 | Loss: 0.001130\n",
      "Epoch: 016/025 | Batch 950/1000 | Loss: 0.002174\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 017/025 | Batch 000/1000 | Loss: 0.000960\n",
      "Epoch: 017/025 | Batch 050/1000 | Loss: 0.003470\n",
      "Epoch: 017/025 | Batch 100/1000 | Loss: 0.007150\n",
      "Epoch: 017/025 | Batch 150/1000 | Loss: 0.001915\n",
      "Epoch: 017/025 | Batch 200/1000 | Loss: 0.005593\n",
      "Epoch: 017/025 | Batch 250/1000 | Loss: 0.001291\n",
      "Epoch: 017/025 | Batch 300/1000 | Loss: 0.001327\n",
      "Epoch: 017/025 | Batch 350/1000 | Loss: 0.003619\n",
      "Epoch: 017/025 | Batch 400/1000 | Loss: 0.001871\n",
      "Epoch: 017/025 | Batch 450/1000 | Loss: 0.004127\n",
      "Epoch: 017/025 | Batch 500/1000 | Loss: 0.001529\n",
      "Epoch: 017/025 | Batch 550/1000 | Loss: 0.002636\n",
      "Epoch: 017/025 | Batch 600/1000 | Loss: 0.002897\n",
      "Epoch: 017/025 | Batch 650/1000 | Loss: 0.002881\n",
      "Epoch: 017/025 | Batch 700/1000 | Loss: 0.001599\n",
      "Epoch: 017/025 | Batch 750/1000 | Loss: 0.001237\n",
      "Epoch: 017/025 | Batch 800/1000 | Loss: 0.001371\n",
      "Epoch: 017/025 | Batch 850/1000 | Loss: 0.000724\n",
      "Epoch: 017/025 | Batch 900/1000 | Loss: 0.002725\n",
      "Epoch: 017/025 | Batch 950/1000 | Loss: 0.004070\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 018/025 | Batch 000/1000 | Loss: 0.000753\n",
      "Epoch: 018/025 | Batch 050/1000 | Loss: 0.001388\n",
      "Epoch: 018/025 | Batch 100/1000 | Loss: 0.001566\n",
      "Epoch: 018/025 | Batch 150/1000 | Loss: 0.002201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018/025 | Batch 200/1000 | Loss: 0.002300\n",
      "Epoch: 018/025 | Batch 250/1000 | Loss: 0.003049\n",
      "Epoch: 018/025 | Batch 300/1000 | Loss: 0.001157\n",
      "Epoch: 018/025 | Batch 350/1000 | Loss: 0.002336\n",
      "Epoch: 018/025 | Batch 400/1000 | Loss: 0.001050\n",
      "Epoch: 018/025 | Batch 450/1000 | Loss: 0.001316\n",
      "Epoch: 018/025 | Batch 500/1000 | Loss: 0.001660\n",
      "Epoch: 018/025 | Batch 550/1000 | Loss: 0.001214\n",
      "Epoch: 018/025 | Batch 600/1000 | Loss: 0.001472\n",
      "Epoch: 018/025 | Batch 650/1000 | Loss: 0.001986\n",
      "Epoch: 018/025 | Batch 700/1000 | Loss: 0.001007\n",
      "Epoch: 018/025 | Batch 750/1000 | Loss: 0.002125\n",
      "Epoch: 018/025 | Batch 800/1000 | Loss: 0.001008\n",
      "Epoch: 018/025 | Batch 850/1000 | Loss: 0.002647\n",
      "Epoch: 018/025 | Batch 900/1000 | Loss: 0.000878\n",
      "Epoch: 018/025 | Batch 950/1000 | Loss: 0.001206\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 019/025 | Batch 000/1000 | Loss: 0.001488\n",
      "Epoch: 019/025 | Batch 050/1000 | Loss: 0.001890\n",
      "Epoch: 019/025 | Batch 100/1000 | Loss: 0.001730\n",
      "Epoch: 019/025 | Batch 150/1000 | Loss: 0.002073\n",
      "Epoch: 019/025 | Batch 200/1000 | Loss: 0.001933\n",
      "Epoch: 019/025 | Batch 250/1000 | Loss: 0.001307\n",
      "Epoch: 019/025 | Batch 300/1000 | Loss: 0.001257\n",
      "Epoch: 019/025 | Batch 350/1000 | Loss: 0.000907\n",
      "Epoch: 019/025 | Batch 400/1000 | Loss: 0.001176\n",
      "Epoch: 019/025 | Batch 450/1000 | Loss: 0.003134\n",
      "Epoch: 019/025 | Batch 500/1000 | Loss: 0.001593\n",
      "Epoch: 019/025 | Batch 550/1000 | Loss: 0.002249\n",
      "Epoch: 019/025 | Batch 600/1000 | Loss: 0.002038\n",
      "Epoch: 019/025 | Batch 650/1000 | Loss: 0.001008\n",
      "Epoch: 019/025 | Batch 700/1000 | Loss: 0.002491\n",
      "Epoch: 019/025 | Batch 750/1000 | Loss: 0.001071\n",
      "Epoch: 019/025 | Batch 800/1000 | Loss: 0.001044\n",
      "Epoch: 019/025 | Batch 850/1000 | Loss: 0.000740\n",
      "Epoch: 019/025 | Batch 900/1000 | Loss: 0.001636\n",
      "Epoch: 019/025 | Batch 950/1000 | Loss: 0.001248\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 020/025 | Batch 000/1000 | Loss: 0.001592\n",
      "Epoch: 020/025 | Batch 050/1000 | Loss: 0.001108\n",
      "Epoch: 020/025 | Batch 100/1000 | Loss: 0.001657\n",
      "Epoch: 020/025 | Batch 150/1000 | Loss: 0.001484\n",
      "Epoch: 020/025 | Batch 200/1000 | Loss: 0.002717\n",
      "Epoch: 020/025 | Batch 250/1000 | Loss: 0.002035\n",
      "Epoch: 020/025 | Batch 300/1000 | Loss: 0.001643\n",
      "Epoch: 020/025 | Batch 350/1000 | Loss: 0.001225\n",
      "Epoch: 020/025 | Batch 400/1000 | Loss: 0.001533\n",
      "Epoch: 020/025 | Batch 450/1000 | Loss: 0.002328\n",
      "Epoch: 020/025 | Batch 500/1000 | Loss: 0.002204\n",
      "Epoch: 020/025 | Batch 550/1000 | Loss: 0.002557\n",
      "Epoch: 020/025 | Batch 600/1000 | Loss: 0.001778\n",
      "Epoch: 020/025 | Batch 650/1000 | Loss: 0.000750\n",
      "Epoch: 020/025 | Batch 700/1000 | Loss: 0.001705\n",
      "Epoch: 020/025 | Batch 750/1000 | Loss: 0.001117\n",
      "Epoch: 020/025 | Batch 800/1000 | Loss: 0.000720\n",
      "Epoch: 020/025 | Batch 850/1000 | Loss: 0.000967\n",
      "Epoch: 020/025 | Batch 900/1000 | Loss: 0.002151\n",
      "Epoch: 020/025 | Batch 950/1000 | Loss: 0.000787\n",
      "Time elapsed: 0.13 min\n",
      "Epoch: 021/025 | Batch 000/1000 | Loss: 0.001198\n",
      "Epoch: 021/025 | Batch 050/1000 | Loss: 0.001113\n",
      "Epoch: 021/025 | Batch 100/1000 | Loss: 0.000804\n",
      "Epoch: 021/025 | Batch 150/1000 | Loss: 0.000799\n",
      "Epoch: 021/025 | Batch 200/1000 | Loss: 0.001176\n",
      "Epoch: 021/025 | Batch 250/1000 | Loss: 0.002561\n",
      "Epoch: 021/025 | Batch 300/1000 | Loss: 0.000920\n",
      "Epoch: 021/025 | Batch 350/1000 | Loss: 0.000851\n",
      "Epoch: 021/025 | Batch 400/1000 | Loss: 0.001154\n",
      "Epoch: 021/025 | Batch 450/1000 | Loss: 0.001165\n",
      "Epoch: 021/025 | Batch 500/1000 | Loss: 0.001320\n",
      "Epoch: 021/025 | Batch 550/1000 | Loss: 0.002534\n",
      "Epoch: 021/025 | Batch 600/1000 | Loss: 0.001421\n",
      "Epoch: 021/025 | Batch 650/1000 | Loss: 0.000759\n",
      "Epoch: 021/025 | Batch 700/1000 | Loss: 0.000897\n",
      "Epoch: 021/025 | Batch 750/1000 | Loss: 0.000786\n",
      "Epoch: 021/025 | Batch 800/1000 | Loss: 0.001816\n",
      "Epoch: 021/025 | Batch 850/1000 | Loss: 0.000498\n",
      "Epoch: 021/025 | Batch 900/1000 | Loss: 0.000841\n",
      "Epoch: 021/025 | Batch 950/1000 | Loss: 0.002065\n",
      "Time elapsed: 0.14 min\n",
      "Epoch: 022/025 | Batch 000/1000 | Loss: 0.001289\n",
      "Epoch: 022/025 | Batch 050/1000 | Loss: 0.000687\n",
      "Epoch: 022/025 | Batch 100/1000 | Loss: 0.001275\n",
      "Epoch: 022/025 | Batch 150/1000 | Loss: 0.000777\n",
      "Epoch: 022/025 | Batch 200/1000 | Loss: 0.002229\n",
      "Epoch: 022/025 | Batch 250/1000 | Loss: 0.000809\n",
      "Epoch: 022/025 | Batch 300/1000 | Loss: 0.001088\n",
      "Epoch: 022/025 | Batch 350/1000 | Loss: 0.002188\n",
      "Epoch: 022/025 | Batch 400/1000 | Loss: 0.000831\n",
      "Epoch: 022/025 | Batch 450/1000 | Loss: 0.002647\n",
      "Epoch: 022/025 | Batch 500/1000 | Loss: 0.001848\n",
      "Epoch: 022/025 | Batch 550/1000 | Loss: 0.001621\n",
      "Epoch: 022/025 | Batch 600/1000 | Loss: 0.000609\n",
      "Epoch: 022/025 | Batch 650/1000 | Loss: 0.000812\n",
      "Epoch: 022/025 | Batch 700/1000 | Loss: 0.001345\n",
      "Epoch: 022/025 | Batch 750/1000 | Loss: 0.000613\n",
      "Epoch: 022/025 | Batch 800/1000 | Loss: 0.000875\n",
      "Epoch: 022/025 | Batch 850/1000 | Loss: 0.000798\n",
      "Epoch: 022/025 | Batch 900/1000 | Loss: 0.000465\n",
      "Epoch: 022/025 | Batch 950/1000 | Loss: 0.002068\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 023/025 | Batch 000/1000 | Loss: 0.000434\n",
      "Epoch: 023/025 | Batch 050/1000 | Loss: 0.000958\n",
      "Epoch: 023/025 | Batch 100/1000 | Loss: 0.001635\n",
      "Epoch: 023/025 | Batch 150/1000 | Loss: 0.000882\n",
      "Epoch: 023/025 | Batch 200/1000 | Loss: 0.001349\n",
      "Epoch: 023/025 | Batch 250/1000 | Loss: 0.000887\n",
      "Epoch: 023/025 | Batch 300/1000 | Loss: 0.000989\n",
      "Epoch: 023/025 | Batch 350/1000 | Loss: 0.001613\n",
      "Epoch: 023/025 | Batch 400/1000 | Loss: 0.001285\n",
      "Epoch: 023/025 | Batch 450/1000 | Loss: 0.000909\n",
      "Epoch: 023/025 | Batch 500/1000 | Loss: 0.001153\n",
      "Epoch: 023/025 | Batch 550/1000 | Loss: 0.003836\n",
      "Epoch: 023/025 | Batch 600/1000 | Loss: 0.000892\n",
      "Epoch: 023/025 | Batch 650/1000 | Loss: 0.000687\n",
      "Epoch: 023/025 | Batch 700/1000 | Loss: 0.001087\n",
      "Epoch: 023/025 | Batch 750/1000 | Loss: 0.000645\n",
      "Epoch: 023/025 | Batch 800/1000 | Loss: 0.001677\n",
      "Epoch: 023/025 | Batch 850/1000 | Loss: 0.001063\n",
      "Epoch: 023/025 | Batch 900/1000 | Loss: 0.000519\n",
      "Epoch: 023/025 | Batch 950/1000 | Loss: 0.001321\n",
      "Time elapsed: 0.15 min\n",
      "Epoch: 024/025 | Batch 000/1000 | Loss: 0.000650\n",
      "Epoch: 024/025 | Batch 050/1000 | Loss: 0.000703\n",
      "Epoch: 024/025 | Batch 100/1000 | Loss: 0.000960\n",
      "Epoch: 024/025 | Batch 150/1000 | Loss: 0.000581\n",
      "Epoch: 024/025 | Batch 200/1000 | Loss: 0.002590\n",
      "Epoch: 024/025 | Batch 250/1000 | Loss: 0.001225\n",
      "Epoch: 024/025 | Batch 300/1000 | Loss: 0.001719\n",
      "Epoch: 024/025 | Batch 350/1000 | Loss: 0.001801\n",
      "Epoch: 024/025 | Batch 400/1000 | Loss: 0.002741\n",
      "Epoch: 024/025 | Batch 450/1000 | Loss: 0.003262\n",
      "Epoch: 024/025 | Batch 500/1000 | Loss: 0.001731\n",
      "Epoch: 024/025 | Batch 550/1000 | Loss: 0.001028\n",
      "Epoch: 024/025 | Batch 600/1000 | Loss: 0.000782\n",
      "Epoch: 024/025 | Batch 650/1000 | Loss: 0.000897\n",
      "Epoch: 024/025 | Batch 700/1000 | Loss: 0.001715\n",
      "Epoch: 024/025 | Batch 750/1000 | Loss: 0.000792\n",
      "Epoch: 024/025 | Batch 800/1000 | Loss: 0.000634\n",
      "Epoch: 024/025 | Batch 850/1000 | Loss: 0.002119\n",
      "Epoch: 024/025 | Batch 900/1000 | Loss: 0.000274\n",
      "Epoch: 024/025 | Batch 950/1000 | Loss: 0.000816\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 025/025 | Batch 000/1000 | Loss: 0.001390\n",
      "Epoch: 025/025 | Batch 050/1000 | Loss: 0.001059\n",
      "Epoch: 025/025 | Batch 100/1000 | Loss: 0.000869\n",
      "Epoch: 025/025 | Batch 150/1000 | Loss: 0.001077\n",
      "Epoch: 025/025 | Batch 200/1000 | Loss: 0.005154\n",
      "Epoch: 025/025 | Batch 250/1000 | Loss: 0.001551\n",
      "Epoch: 025/025 | Batch 300/1000 | Loss: 0.000769\n",
      "Epoch: 025/025 | Batch 350/1000 | Loss: 0.001228\n",
      "Epoch: 025/025 | Batch 400/1000 | Loss: 0.000804\n",
      "Epoch: 025/025 | Batch 450/1000 | Loss: 0.001627\n",
      "Epoch: 025/025 | Batch 500/1000 | Loss: 0.001577\n",
      "Epoch: 025/025 | Batch 550/1000 | Loss: 0.002254\n",
      "Epoch: 025/025 | Batch 600/1000 | Loss: 0.001184\n",
      "Epoch: 025/025 | Batch 650/1000 | Loss: 0.000567\n",
      "Epoch: 025/025 | Batch 700/1000 | Loss: 0.000860\n",
      "Epoch: 025/025 | Batch 750/1000 | Loss: 0.001136\n",
      "Epoch: 025/025 | Batch 800/1000 | Loss: 0.000527\n",
      "Epoch: 025/025 | Batch 850/1000 | Loss: 0.000504\n",
      "Epoch: 025/025 | Batch 900/1000 | Loss: 0.000998\n",
      "Epoch: 025/025 | Batch 950/1000 | Loss: 0.000339\n",
      "Time elapsed: 0.17 min\n",
      "Total Training Time: 0.17 min\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "my_model.trainer(train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error : 0.0% \n",
      "Test error : 16.1%\n",
      "The total number of the parameters is: 102466\n"
     ]
    }
   ],
   "source": [
    "# output the train error and test error\n",
    "print(\"Train error : %.1f%% \\nTest error : %.1f%%\" %\n",
    "      (my_model.compute_error(train_input, train_target),\n",
    "       my_model.compute_error(test_input, test_target)))\n",
    "\n",
    "print(\"The total number of the parameters is: %d\" % (sum(p.numel() for p in my_model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
