{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_practical_prologue as prologue\n",
    "#直接导入出现http403错误\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接导入出现http403错误\n",
    "# have to add a header to your urllib request (due to that site moving to Cloudflare protection)\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "#*********************** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "N_PAIRS = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2, 14, 14])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_input[0][0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_input[0][1],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.float32\n",
      "torch.Size([1000])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(train_input.dtype)\n",
    "print(train_target.shape)\n",
    "print(train_target.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(64,128,kernel_size=2)\n",
    "        self.conv2_drop=nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        #parameters\n",
    "        self.batch_size = 50\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.num_epochs = 25\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 512)))\n",
    "        x = F.dropout(x,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "        # Training Function\n",
    "\n",
    "    def trainer(self, train_input, train_target):\n",
    "        \"\"\"\n",
    "        Train the model on a training set\n",
    "        :param train_input: Training features\n",
    "        :param train_target: Training labels\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch_idx in range(0,train_input.size(0),self.batch_size):\n",
    "                output = self(train_input[batch_idx:batch_idx+self.batch_size]) \n",
    "                loss = self.criterion(output, train_target[batch_idx:batch_idx+self.batch_size])  \n",
    "                self.optimizer.zero_grad()                          #清零梯度\n",
    "                loss.backward()                                #反向求梯度\n",
    "                self.optimizer.step()\n",
    "#                 每隔50组数据，输出一次loss值\n",
    "                if not batch_idx % 50:\n",
    "                    print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.6f' \n",
    "                           %(epoch+1, self.num_epochs, batch_idx, \n",
    "                             len(train_input), loss))\n",
    "            print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "        \n",
    "\n",
    "        # Test error\n",
    "\n",
    "    def compute_error(self, input_data, target):\n",
    "        \"\"\"\n",
    "        Compute the number of error of the model on a test set\n",
    "        :param input_data: test features\n",
    "        :param target: test target\n",
    "        :return: error rate of the input data\n",
    "        \"\"\"  \n",
    "    \n",
    "        #测试模型\n",
    "        self.eval()      #测试模式，关闭正则化\n",
    "        errors = 0\n",
    "        for idx in range(0,input_data.size(0),self.batch_size):\n",
    "            input_batch=input_data.narrow(0,idx,self.batch_size)\n",
    "            outputs = self(input_batch)\n",
    "            _, predicted = torch.max(outputs, 1)   #返回值和索引\n",
    "            target_labels = target.narrow(0, idx, self.batch_size)\n",
    "            errors += torch.sum(predicted != target_labels)\n",
    "\n",
    "        return float(errors)*100/input_data.size(0)\n",
    "    def save_model(self,model_name):\n",
    "        \"\"\"\n",
    "        Save the model to a direction\n",
    "        :param model_name: the model name, e.g. CNN_Net.pth\n",
    "        \"\"\"         \n",
    "        torch.save(self, './model/'+ model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=CNN_Net()\n",
    "my_model.save_model('CNN_Net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/025 | Batch 000/1000 | Loss: 4.731062\n",
      "Epoch: 001/025 | Batch 050/1000 | Loss: 21.421619\n",
      "Epoch: 001/025 | Batch 100/1000 | Loss: 7.740621\n",
      "Epoch: 001/025 | Batch 150/1000 | Loss: 8.113548\n",
      "Epoch: 001/025 | Batch 200/1000 | Loss: 6.877905\n",
      "Epoch: 001/025 | Batch 250/1000 | Loss: 3.446219\n",
      "Epoch: 001/025 | Batch 300/1000 | Loss: 2.487326\n",
      "Epoch: 001/025 | Batch 350/1000 | Loss: 2.702688\n",
      "Epoch: 001/025 | Batch 400/1000 | Loss: 2.870310\n",
      "Epoch: 001/025 | Batch 450/1000 | Loss: 1.540186\n",
      "Epoch: 001/025 | Batch 500/1000 | Loss: 1.599501\n",
      "Epoch: 001/025 | Batch 550/1000 | Loss: 1.772361\n",
      "Epoch: 001/025 | Batch 600/1000 | Loss: 1.484443\n",
      "Epoch: 001/025 | Batch 650/1000 | Loss: 1.555787\n",
      "Epoch: 001/025 | Batch 700/1000 | Loss: 0.902332\n",
      "Epoch: 001/025 | Batch 750/1000 | Loss: 0.827083\n",
      "Epoch: 001/025 | Batch 800/1000 | Loss: 0.790280\n",
      "Epoch: 001/025 | Batch 850/1000 | Loss: 0.961580\n",
      "Epoch: 001/025 | Batch 900/1000 | Loss: 0.960126\n",
      "Epoch: 001/025 | Batch 950/1000 | Loss: 0.887761\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/025 | Batch 000/1000 | Loss: 0.573045\n",
      "Epoch: 002/025 | Batch 050/1000 | Loss: 0.738244\n",
      "Epoch: 002/025 | Batch 100/1000 | Loss: 0.527721\n",
      "Epoch: 002/025 | Batch 150/1000 | Loss: 0.665455\n",
      "Epoch: 002/025 | Batch 200/1000 | Loss: 0.729717\n",
      "Epoch: 002/025 | Batch 250/1000 | Loss: 0.673003\n",
      "Epoch: 002/025 | Batch 300/1000 | Loss: 0.709723\n",
      "Epoch: 002/025 | Batch 350/1000 | Loss: 0.617574\n",
      "Epoch: 002/025 | Batch 400/1000 | Loss: 0.601295\n",
      "Epoch: 002/025 | Batch 450/1000 | Loss: 0.616362\n",
      "Epoch: 002/025 | Batch 500/1000 | Loss: 0.559285\n",
      "Epoch: 002/025 | Batch 550/1000 | Loss: 0.654078\n",
      "Epoch: 002/025 | Batch 600/1000 | Loss: 0.652487\n",
      "Epoch: 002/025 | Batch 650/1000 | Loss: 0.736022\n",
      "Epoch: 002/025 | Batch 700/1000 | Loss: 0.572177\n",
      "Epoch: 002/025 | Batch 750/1000 | Loss: 0.619889\n",
      "Epoch: 002/025 | Batch 800/1000 | Loss: 0.678612\n",
      "Epoch: 002/025 | Batch 850/1000 | Loss: 0.603896\n",
      "Epoch: 002/025 | Batch 900/1000 | Loss: 0.591622\n",
      "Epoch: 002/025 | Batch 950/1000 | Loss: 0.631472\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/025 | Batch 000/1000 | Loss: 0.579435\n",
      "Epoch: 003/025 | Batch 050/1000 | Loss: 0.585374\n",
      "Epoch: 003/025 | Batch 100/1000 | Loss: 0.634947\n",
      "Epoch: 003/025 | Batch 150/1000 | Loss: 0.568242\n",
      "Epoch: 003/025 | Batch 200/1000 | Loss: 0.560983\n",
      "Epoch: 003/025 | Batch 250/1000 | Loss: 0.526275\n",
      "Epoch: 003/025 | Batch 300/1000 | Loss: 0.464430\n",
      "Epoch: 003/025 | Batch 350/1000 | Loss: 0.538014\n",
      "Epoch: 003/025 | Batch 400/1000 | Loss: 0.540557\n",
      "Epoch: 003/025 | Batch 450/1000 | Loss: 0.550866\n",
      "Epoch: 003/025 | Batch 500/1000 | Loss: 0.566700\n",
      "Epoch: 003/025 | Batch 550/1000 | Loss: 0.532265\n",
      "Epoch: 003/025 | Batch 600/1000 | Loss: 0.526688\n",
      "Epoch: 003/025 | Batch 650/1000 | Loss: 0.728579\n",
      "Epoch: 003/025 | Batch 700/1000 | Loss: 0.445289\n",
      "Epoch: 003/025 | Batch 750/1000 | Loss: 0.515461\n",
      "Epoch: 003/025 | Batch 800/1000 | Loss: 0.550428\n",
      "Epoch: 003/025 | Batch 850/1000 | Loss: 0.581231\n",
      "Epoch: 003/025 | Batch 900/1000 | Loss: 0.673026\n",
      "Epoch: 003/025 | Batch 950/1000 | Loss: 0.541919\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 004/025 | Batch 000/1000 | Loss: 0.538477\n",
      "Epoch: 004/025 | Batch 050/1000 | Loss: 0.451511\n",
      "Epoch: 004/025 | Batch 100/1000 | Loss: 0.600619\n",
      "Epoch: 004/025 | Batch 150/1000 | Loss: 0.549738\n",
      "Epoch: 004/025 | Batch 200/1000 | Loss: 0.456430\n",
      "Epoch: 004/025 | Batch 250/1000 | Loss: 0.510533\n",
      "Epoch: 004/025 | Batch 300/1000 | Loss: 0.381545\n",
      "Epoch: 004/025 | Batch 350/1000 | Loss: 0.456596\n",
      "Epoch: 004/025 | Batch 400/1000 | Loss: 0.520832\n",
      "Epoch: 004/025 | Batch 450/1000 | Loss: 0.488876\n",
      "Epoch: 004/025 | Batch 500/1000 | Loss: 0.498357\n",
      "Epoch: 004/025 | Batch 550/1000 | Loss: 0.511101\n",
      "Epoch: 004/025 | Batch 600/1000 | Loss: 0.494239\n",
      "Epoch: 004/025 | Batch 650/1000 | Loss: 0.556805\n",
      "Epoch: 004/025 | Batch 700/1000 | Loss: 0.474647\n",
      "Epoch: 004/025 | Batch 750/1000 | Loss: 0.467047\n",
      "Epoch: 004/025 | Batch 800/1000 | Loss: 0.567605\n",
      "Epoch: 004/025 | Batch 850/1000 | Loss: 0.442505\n",
      "Epoch: 004/025 | Batch 900/1000 | Loss: 0.588399\n",
      "Epoch: 004/025 | Batch 950/1000 | Loss: 0.593707\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 005/025 | Batch 000/1000 | Loss: 0.459058\n",
      "Epoch: 005/025 | Batch 050/1000 | Loss: 0.447412\n",
      "Epoch: 005/025 | Batch 100/1000 | Loss: 0.444444\n",
      "Epoch: 005/025 | Batch 150/1000 | Loss: 0.497926\n",
      "Epoch: 005/025 | Batch 200/1000 | Loss: 0.446190\n",
      "Epoch: 005/025 | Batch 250/1000 | Loss: 0.546939\n",
      "Epoch: 005/025 | Batch 300/1000 | Loss: 0.315360\n",
      "Epoch: 005/025 | Batch 350/1000 | Loss: 0.447394\n",
      "Epoch: 005/025 | Batch 400/1000 | Loss: 0.492070\n",
      "Epoch: 005/025 | Batch 450/1000 | Loss: 0.443280\n",
      "Epoch: 005/025 | Batch 500/1000 | Loss: 0.463187\n",
      "Epoch: 005/025 | Batch 550/1000 | Loss: 0.457195\n",
      "Epoch: 005/025 | Batch 600/1000 | Loss: 0.478632\n",
      "Epoch: 005/025 | Batch 650/1000 | Loss: 0.552571\n",
      "Epoch: 005/025 | Batch 700/1000 | Loss: 0.421226\n",
      "Epoch: 005/025 | Batch 750/1000 | Loss: 0.344561\n",
      "Epoch: 005/025 | Batch 800/1000 | Loss: 0.468969\n",
      "Epoch: 005/025 | Batch 850/1000 | Loss: 0.384089\n",
      "Epoch: 005/025 | Batch 900/1000 | Loss: 0.450230\n",
      "Epoch: 005/025 | Batch 950/1000 | Loss: 0.428151\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 006/025 | Batch 000/1000 | Loss: 0.426197\n",
      "Epoch: 006/025 | Batch 050/1000 | Loss: 0.342667\n",
      "Epoch: 006/025 | Batch 100/1000 | Loss: 0.387906\n",
      "Epoch: 006/025 | Batch 150/1000 | Loss: 0.392275\n",
      "Epoch: 006/025 | Batch 200/1000 | Loss: 0.445420\n",
      "Epoch: 006/025 | Batch 250/1000 | Loss: 0.400634\n",
      "Epoch: 006/025 | Batch 300/1000 | Loss: 0.378340\n",
      "Epoch: 006/025 | Batch 350/1000 | Loss: 0.349199\n",
      "Epoch: 006/025 | Batch 400/1000 | Loss: 0.320231\n",
      "Epoch: 006/025 | Batch 450/1000 | Loss: 0.427471\n",
      "Epoch: 006/025 | Batch 500/1000 | Loss: 0.307822\n",
      "Epoch: 006/025 | Batch 550/1000 | Loss: 0.439976\n",
      "Epoch: 006/025 | Batch 600/1000 | Loss: 0.429583\n",
      "Epoch: 006/025 | Batch 650/1000 | Loss: 0.481947\n",
      "Epoch: 006/025 | Batch 700/1000 | Loss: 0.403157\n",
      "Epoch: 006/025 | Batch 750/1000 | Loss: 0.281435\n",
      "Epoch: 006/025 | Batch 800/1000 | Loss: 0.400174\n",
      "Epoch: 006/025 | Batch 850/1000 | Loss: 0.376436\n",
      "Epoch: 006/025 | Batch 900/1000 | Loss: 0.389926\n",
      "Epoch: 006/025 | Batch 950/1000 | Loss: 0.406990\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 007/025 | Batch 000/1000 | Loss: 0.323499\n",
      "Epoch: 007/025 | Batch 050/1000 | Loss: 0.314628\n",
      "Epoch: 007/025 | Batch 100/1000 | Loss: 0.329685\n",
      "Epoch: 007/025 | Batch 150/1000 | Loss: 0.308174\n",
      "Epoch: 007/025 | Batch 200/1000 | Loss: 0.299277\n",
      "Epoch: 007/025 | Batch 250/1000 | Loss: 0.278325\n",
      "Epoch: 007/025 | Batch 300/1000 | Loss: 0.372584\n",
      "Epoch: 007/025 | Batch 350/1000 | Loss: 0.340492\n",
      "Epoch: 007/025 | Batch 400/1000 | Loss: 0.267975\n",
      "Epoch: 007/025 | Batch 450/1000 | Loss: 0.346298\n",
      "Epoch: 007/025 | Batch 500/1000 | Loss: 0.326139\n",
      "Epoch: 007/025 | Batch 550/1000 | Loss: 0.381250\n",
      "Epoch: 007/025 | Batch 600/1000 | Loss: 0.272025\n",
      "Epoch: 007/025 | Batch 650/1000 | Loss: 0.470645\n",
      "Epoch: 007/025 | Batch 700/1000 | Loss: 0.293318\n",
      "Epoch: 007/025 | Batch 750/1000 | Loss: 0.348284\n",
      "Epoch: 007/025 | Batch 800/1000 | Loss: 0.395178\n",
      "Epoch: 007/025 | Batch 850/1000 | Loss: 0.377254\n",
      "Epoch: 007/025 | Batch 900/1000 | Loss: 0.368299\n",
      "Epoch: 007/025 | Batch 950/1000 | Loss: 0.317998\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 008/025 | Batch 000/1000 | Loss: 0.226172\n",
      "Epoch: 008/025 | Batch 050/1000 | Loss: 0.267328\n",
      "Epoch: 008/025 | Batch 100/1000 | Loss: 0.360527\n",
      "Epoch: 008/025 | Batch 150/1000 | Loss: 0.385709\n",
      "Epoch: 008/025 | Batch 200/1000 | Loss: 0.333758\n",
      "Epoch: 008/025 | Batch 250/1000 | Loss: 0.372300\n",
      "Epoch: 008/025 | Batch 300/1000 | Loss: 0.193387\n",
      "Epoch: 008/025 | Batch 350/1000 | Loss: 0.249546\n",
      "Epoch: 008/025 | Batch 400/1000 | Loss: 0.226112\n",
      "Epoch: 008/025 | Batch 450/1000 | Loss: 0.370683\n",
      "Epoch: 008/025 | Batch 500/1000 | Loss: 0.331729\n",
      "Epoch: 008/025 | Batch 550/1000 | Loss: 0.284790\n",
      "Epoch: 008/025 | Batch 600/1000 | Loss: 0.266709\n",
      "Epoch: 008/025 | Batch 650/1000 | Loss: 0.438677\n",
      "Epoch: 008/025 | Batch 700/1000 | Loss: 0.293038\n",
      "Epoch: 008/025 | Batch 750/1000 | Loss: 0.298480\n",
      "Epoch: 008/025 | Batch 800/1000 | Loss: 0.312670\n",
      "Epoch: 008/025 | Batch 850/1000 | Loss: 0.284073\n",
      "Epoch: 008/025 | Batch 900/1000 | Loss: 0.266321\n",
      "Epoch: 008/025 | Batch 950/1000 | Loss: 0.271202\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 009/025 | Batch 000/1000 | Loss: 0.231667\n",
      "Epoch: 009/025 | Batch 050/1000 | Loss: 0.165279\n",
      "Epoch: 009/025 | Batch 100/1000 | Loss: 0.349806\n",
      "Epoch: 009/025 | Batch 150/1000 | Loss: 0.252171\n",
      "Epoch: 009/025 | Batch 200/1000 | Loss: 0.336492\n",
      "Epoch: 009/025 | Batch 250/1000 | Loss: 0.349646\n",
      "Epoch: 009/025 | Batch 300/1000 | Loss: 0.117548\n",
      "Epoch: 009/025 | Batch 350/1000 | Loss: 0.229992\n",
      "Epoch: 009/025 | Batch 400/1000 | Loss: 0.195837\n",
      "Epoch: 009/025 | Batch 450/1000 | Loss: 0.356824\n",
      "Epoch: 009/025 | Batch 500/1000 | Loss: 0.323379\n",
      "Epoch: 009/025 | Batch 550/1000 | Loss: 0.190583\n",
      "Epoch: 009/025 | Batch 600/1000 | Loss: 0.224648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009/025 | Batch 650/1000 | Loss: 0.404976\n",
      "Epoch: 009/025 | Batch 700/1000 | Loss: 0.423478\n",
      "Epoch: 009/025 | Batch 750/1000 | Loss: 0.230371\n",
      "Epoch: 009/025 | Batch 800/1000 | Loss: 0.329527\n",
      "Epoch: 009/025 | Batch 850/1000 | Loss: 0.230468\n",
      "Epoch: 009/025 | Batch 900/1000 | Loss: 0.237628\n",
      "Epoch: 009/025 | Batch 950/1000 | Loss: 0.211852\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 010/025 | Batch 000/1000 | Loss: 0.201320\n",
      "Epoch: 010/025 | Batch 050/1000 | Loss: 0.165417\n",
      "Epoch: 010/025 | Batch 100/1000 | Loss: 0.263065\n",
      "Epoch: 010/025 | Batch 150/1000 | Loss: 0.157586\n",
      "Epoch: 010/025 | Batch 200/1000 | Loss: 0.168193\n",
      "Epoch: 010/025 | Batch 250/1000 | Loss: 0.126641\n",
      "Epoch: 010/025 | Batch 300/1000 | Loss: 0.216775\n",
      "Epoch: 010/025 | Batch 350/1000 | Loss: 0.231797\n",
      "Epoch: 010/025 | Batch 400/1000 | Loss: 0.157640\n",
      "Epoch: 010/025 | Batch 450/1000 | Loss: 0.167629\n",
      "Epoch: 010/025 | Batch 500/1000 | Loss: 0.227921\n",
      "Epoch: 010/025 | Batch 550/1000 | Loss: 0.206626\n",
      "Epoch: 010/025 | Batch 600/1000 | Loss: 0.100280\n",
      "Epoch: 010/025 | Batch 650/1000 | Loss: 0.248691\n",
      "Epoch: 010/025 | Batch 700/1000 | Loss: 0.196558\n",
      "Epoch: 010/025 | Batch 750/1000 | Loss: 0.174794\n",
      "Epoch: 010/025 | Batch 800/1000 | Loss: 0.160502\n",
      "Epoch: 010/025 | Batch 850/1000 | Loss: 0.181809\n",
      "Epoch: 010/025 | Batch 900/1000 | Loss: 0.200313\n",
      "Epoch: 010/025 | Batch 950/1000 | Loss: 0.238089\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 011/025 | Batch 000/1000 | Loss: 0.269489\n",
      "Epoch: 011/025 | Batch 050/1000 | Loss: 0.118494\n",
      "Epoch: 011/025 | Batch 100/1000 | Loss: 0.197406\n",
      "Epoch: 011/025 | Batch 150/1000 | Loss: 0.073923\n",
      "Epoch: 011/025 | Batch 200/1000 | Loss: 0.309551\n",
      "Epoch: 011/025 | Batch 250/1000 | Loss: 0.209306\n",
      "Epoch: 011/025 | Batch 300/1000 | Loss: 0.085648\n",
      "Epoch: 011/025 | Batch 350/1000 | Loss: 0.249824\n",
      "Epoch: 011/025 | Batch 400/1000 | Loss: 0.125047\n",
      "Epoch: 011/025 | Batch 450/1000 | Loss: 0.112390\n",
      "Epoch: 011/025 | Batch 500/1000 | Loss: 0.208827\n",
      "Epoch: 011/025 | Batch 550/1000 | Loss: 0.263713\n",
      "Epoch: 011/025 | Batch 600/1000 | Loss: 0.116564\n",
      "Epoch: 011/025 | Batch 650/1000 | Loss: 0.351912\n",
      "Epoch: 011/025 | Batch 700/1000 | Loss: 0.229737\n",
      "Epoch: 011/025 | Batch 750/1000 | Loss: 0.260394\n",
      "Epoch: 011/025 | Batch 800/1000 | Loss: 0.212253\n",
      "Epoch: 011/025 | Batch 850/1000 | Loss: 0.192551\n",
      "Epoch: 011/025 | Batch 900/1000 | Loss: 0.253873\n",
      "Epoch: 011/025 | Batch 950/1000 | Loss: 0.265099\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 012/025 | Batch 000/1000 | Loss: 0.186336\n",
      "Epoch: 012/025 | Batch 050/1000 | Loss: 0.119289\n",
      "Epoch: 012/025 | Batch 100/1000 | Loss: 0.210292\n",
      "Epoch: 012/025 | Batch 150/1000 | Loss: 0.224661\n",
      "Epoch: 012/025 | Batch 200/1000 | Loss: 0.229896\n",
      "Epoch: 012/025 | Batch 250/1000 | Loss: 0.130777\n",
      "Epoch: 012/025 | Batch 300/1000 | Loss: 0.082506\n",
      "Epoch: 012/025 | Batch 350/1000 | Loss: 0.132155\n",
      "Epoch: 012/025 | Batch 400/1000 | Loss: 0.131071\n",
      "Epoch: 012/025 | Batch 450/1000 | Loss: 0.123779\n",
      "Epoch: 012/025 | Batch 500/1000 | Loss: 0.117508\n",
      "Epoch: 012/025 | Batch 550/1000 | Loss: 0.131286\n",
      "Epoch: 012/025 | Batch 600/1000 | Loss: 0.149029\n",
      "Epoch: 012/025 | Batch 650/1000 | Loss: 0.226095\n",
      "Epoch: 012/025 | Batch 700/1000 | Loss: 0.116433\n",
      "Epoch: 012/025 | Batch 750/1000 | Loss: 0.154123\n",
      "Epoch: 012/025 | Batch 800/1000 | Loss: 0.198942\n",
      "Epoch: 012/025 | Batch 850/1000 | Loss: 0.142949\n",
      "Epoch: 012/025 | Batch 900/1000 | Loss: 0.091713\n",
      "Epoch: 012/025 | Batch 950/1000 | Loss: 0.135085\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 013/025 | Batch 000/1000 | Loss: 0.107842\n",
      "Epoch: 013/025 | Batch 050/1000 | Loss: 0.074750\n",
      "Epoch: 013/025 | Batch 100/1000 | Loss: 0.273414\n",
      "Epoch: 013/025 | Batch 150/1000 | Loss: 0.111845\n",
      "Epoch: 013/025 | Batch 200/1000 | Loss: 0.128742\n",
      "Epoch: 013/025 | Batch 250/1000 | Loss: 0.149046\n",
      "Epoch: 013/025 | Batch 300/1000 | Loss: 0.059632\n",
      "Epoch: 013/025 | Batch 350/1000 | Loss: 0.272638\n",
      "Epoch: 013/025 | Batch 400/1000 | Loss: 0.125197\n",
      "Epoch: 013/025 | Batch 450/1000 | Loss: 0.068142\n",
      "Epoch: 013/025 | Batch 500/1000 | Loss: 0.209823\n",
      "Epoch: 013/025 | Batch 550/1000 | Loss: 0.133152\n",
      "Epoch: 013/025 | Batch 600/1000 | Loss: 0.287062\n",
      "Epoch: 013/025 | Batch 650/1000 | Loss: 0.164358\n",
      "Epoch: 013/025 | Batch 700/1000 | Loss: 0.059015\n",
      "Epoch: 013/025 | Batch 750/1000 | Loss: 0.076990\n",
      "Epoch: 013/025 | Batch 800/1000 | Loss: 0.237572\n",
      "Epoch: 013/025 | Batch 850/1000 | Loss: 0.189492\n",
      "Epoch: 013/025 | Batch 900/1000 | Loss: 0.105664\n",
      "Epoch: 013/025 | Batch 950/1000 | Loss: 0.151212\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 014/025 | Batch 000/1000 | Loss: 0.136928\n",
      "Epoch: 014/025 | Batch 050/1000 | Loss: 0.077486\n",
      "Epoch: 014/025 | Batch 100/1000 | Loss: 0.206354\n",
      "Epoch: 014/025 | Batch 150/1000 | Loss: 0.130979\n",
      "Epoch: 014/025 | Batch 200/1000 | Loss: 0.227820\n",
      "Epoch: 014/025 | Batch 250/1000 | Loss: 0.092258\n",
      "Epoch: 014/025 | Batch 300/1000 | Loss: 0.101472\n",
      "Epoch: 014/025 | Batch 350/1000 | Loss: 0.164238\n",
      "Epoch: 014/025 | Batch 400/1000 | Loss: 0.114747\n",
      "Epoch: 014/025 | Batch 450/1000 | Loss: 0.121462\n",
      "Epoch: 014/025 | Batch 500/1000 | Loss: 0.134388\n",
      "Epoch: 014/025 | Batch 550/1000 | Loss: 0.100622\n",
      "Epoch: 014/025 | Batch 600/1000 | Loss: 0.081170\n",
      "Epoch: 014/025 | Batch 650/1000 | Loss: 0.113610\n",
      "Epoch: 014/025 | Batch 700/1000 | Loss: 0.235887\n",
      "Epoch: 014/025 | Batch 750/1000 | Loss: 0.192499\n",
      "Epoch: 014/025 | Batch 800/1000 | Loss: 0.150184\n",
      "Epoch: 014/025 | Batch 850/1000 | Loss: 0.112312\n",
      "Epoch: 014/025 | Batch 900/1000 | Loss: 0.074413\n",
      "Epoch: 014/025 | Batch 950/1000 | Loss: 0.081298\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 015/025 | Batch 000/1000 | Loss: 0.111077\n",
      "Epoch: 015/025 | Batch 050/1000 | Loss: 0.198215\n",
      "Epoch: 015/025 | Batch 100/1000 | Loss: 0.054148\n",
      "Epoch: 015/025 | Batch 150/1000 | Loss: 0.080890\n",
      "Epoch: 015/025 | Batch 200/1000 | Loss: 0.106416\n",
      "Epoch: 015/025 | Batch 250/1000 | Loss: 0.046914\n",
      "Epoch: 015/025 | Batch 300/1000 | Loss: 0.078341\n",
      "Epoch: 015/025 | Batch 350/1000 | Loss: 0.155448\n",
      "Epoch: 015/025 | Batch 400/1000 | Loss: 0.047500\n",
      "Epoch: 015/025 | Batch 450/1000 | Loss: 0.038486\n",
      "Epoch: 015/025 | Batch 500/1000 | Loss: 0.061947\n",
      "Epoch: 015/025 | Batch 550/1000 | Loss: 0.102267\n",
      "Epoch: 015/025 | Batch 600/1000 | Loss: 0.029858\n",
      "Epoch: 015/025 | Batch 650/1000 | Loss: 0.070778\n",
      "Epoch: 015/025 | Batch 700/1000 | Loss: 0.037744\n",
      "Epoch: 015/025 | Batch 750/1000 | Loss: 0.181318\n",
      "Epoch: 015/025 | Batch 800/1000 | Loss: 0.117087\n",
      "Epoch: 015/025 | Batch 850/1000 | Loss: 0.069410\n",
      "Epoch: 015/025 | Batch 900/1000 | Loss: 0.079814\n",
      "Epoch: 015/025 | Batch 950/1000 | Loss: 0.064740\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 016/025 | Batch 000/1000 | Loss: 0.061761\n",
      "Epoch: 016/025 | Batch 050/1000 | Loss: 0.070197\n",
      "Epoch: 016/025 | Batch 100/1000 | Loss: 0.075253\n",
      "Epoch: 016/025 | Batch 150/1000 | Loss: 0.031266\n",
      "Epoch: 016/025 | Batch 200/1000 | Loss: 0.064606\n",
      "Epoch: 016/025 | Batch 250/1000 | Loss: 0.028166\n",
      "Epoch: 016/025 | Batch 300/1000 | Loss: 0.053810\n",
      "Epoch: 016/025 | Batch 350/1000 | Loss: 0.082777\n",
      "Epoch: 016/025 | Batch 400/1000 | Loss: 0.036575\n",
      "Epoch: 016/025 | Batch 450/1000 | Loss: 0.049753\n",
      "Epoch: 016/025 | Batch 500/1000 | Loss: 0.035560\n",
      "Epoch: 016/025 | Batch 550/1000 | Loss: 0.104927\n",
      "Epoch: 016/025 | Batch 600/1000 | Loss: 0.024542\n",
      "Epoch: 016/025 | Batch 650/1000 | Loss: 0.136153\n",
      "Epoch: 016/025 | Batch 700/1000 | Loss: 0.038507\n",
      "Epoch: 016/025 | Batch 750/1000 | Loss: 0.038492\n",
      "Epoch: 016/025 | Batch 800/1000 | Loss: 0.119533\n",
      "Epoch: 016/025 | Batch 850/1000 | Loss: 0.034378\n",
      "Epoch: 016/025 | Batch 900/1000 | Loss: 0.076631\n",
      "Epoch: 016/025 | Batch 950/1000 | Loss: 0.077523\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 017/025 | Batch 000/1000 | Loss: 0.025439\n",
      "Epoch: 017/025 | Batch 050/1000 | Loss: 0.014967\n",
      "Epoch: 017/025 | Batch 100/1000 | Loss: 0.025406\n",
      "Epoch: 017/025 | Batch 150/1000 | Loss: 0.092801\n",
      "Epoch: 017/025 | Batch 200/1000 | Loss: 0.050715\n",
      "Epoch: 017/025 | Batch 250/1000 | Loss: 0.070019\n",
      "Epoch: 017/025 | Batch 300/1000 | Loss: 0.019367\n",
      "Epoch: 017/025 | Batch 350/1000 | Loss: 0.027281\n",
      "Epoch: 017/025 | Batch 400/1000 | Loss: 0.023533\n",
      "Epoch: 017/025 | Batch 450/1000 | Loss: 0.016252\n",
      "Epoch: 017/025 | Batch 500/1000 | Loss: 0.015885\n",
      "Epoch: 017/025 | Batch 550/1000 | Loss: 0.077721\n",
      "Epoch: 017/025 | Batch 600/1000 | Loss: 0.045676\n",
      "Epoch: 017/025 | Batch 650/1000 | Loss: 0.095024\n",
      "Epoch: 017/025 | Batch 700/1000 | Loss: 0.051493\n",
      "Epoch: 017/025 | Batch 750/1000 | Loss: 0.045704\n",
      "Epoch: 017/025 | Batch 800/1000 | Loss: 0.066456\n",
      "Epoch: 017/025 | Batch 850/1000 | Loss: 0.039687\n",
      "Epoch: 017/025 | Batch 900/1000 | Loss: 0.055327\n",
      "Epoch: 017/025 | Batch 950/1000 | Loss: 0.042512\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 018/025 | Batch 000/1000 | Loss: 0.042299\n",
      "Epoch: 018/025 | Batch 050/1000 | Loss: 0.028543\n",
      "Epoch: 018/025 | Batch 100/1000 | Loss: 0.038006\n",
      "Epoch: 018/025 | Batch 150/1000 | Loss: 0.033951\n",
      "Epoch: 018/025 | Batch 200/1000 | Loss: 0.061451\n",
      "Epoch: 018/025 | Batch 250/1000 | Loss: 0.040797\n",
      "Epoch: 018/025 | Batch 300/1000 | Loss: 0.021935\n",
      "Epoch: 018/025 | Batch 350/1000 | Loss: 0.043018\n",
      "Epoch: 018/025 | Batch 400/1000 | Loss: 0.024615\n",
      "Epoch: 018/025 | Batch 450/1000 | Loss: 0.022348\n",
      "Epoch: 018/025 | Batch 500/1000 | Loss: 0.037693\n",
      "Epoch: 018/025 | Batch 550/1000 | Loss: 0.019750\n",
      "Epoch: 018/025 | Batch 600/1000 | Loss: 0.044843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018/025 | Batch 650/1000 | Loss: 0.010801\n",
      "Epoch: 018/025 | Batch 700/1000 | Loss: 0.051363\n",
      "Epoch: 018/025 | Batch 750/1000 | Loss: 0.025041\n",
      "Epoch: 018/025 | Batch 800/1000 | Loss: 0.017854\n",
      "Epoch: 018/025 | Batch 850/1000 | Loss: 0.029632\n",
      "Epoch: 018/025 | Batch 900/1000 | Loss: 0.014940\n",
      "Epoch: 018/025 | Batch 950/1000 | Loss: 0.030197\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 019/025 | Batch 000/1000 | Loss: 0.020383\n",
      "Epoch: 019/025 | Batch 050/1000 | Loss: 0.028278\n",
      "Epoch: 019/025 | Batch 100/1000 | Loss: 0.050992\n",
      "Epoch: 019/025 | Batch 150/1000 | Loss: 0.031504\n",
      "Epoch: 019/025 | Batch 200/1000 | Loss: 0.017007\n",
      "Epoch: 019/025 | Batch 250/1000 | Loss: 0.009264\n",
      "Epoch: 019/025 | Batch 300/1000 | Loss: 0.017830\n",
      "Epoch: 019/025 | Batch 350/1000 | Loss: 0.032291\n",
      "Epoch: 019/025 | Batch 400/1000 | Loss: 0.033403\n",
      "Epoch: 019/025 | Batch 450/1000 | Loss: 0.022682\n",
      "Epoch: 019/025 | Batch 500/1000 | Loss: 0.031248\n",
      "Epoch: 019/025 | Batch 550/1000 | Loss: 0.005878\n",
      "Epoch: 019/025 | Batch 600/1000 | Loss: 0.023204\n",
      "Epoch: 019/025 | Batch 650/1000 | Loss: 0.021654\n",
      "Epoch: 019/025 | Batch 700/1000 | Loss: 0.054388\n",
      "Epoch: 019/025 | Batch 750/1000 | Loss: 0.030901\n",
      "Epoch: 019/025 | Batch 800/1000 | Loss: 0.057319\n",
      "Epoch: 019/025 | Batch 850/1000 | Loss: 0.012230\n",
      "Epoch: 019/025 | Batch 900/1000 | Loss: 0.035410\n",
      "Epoch: 019/025 | Batch 950/1000 | Loss: 0.012657\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 020/025 | Batch 000/1000 | Loss: 0.047899\n",
      "Epoch: 020/025 | Batch 050/1000 | Loss: 0.011100\n",
      "Epoch: 020/025 | Batch 100/1000 | Loss: 0.015701\n",
      "Epoch: 020/025 | Batch 150/1000 | Loss: 0.037976\n",
      "Epoch: 020/025 | Batch 200/1000 | Loss: 0.018811\n",
      "Epoch: 020/025 | Batch 250/1000 | Loss: 0.080132\n",
      "Epoch: 020/025 | Batch 300/1000 | Loss: 0.022394\n",
      "Epoch: 020/025 | Batch 350/1000 | Loss: 0.047923\n",
      "Epoch: 020/025 | Batch 400/1000 | Loss: 0.015917\n",
      "Epoch: 020/025 | Batch 450/1000 | Loss: 0.011802\n",
      "Epoch: 020/025 | Batch 500/1000 | Loss: 0.004915\n",
      "Epoch: 020/025 | Batch 550/1000 | Loss: 0.032534\n",
      "Epoch: 020/025 | Batch 600/1000 | Loss: 0.019221\n",
      "Epoch: 020/025 | Batch 650/1000 | Loss: 0.016089\n",
      "Epoch: 020/025 | Batch 700/1000 | Loss: 0.040766\n",
      "Epoch: 020/025 | Batch 750/1000 | Loss: 0.088732\n",
      "Epoch: 020/025 | Batch 800/1000 | Loss: 0.059572\n",
      "Epoch: 020/025 | Batch 850/1000 | Loss: 0.061524\n",
      "Epoch: 020/025 | Batch 900/1000 | Loss: 0.027651\n",
      "Epoch: 020/025 | Batch 950/1000 | Loss: 0.019945\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 021/025 | Batch 000/1000 | Loss: 0.019171\n",
      "Epoch: 021/025 | Batch 050/1000 | Loss: 0.048587\n",
      "Epoch: 021/025 | Batch 100/1000 | Loss: 0.026183\n",
      "Epoch: 021/025 | Batch 150/1000 | Loss: 0.010609\n",
      "Epoch: 021/025 | Batch 200/1000 | Loss: 0.017761\n",
      "Epoch: 021/025 | Batch 250/1000 | Loss: 0.022434\n",
      "Epoch: 021/025 | Batch 300/1000 | Loss: 0.009484\n",
      "Epoch: 021/025 | Batch 350/1000 | Loss: 0.023150\n",
      "Epoch: 021/025 | Batch 400/1000 | Loss: 0.028838\n",
      "Epoch: 021/025 | Batch 450/1000 | Loss: 0.084645\n",
      "Epoch: 021/025 | Batch 500/1000 | Loss: 0.031406\n",
      "Epoch: 021/025 | Batch 550/1000 | Loss: 0.006326\n",
      "Epoch: 021/025 | Batch 600/1000 | Loss: 0.008587\n",
      "Epoch: 021/025 | Batch 650/1000 | Loss: 0.045901\n",
      "Epoch: 021/025 | Batch 700/1000 | Loss: 0.023168\n",
      "Epoch: 021/025 | Batch 750/1000 | Loss: 0.020086\n",
      "Epoch: 021/025 | Batch 800/1000 | Loss: 0.055364\n",
      "Epoch: 021/025 | Batch 850/1000 | Loss: 0.023896\n",
      "Epoch: 021/025 | Batch 900/1000 | Loss: 0.152737\n",
      "Epoch: 021/025 | Batch 950/1000 | Loss: 0.016929\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 022/025 | Batch 000/1000 | Loss: 0.017670\n",
      "Epoch: 022/025 | Batch 050/1000 | Loss: 0.034526\n",
      "Epoch: 022/025 | Batch 100/1000 | Loss: 0.026305\n",
      "Epoch: 022/025 | Batch 150/1000 | Loss: 0.063317\n",
      "Epoch: 022/025 | Batch 200/1000 | Loss: 0.093546\n",
      "Epoch: 022/025 | Batch 250/1000 | Loss: 0.024594\n",
      "Epoch: 022/025 | Batch 300/1000 | Loss: 0.018012\n",
      "Epoch: 022/025 | Batch 350/1000 | Loss: 0.023955\n",
      "Epoch: 022/025 | Batch 400/1000 | Loss: 0.006996\n",
      "Epoch: 022/025 | Batch 450/1000 | Loss: 0.032960\n",
      "Epoch: 022/025 | Batch 500/1000 | Loss: 0.019352\n",
      "Epoch: 022/025 | Batch 550/1000 | Loss: 0.035469\n",
      "Epoch: 022/025 | Batch 600/1000 | Loss: 0.011306\n",
      "Epoch: 022/025 | Batch 650/1000 | Loss: 0.030654\n",
      "Epoch: 022/025 | Batch 700/1000 | Loss: 0.022058\n",
      "Epoch: 022/025 | Batch 750/1000 | Loss: 0.024365\n",
      "Epoch: 022/025 | Batch 800/1000 | Loss: 0.028836\n",
      "Epoch: 022/025 | Batch 850/1000 | Loss: 0.078042\n",
      "Epoch: 022/025 | Batch 900/1000 | Loss: 0.058716\n",
      "Epoch: 022/025 | Batch 950/1000 | Loss: 0.082188\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 023/025 | Batch 000/1000 | Loss: 0.026538\n",
      "Epoch: 023/025 | Batch 050/1000 | Loss: 0.005932\n",
      "Epoch: 023/025 | Batch 100/1000 | Loss: 0.034869\n",
      "Epoch: 023/025 | Batch 150/1000 | Loss: 0.036110\n",
      "Epoch: 023/025 | Batch 200/1000 | Loss: 0.065824\n",
      "Epoch: 023/025 | Batch 250/1000 | Loss: 0.056818\n",
      "Epoch: 023/025 | Batch 300/1000 | Loss: 0.002847\n",
      "Epoch: 023/025 | Batch 350/1000 | Loss: 0.033592\n",
      "Epoch: 023/025 | Batch 400/1000 | Loss: 0.102010\n",
      "Epoch: 023/025 | Batch 450/1000 | Loss: 0.026490\n",
      "Epoch: 023/025 | Batch 500/1000 | Loss: 0.062802\n",
      "Epoch: 023/025 | Batch 550/1000 | Loss: 0.041519\n",
      "Epoch: 023/025 | Batch 600/1000 | Loss: 0.030096\n",
      "Epoch: 023/025 | Batch 650/1000 | Loss: 0.069811\n",
      "Epoch: 023/025 | Batch 700/1000 | Loss: 0.041265\n",
      "Epoch: 023/025 | Batch 750/1000 | Loss: 0.101646\n",
      "Epoch: 023/025 | Batch 800/1000 | Loss: 0.129341\n",
      "Epoch: 023/025 | Batch 850/1000 | Loss: 0.034423\n",
      "Epoch: 023/025 | Batch 900/1000 | Loss: 0.012539\n",
      "Epoch: 023/025 | Batch 950/1000 | Loss: 0.050510\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 024/025 | Batch 000/1000 | Loss: 0.024169\n",
      "Epoch: 024/025 | Batch 050/1000 | Loss: 0.009063\n",
      "Epoch: 024/025 | Batch 100/1000 | Loss: 0.017680\n",
      "Epoch: 024/025 | Batch 150/1000 | Loss: 0.047431\n",
      "Epoch: 024/025 | Batch 200/1000 | Loss: 0.029548\n",
      "Epoch: 024/025 | Batch 250/1000 | Loss: 0.063228\n",
      "Epoch: 024/025 | Batch 300/1000 | Loss: 0.011424\n",
      "Epoch: 024/025 | Batch 350/1000 | Loss: 0.041644\n",
      "Epoch: 024/025 | Batch 400/1000 | Loss: 0.021144\n",
      "Epoch: 024/025 | Batch 450/1000 | Loss: 0.056186\n",
      "Epoch: 024/025 | Batch 500/1000 | Loss: 0.034025\n",
      "Epoch: 024/025 | Batch 550/1000 | Loss: 0.019825\n",
      "Epoch: 024/025 | Batch 600/1000 | Loss: 0.069601\n",
      "Epoch: 024/025 | Batch 650/1000 | Loss: 0.039288\n",
      "Epoch: 024/025 | Batch 700/1000 | Loss: 0.048438\n",
      "Epoch: 024/025 | Batch 750/1000 | Loss: 0.007137\n",
      "Epoch: 024/025 | Batch 800/1000 | Loss: 0.008740\n",
      "Epoch: 024/025 | Batch 850/1000 | Loss: 0.013044\n",
      "Epoch: 024/025 | Batch 900/1000 | Loss: 0.073198\n",
      "Epoch: 024/025 | Batch 950/1000 | Loss: 0.024745\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 025/025 | Batch 000/1000 | Loss: 0.141614\n",
      "Epoch: 025/025 | Batch 050/1000 | Loss: 0.004594\n",
      "Epoch: 025/025 | Batch 100/1000 | Loss: 0.116938\n",
      "Epoch: 025/025 | Batch 150/1000 | Loss: 0.050116\n",
      "Epoch: 025/025 | Batch 200/1000 | Loss: 0.108576\n",
      "Epoch: 025/025 | Batch 250/1000 | Loss: 0.034561\n",
      "Epoch: 025/025 | Batch 300/1000 | Loss: 0.034746\n",
      "Epoch: 025/025 | Batch 350/1000 | Loss: 0.333417\n",
      "Epoch: 025/025 | Batch 400/1000 | Loss: 0.039246\n",
      "Epoch: 025/025 | Batch 450/1000 | Loss: 0.040840\n",
      "Epoch: 025/025 | Batch 500/1000 | Loss: 0.016021\n",
      "Epoch: 025/025 | Batch 550/1000 | Loss: 0.018000\n",
      "Epoch: 025/025 | Batch 600/1000 | Loss: 0.017742\n",
      "Epoch: 025/025 | Batch 650/1000 | Loss: 0.021280\n",
      "Epoch: 025/025 | Batch 700/1000 | Loss: 0.036571\n",
      "Epoch: 025/025 | Batch 750/1000 | Loss: 0.039548\n",
      "Epoch: 025/025 | Batch 800/1000 | Loss: 0.052096\n",
      "Epoch: 025/025 | Batch 850/1000 | Loss: 0.029196\n",
      "Epoch: 025/025 | Batch 900/1000 | Loss: 0.251391\n",
      "Epoch: 025/025 | Batch 950/1000 | Loss: 0.092424\n",
      "Time elapsed: 0.09 min\n",
      "Total Training Time: 0.09 min\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "my_model.trainer(train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error : 0.6% \n",
      "Test error : 18.7%\n",
      "The total number of the parameters is: 102082\n"
     ]
    }
   ],
   "source": [
    "# output the train error and test error\n",
    "print(\"Train error : %.1f%% \\nTest error : %.1f%%\" %\n",
    "      (my_model.compute_error(train_input, train_target),\n",
    "       my_model.compute_error(test_input, test_target)))\n",
    "\n",
    "print(\"The total number of the parameters is: %d\" % (sum(p.numel() for p in my_model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
