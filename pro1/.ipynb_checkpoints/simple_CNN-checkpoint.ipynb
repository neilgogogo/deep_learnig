{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes directly download will cause http403 error\n",
    "from six.moves import urllib\n",
    "# have to add a header to your urllib request (due to that site moving to Cloudflare protection)\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and testing data\n",
    "N_PAIRS = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N_PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 2, 14, 14])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(train_classes.shape)\n",
    "print(train_target.shape)\n",
    "print(test_input.shape)\n",
    "print(test_classes.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test classes are: 8 and 5.\n",
      "The test target is: 0\n"
     ]
    }
   ],
   "source": [
    "# discribe the data\n",
    "fig = plt.figure()\n",
    "for i in range(2):\n",
    "  plt.subplot(1,2,i+1)\n",
    "  plt.imshow(test_input[0][i], cmap='gray')\n",
    "  plt.title(\"test input pair\")  \n",
    "  plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"The test classes are: %d and %d.\"%(test_classes[0][0].item(),test_classes[0][1].item()))\n",
    "print(\"The test target is: %d\"%(train_target[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple CNN\n",
    "class CNN_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32,64,kernel_size=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv2_drop=nn.Dropout2d()\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "        #parameters\n",
    "        self.batch_size = 50\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.num_epochs = 25\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.bn2(self.conv2(x)), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = F.dropout(x,training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "        # Training Function\n",
    "\n",
    "    def trainer(self, train_input, train_target, test_input, test_target):\n",
    "        \"\"\"\n",
    "        Train the model on a training set, and plot the loss and accuracy function\n",
    "        Print the used time.\n",
    "        :param train_input: Training input data\n",
    "        :param train_target: Training labels\n",
    "        :param test_input: Testing input data\n",
    "        :param test_target: Testing labels\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        train_loss_history=[]\n",
    "        test_loss_history=[]\n",
    "        train_accuracy=[]\n",
    "        test_accuracy=[]\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.train()\n",
    "            for batch_idx in range(0,train_input.size(0),self.batch_size):\n",
    "                output = self(train_input[batch_idx:batch_idx+self.batch_size]) \n",
    "                loss = self.criterion(output, train_target[batch_idx:batch_idx+self.batch_size])  \n",
    "                self.optimizer.zero_grad()                          #set the weight and bias gradients to zero\n",
    "                loss.backward()                                #backpropagation\n",
    "                self.optimizer.step()\n",
    "#                 # print the loss in every 50 epoch\n",
    "                if not batch_idx % 50:\n",
    "                    print ('Epoch: %03d/%03d | Batch %03d/%03d | Loss: %.6f' \n",
    "                           %(epoch+1, self.num_epochs, batch_idx, \n",
    "                             len(train_input), loss))\n",
    "            print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "            # test mode\n",
    "            self.eval()\n",
    "            # get the training loss and accuracy\n",
    "            train_predicted = self(train_input)\n",
    "            train_loss = self.criterion(train_predicted, train_target)\n",
    "            train_loss_history.append(train_loss)\n",
    "            _, train_pred = torch.max(train_predicted, 1)   #return the index of the bigger result\n",
    "            train_accuracy_result=self.compute_accuracy(train_target,train_pred)\n",
    "            train_accuracy.append(train_accuracy_result)\n",
    "\n",
    "            # get the testing loss and accuracy\n",
    "            test_predicted = self(test_input)\n",
    "            test_loss = self.criterion(test_predicted, test_target)\n",
    "            test_loss_history.append(test_loss)\n",
    "            _, test_pred = torch.max(test_predicted, 1)   #return the index of the bigger result\n",
    "            test_accuracy_result=self.compute_accuracy(test_target,test_pred)\n",
    "            test_accuracy.append(test_accuracy_result)\n",
    "            \n",
    "        print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "        \n",
    "        #Setting-up the plot\n",
    "        plt.figure(figsize=(15,8))\n",
    "\n",
    "        ax1 = plt.subplot(1,2,1)\n",
    "\n",
    "        ax2 = plt.subplot(1,2,2)\n",
    "\n",
    "        #Drawing and labeling the curves\n",
    "        ax1.plot(train_loss_history, label=\"Training Loss\")\n",
    "        ax1.plot(test_loss_history, label=\"Test Loss\")\n",
    "\n",
    "        #Adding the title and axis labels\n",
    "        ax1.set_title('Train VS Test Loss')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.legend()\n",
    "\n",
    "    #     #Saving the plot\n",
    "    #     ax1.figure.savefig(model.model_name+'loss.png')\n",
    "\n",
    "\n",
    "        #Drawing and labeling the curves\n",
    "        ax2.plot(train_accuracy, label=\"Train Accuracy\")\n",
    "        ax2.plot(test_accuracy, label=\"Test Accuracy\")\n",
    "\n",
    "        #Adding the title and axis labels\n",
    "        ax2.set_title('Train VS Test Accuracy')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.legend()\n",
    "\n",
    "        # Test error\n",
    "\n",
    "    def compute_error(self, input_data, target):\n",
    "        \"\"\"\n",
    "        Compute the number of error of the model on a test set\n",
    "        :param input_data: test features\n",
    "        :param target: test target\n",
    "        :return: error rate of the input data\n",
    "        \"\"\"  \n",
    "    \n",
    "        #test mode\n",
    "        self.eval()\n",
    "        outputs= self(input_data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return 1-self.compute_accuracy(target,predicted)\n",
    "    \n",
    "    def compute_accuracy(self, target, pred):\n",
    "        \"\"\"\n",
    "        Compute the training and testing error\n",
    "        :param target: target data (whether 1 or 0)\n",
    "        :param pred: predicted data\n",
    "        :return \n",
    "        \"\"\"\n",
    "        return (target-pred).eq(0).float().mean().item()\n",
    "    \n",
    "    \n",
    "    def save_model(self,model_name):\n",
    "        \"\"\"\n",
    "        Save the model to a direction\n",
    "        :param model_name: the model name, e.g. CNN_Net.pth\n",
    "        \"\"\"         \n",
    "        torch.save(self, './model/'+ model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the net instance\n",
    "my_model=CNN_Net()\n",
    "# save the model\n",
    "my_model.save_model('CNN_Net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/025 | Batch 000/1000 | Loss: 0.729384\n",
      "Epoch: 001/025 | Batch 050/1000 | Loss: 0.819978\n",
      "Epoch: 001/025 | Batch 100/1000 | Loss: 0.744958\n",
      "Epoch: 001/025 | Batch 150/1000 | Loss: 0.699671\n",
      "Epoch: 001/025 | Batch 200/1000 | Loss: 0.780112\n",
      "Epoch: 001/025 | Batch 250/1000 | Loss: 0.890692\n",
      "Epoch: 001/025 | Batch 300/1000 | Loss: 0.634096\n",
      "Epoch: 001/025 | Batch 350/1000 | Loss: 0.653294\n",
      "Epoch: 001/025 | Batch 400/1000 | Loss: 0.636038\n",
      "Epoch: 001/025 | Batch 450/1000 | Loss: 0.648608\n",
      "Epoch: 001/025 | Batch 500/1000 | Loss: 0.689758\n",
      "Epoch: 001/025 | Batch 550/1000 | Loss: 0.678148\n",
      "Epoch: 001/025 | Batch 600/1000 | Loss: 0.717753\n",
      "Epoch: 001/025 | Batch 650/1000 | Loss: 0.675808\n",
      "Epoch: 001/025 | Batch 700/1000 | Loss: 0.628812\n",
      "Epoch: 001/025 | Batch 750/1000 | Loss: 0.632603\n",
      "Epoch: 001/025 | Batch 800/1000 | Loss: 0.574039\n",
      "Epoch: 001/025 | Batch 850/1000 | Loss: 0.665985\n",
      "Epoch: 001/025 | Batch 900/1000 | Loss: 0.610171\n",
      "Epoch: 001/025 | Batch 950/1000 | Loss: 0.607084\n",
      "Time elapsed: 0.00 min\n",
      "Epoch: 002/025 | Batch 000/1000 | Loss: 0.597033\n",
      "Epoch: 002/025 | Batch 050/1000 | Loss: 0.549515\n",
      "Epoch: 002/025 | Batch 100/1000 | Loss: 0.594714\n",
      "Epoch: 002/025 | Batch 150/1000 | Loss: 0.552192\n",
      "Epoch: 002/025 | Batch 200/1000 | Loss: 0.527080\n",
      "Epoch: 002/025 | Batch 250/1000 | Loss: 0.552582\n",
      "Epoch: 002/025 | Batch 300/1000 | Loss: 0.490517\n",
      "Epoch: 002/025 | Batch 350/1000 | Loss: 0.549741\n",
      "Epoch: 002/025 | Batch 400/1000 | Loss: 0.520964\n",
      "Epoch: 002/025 | Batch 450/1000 | Loss: 0.579909\n",
      "Epoch: 002/025 | Batch 500/1000 | Loss: 0.479176\n",
      "Epoch: 002/025 | Batch 550/1000 | Loss: 0.485235\n",
      "Epoch: 002/025 | Batch 600/1000 | Loss: 0.543312\n",
      "Epoch: 002/025 | Batch 650/1000 | Loss: 0.573177\n",
      "Epoch: 002/025 | Batch 700/1000 | Loss: 0.499850\n",
      "Epoch: 002/025 | Batch 750/1000 | Loss: 0.591309\n",
      "Epoch: 002/025 | Batch 800/1000 | Loss: 0.530808\n",
      "Epoch: 002/025 | Batch 850/1000 | Loss: 0.500123\n",
      "Epoch: 002/025 | Batch 900/1000 | Loss: 0.577261\n",
      "Epoch: 002/025 | Batch 950/1000 | Loss: 0.412933\n",
      "Time elapsed: 0.01 min\n",
      "Epoch: 003/025 | Batch 000/1000 | Loss: 0.460241\n",
      "Epoch: 003/025 | Batch 050/1000 | Loss: 0.448242\n",
      "Epoch: 003/025 | Batch 100/1000 | Loss: 0.391238\n",
      "Epoch: 003/025 | Batch 150/1000 | Loss: 0.380271\n",
      "Epoch: 003/025 | Batch 200/1000 | Loss: 0.483522\n",
      "Epoch: 003/025 | Batch 250/1000 | Loss: 0.483549\n",
      "Epoch: 003/025 | Batch 300/1000 | Loss: 0.384377\n",
      "Epoch: 003/025 | Batch 350/1000 | Loss: 0.524136\n",
      "Epoch: 003/025 | Batch 400/1000 | Loss: 0.461351\n",
      "Epoch: 003/025 | Batch 450/1000 | Loss: 0.420015\n",
      "Epoch: 003/025 | Batch 500/1000 | Loss: 0.380188\n",
      "Epoch: 003/025 | Batch 550/1000 | Loss: 0.464805\n",
      "Epoch: 003/025 | Batch 600/1000 | Loss: 0.533069\n",
      "Epoch: 003/025 | Batch 650/1000 | Loss: 0.450039\n",
      "Epoch: 003/025 | Batch 700/1000 | Loss: 0.363229\n",
      "Epoch: 003/025 | Batch 750/1000 | Loss: 0.491071\n",
      "Epoch: 003/025 | Batch 800/1000 | Loss: 0.502186\n",
      "Epoch: 003/025 | Batch 850/1000 | Loss: 0.467118\n",
      "Epoch: 003/025 | Batch 900/1000 | Loss: 0.530683\n",
      "Epoch: 003/025 | Batch 950/1000 | Loss: 0.332202\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 004/025 | Batch 000/1000 | Loss: 0.359170\n",
      "Epoch: 004/025 | Batch 050/1000 | Loss: 0.335173\n",
      "Epoch: 004/025 | Batch 100/1000 | Loss: 0.351534\n",
      "Epoch: 004/025 | Batch 150/1000 | Loss: 0.274442\n",
      "Epoch: 004/025 | Batch 200/1000 | Loss: 0.327749\n",
      "Epoch: 004/025 | Batch 250/1000 | Loss: 0.389971\n",
      "Epoch: 004/025 | Batch 300/1000 | Loss: 0.321920\n",
      "Epoch: 004/025 | Batch 350/1000 | Loss: 0.537682\n",
      "Epoch: 004/025 | Batch 400/1000 | Loss: 0.345969\n",
      "Epoch: 004/025 | Batch 450/1000 | Loss: 0.356376\n",
      "Epoch: 004/025 | Batch 500/1000 | Loss: 0.317705\n",
      "Epoch: 004/025 | Batch 550/1000 | Loss: 0.371618\n",
      "Epoch: 004/025 | Batch 600/1000 | Loss: 0.411309\n",
      "Epoch: 004/025 | Batch 650/1000 | Loss: 0.354850\n",
      "Epoch: 004/025 | Batch 700/1000 | Loss: 0.260802\n",
      "Epoch: 004/025 | Batch 750/1000 | Loss: 0.329367\n",
      "Epoch: 004/025 | Batch 800/1000 | Loss: 0.343330\n",
      "Epoch: 004/025 | Batch 850/1000 | Loss: 0.336276\n",
      "Epoch: 004/025 | Batch 900/1000 | Loss: 0.469877\n",
      "Epoch: 004/025 | Batch 950/1000 | Loss: 0.200478\n",
      "Time elapsed: 0.02 min\n",
      "Epoch: 005/025 | Batch 000/1000 | Loss: 0.265979\n",
      "Epoch: 005/025 | Batch 050/1000 | Loss: 0.197586\n",
      "Epoch: 005/025 | Batch 100/1000 | Loss: 0.224112\n",
      "Epoch: 005/025 | Batch 150/1000 | Loss: 0.233573\n",
      "Epoch: 005/025 | Batch 200/1000 | Loss: 0.255103\n",
      "Epoch: 005/025 | Batch 250/1000 | Loss: 0.305481\n",
      "Epoch: 005/025 | Batch 300/1000 | Loss: 0.254936\n",
      "Epoch: 005/025 | Batch 350/1000 | Loss: 0.330042\n",
      "Epoch: 005/025 | Batch 400/1000 | Loss: 0.305769\n",
      "Epoch: 005/025 | Batch 450/1000 | Loss: 0.244677\n",
      "Epoch: 005/025 | Batch 500/1000 | Loss: 0.181222\n",
      "Epoch: 005/025 | Batch 550/1000 | Loss: 0.260564\n",
      "Epoch: 005/025 | Batch 600/1000 | Loss: 0.256576\n",
      "Epoch: 005/025 | Batch 650/1000 | Loss: 0.273471\n",
      "Epoch: 005/025 | Batch 700/1000 | Loss: 0.174678\n",
      "Epoch: 005/025 | Batch 750/1000 | Loss: 0.265590\n",
      "Epoch: 005/025 | Batch 800/1000 | Loss: 0.268999\n",
      "Epoch: 005/025 | Batch 850/1000 | Loss: 0.249010\n",
      "Epoch: 005/025 | Batch 900/1000 | Loss: 0.400598\n",
      "Epoch: 005/025 | Batch 950/1000 | Loss: 0.152473\n",
      "Time elapsed: 0.03 min\n",
      "Epoch: 006/025 | Batch 000/1000 | Loss: 0.160371\n",
      "Epoch: 006/025 | Batch 050/1000 | Loss: 0.172622\n",
      "Epoch: 006/025 | Batch 100/1000 | Loss: 0.175693\n",
      "Epoch: 006/025 | Batch 150/1000 | Loss: 0.163372\n",
      "Epoch: 006/025 | Batch 200/1000 | Loss: 0.190623\n",
      "Epoch: 006/025 | Batch 250/1000 | Loss: 0.195209\n",
      "Epoch: 006/025 | Batch 300/1000 | Loss: 0.215103\n",
      "Epoch: 006/025 | Batch 350/1000 | Loss: 0.277396\n",
      "Epoch: 006/025 | Batch 400/1000 | Loss: 0.246885\n",
      "Epoch: 006/025 | Batch 450/1000 | Loss: 0.199980\n",
      "Epoch: 006/025 | Batch 500/1000 | Loss: 0.146850\n",
      "Epoch: 006/025 | Batch 550/1000 | Loss: 0.239578\n",
      "Epoch: 006/025 | Batch 600/1000 | Loss: 0.156816\n",
      "Epoch: 006/025 | Batch 650/1000 | Loss: 0.211436\n",
      "Epoch: 006/025 | Batch 700/1000 | Loss: 0.133583\n",
      "Epoch: 006/025 | Batch 750/1000 | Loss: 0.147876\n",
      "Epoch: 006/025 | Batch 800/1000 | Loss: 0.228405\n",
      "Epoch: 006/025 | Batch 850/1000 | Loss: 0.191815\n",
      "Epoch: 006/025 | Batch 900/1000 | Loss: 0.296559\n",
      "Epoch: 006/025 | Batch 950/1000 | Loss: 0.098400\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 007/025 | Batch 000/1000 | Loss: 0.138794\n",
      "Epoch: 007/025 | Batch 050/1000 | Loss: 0.109934\n",
      "Epoch: 007/025 | Batch 100/1000 | Loss: 0.098663\n",
      "Epoch: 007/025 | Batch 150/1000 | Loss: 0.096234\n",
      "Epoch: 007/025 | Batch 200/1000 | Loss: 0.146587\n",
      "Epoch: 007/025 | Batch 250/1000 | Loss: 0.142849\n",
      "Epoch: 007/025 | Batch 300/1000 | Loss: 0.184000\n",
      "Epoch: 007/025 | Batch 350/1000 | Loss: 0.133157\n",
      "Epoch: 007/025 | Batch 400/1000 | Loss: 0.173542\n",
      "Epoch: 007/025 | Batch 450/1000 | Loss: 0.175338\n",
      "Epoch: 007/025 | Batch 500/1000 | Loss: 0.080782\n",
      "Epoch: 007/025 | Batch 550/1000 | Loss: 0.138554\n",
      "Epoch: 007/025 | Batch 600/1000 | Loss: 0.072227\n",
      "Epoch: 007/025 | Batch 650/1000 | Loss: 0.157244\n",
      "Epoch: 007/025 | Batch 700/1000 | Loss: 0.089607\n",
      "Epoch: 007/025 | Batch 750/1000 | Loss: 0.112876\n",
      "Epoch: 007/025 | Batch 800/1000 | Loss: 0.114882\n",
      "Epoch: 007/025 | Batch 850/1000 | Loss: 0.143222\n",
      "Epoch: 007/025 | Batch 900/1000 | Loss: 0.182040\n",
      "Epoch: 007/025 | Batch 950/1000 | Loss: 0.062619\n",
      "Time elapsed: 0.04 min\n",
      "Epoch: 008/025 | Batch 000/1000 | Loss: 0.101796\n",
      "Epoch: 008/025 | Batch 050/1000 | Loss: 0.048279\n",
      "Epoch: 008/025 | Batch 100/1000 | Loss: 0.104552\n",
      "Epoch: 008/025 | Batch 150/1000 | Loss: 0.067290\n",
      "Epoch: 008/025 | Batch 200/1000 | Loss: 0.074896\n",
      "Epoch: 008/025 | Batch 250/1000 | Loss: 0.162337\n",
      "Epoch: 008/025 | Batch 300/1000 | Loss: 0.170033\n",
      "Epoch: 008/025 | Batch 350/1000 | Loss: 0.129457\n",
      "Epoch: 008/025 | Batch 400/1000 | Loss: 0.117770\n",
      "Epoch: 008/025 | Batch 450/1000 | Loss: 0.098518\n",
      "Epoch: 008/025 | Batch 500/1000 | Loss: 0.052057\n",
      "Epoch: 008/025 | Batch 550/1000 | Loss: 0.135879\n",
      "Epoch: 008/025 | Batch 600/1000 | Loss: 0.055256\n",
      "Epoch: 008/025 | Batch 650/1000 | Loss: 0.116250\n",
      "Epoch: 008/025 | Batch 700/1000 | Loss: 0.048341\n",
      "Epoch: 008/025 | Batch 750/1000 | Loss: 0.042505\n",
      "Epoch: 008/025 | Batch 800/1000 | Loss: 0.100095\n",
      "Epoch: 008/025 | Batch 850/1000 | Loss: 0.099305\n",
      "Epoch: 008/025 | Batch 900/1000 | Loss: 0.110706\n",
      "Epoch: 008/025 | Batch 950/1000 | Loss: 0.058050\n",
      "Time elapsed: 0.05 min\n",
      "Epoch: 009/025 | Batch 000/1000 | Loss: 0.061829\n",
      "Epoch: 009/025 | Batch 050/1000 | Loss: 0.043295\n",
      "Epoch: 009/025 | Batch 100/1000 | Loss: 0.048207\n",
      "Epoch: 009/025 | Batch 150/1000 | Loss: 0.041585\n",
      "Epoch: 009/025 | Batch 200/1000 | Loss: 0.060129\n",
      "Epoch: 009/025 | Batch 250/1000 | Loss: 0.056383\n",
      "Epoch: 009/025 | Batch 300/1000 | Loss: 0.061109\n",
      "Epoch: 009/025 | Batch 350/1000 | Loss: 0.064298\n",
      "Epoch: 009/025 | Batch 400/1000 | Loss: 0.031723\n",
      "Epoch: 009/025 | Batch 450/1000 | Loss: 0.068825\n",
      "Epoch: 009/025 | Batch 500/1000 | Loss: 0.056255\n",
      "Epoch: 009/025 | Batch 550/1000 | Loss: 0.065385\n",
      "Epoch: 009/025 | Batch 600/1000 | Loss: 0.031456\n",
      "Epoch: 009/025 | Batch 650/1000 | Loss: 0.085873\n",
      "Epoch: 009/025 | Batch 700/1000 | Loss: 0.050825\n",
      "Epoch: 009/025 | Batch 750/1000 | Loss: 0.046007\n",
      "Epoch: 009/025 | Batch 800/1000 | Loss: 0.040006\n",
      "Epoch: 009/025 | Batch 850/1000 | Loss: 0.035642\n",
      "Epoch: 009/025 | Batch 900/1000 | Loss: 0.074765\n",
      "Epoch: 009/025 | Batch 950/1000 | Loss: 0.044641\n",
      "Time elapsed: 0.05 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010/025 | Batch 000/1000 | Loss: 0.044523\n",
      "Epoch: 010/025 | Batch 050/1000 | Loss: 0.029925\n",
      "Epoch: 010/025 | Batch 100/1000 | Loss: 0.031233\n",
      "Epoch: 010/025 | Batch 150/1000 | Loss: 0.026658\n",
      "Epoch: 010/025 | Batch 200/1000 | Loss: 0.033286\n",
      "Epoch: 010/025 | Batch 250/1000 | Loss: 0.041733\n",
      "Epoch: 010/025 | Batch 300/1000 | Loss: 0.051752\n",
      "Epoch: 010/025 | Batch 350/1000 | Loss: 0.043528\n",
      "Epoch: 010/025 | Batch 400/1000 | Loss: 0.022649\n",
      "Epoch: 010/025 | Batch 450/1000 | Loss: 0.040292\n",
      "Epoch: 010/025 | Batch 500/1000 | Loss: 0.025088\n",
      "Epoch: 010/025 | Batch 550/1000 | Loss: 0.025357\n",
      "Epoch: 010/025 | Batch 600/1000 | Loss: 0.018161\n",
      "Epoch: 010/025 | Batch 650/1000 | Loss: 0.036551\n",
      "Epoch: 010/025 | Batch 700/1000 | Loss: 0.016112\n",
      "Epoch: 010/025 | Batch 750/1000 | Loss: 0.025263\n",
      "Epoch: 010/025 | Batch 800/1000 | Loss: 0.020870\n",
      "Epoch: 010/025 | Batch 850/1000 | Loss: 0.027776\n",
      "Epoch: 010/025 | Batch 900/1000 | Loss: 0.038283\n",
      "Epoch: 010/025 | Batch 950/1000 | Loss: 0.018820\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 011/025 | Batch 000/1000 | Loss: 0.023051\n",
      "Epoch: 011/025 | Batch 050/1000 | Loss: 0.021086\n",
      "Epoch: 011/025 | Batch 100/1000 | Loss: 0.024549\n",
      "Epoch: 011/025 | Batch 150/1000 | Loss: 0.013837\n",
      "Epoch: 011/025 | Batch 200/1000 | Loss: 0.016440\n",
      "Epoch: 011/025 | Batch 250/1000 | Loss: 0.067961\n",
      "Epoch: 011/025 | Batch 300/1000 | Loss: 0.036164\n",
      "Epoch: 011/025 | Batch 350/1000 | Loss: 0.028124\n",
      "Epoch: 011/025 | Batch 400/1000 | Loss: 0.032068\n",
      "Epoch: 011/025 | Batch 450/1000 | Loss: 0.032388\n",
      "Epoch: 011/025 | Batch 500/1000 | Loss: 0.019836\n",
      "Epoch: 011/025 | Batch 550/1000 | Loss: 0.032621\n",
      "Epoch: 011/025 | Batch 600/1000 | Loss: 0.019796\n",
      "Epoch: 011/025 | Batch 650/1000 | Loss: 0.013202\n",
      "Epoch: 011/025 | Batch 700/1000 | Loss: 0.025636\n",
      "Epoch: 011/025 | Batch 750/1000 | Loss: 0.028915\n",
      "Epoch: 011/025 | Batch 800/1000 | Loss: 0.028812\n",
      "Epoch: 011/025 | Batch 850/1000 | Loss: 0.033577\n",
      "Epoch: 011/025 | Batch 900/1000 | Loss: 0.025156\n",
      "Epoch: 011/025 | Batch 950/1000 | Loss: 0.011265\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 012/025 | Batch 000/1000 | Loss: 0.012684\n",
      "Epoch: 012/025 | Batch 050/1000 | Loss: 0.015848\n",
      "Epoch: 012/025 | Batch 100/1000 | Loss: 0.009963\n",
      "Epoch: 012/025 | Batch 150/1000 | Loss: 0.010708\n",
      "Epoch: 012/025 | Batch 200/1000 | Loss: 0.021600\n",
      "Epoch: 012/025 | Batch 250/1000 | Loss: 0.005422\n",
      "Epoch: 012/025 | Batch 300/1000 | Loss: 0.020664\n",
      "Epoch: 012/025 | Batch 350/1000 | Loss: 0.040971\n",
      "Epoch: 012/025 | Batch 400/1000 | Loss: 0.035046\n",
      "Epoch: 012/025 | Batch 450/1000 | Loss: 0.022835\n",
      "Epoch: 012/025 | Batch 500/1000 | Loss: 0.027347\n",
      "Epoch: 012/025 | Batch 550/1000 | Loss: 0.034889\n",
      "Epoch: 012/025 | Batch 600/1000 | Loss: 0.025830\n",
      "Epoch: 012/025 | Batch 650/1000 | Loss: 0.015625\n",
      "Epoch: 012/025 | Batch 700/1000 | Loss: 0.007914\n",
      "Epoch: 012/025 | Batch 750/1000 | Loss: 0.010843\n",
      "Epoch: 012/025 | Batch 800/1000 | Loss: 0.006492\n",
      "Epoch: 012/025 | Batch 850/1000 | Loss: 0.031654\n",
      "Epoch: 012/025 | Batch 900/1000 | Loss: 0.049423\n",
      "Epoch: 012/025 | Batch 950/1000 | Loss: 0.014387\n",
      "Time elapsed: 0.07 min\n",
      "Epoch: 013/025 | Batch 000/1000 | Loss: 0.009487\n",
      "Epoch: 013/025 | Batch 050/1000 | Loss: 0.003721\n",
      "Epoch: 013/025 | Batch 100/1000 | Loss: 0.008282\n",
      "Epoch: 013/025 | Batch 150/1000 | Loss: 0.019637\n",
      "Epoch: 013/025 | Batch 200/1000 | Loss: 0.016353\n",
      "Epoch: 013/025 | Batch 250/1000 | Loss: 0.020368\n",
      "Epoch: 013/025 | Batch 300/1000 | Loss: 0.038942\n",
      "Epoch: 013/025 | Batch 350/1000 | Loss: 0.018707\n",
      "Epoch: 013/025 | Batch 400/1000 | Loss: 0.008677\n",
      "Epoch: 013/025 | Batch 450/1000 | Loss: 0.033765\n",
      "Epoch: 013/025 | Batch 500/1000 | Loss: 0.018503\n",
      "Epoch: 013/025 | Batch 550/1000 | Loss: 0.075071\n",
      "Epoch: 013/025 | Batch 600/1000 | Loss: 0.005934\n",
      "Epoch: 013/025 | Batch 650/1000 | Loss: 0.009649\n",
      "Epoch: 013/025 | Batch 700/1000 | Loss: 0.018453\n",
      "Epoch: 013/025 | Batch 750/1000 | Loss: 0.065677\n",
      "Epoch: 013/025 | Batch 800/1000 | Loss: 0.053257\n",
      "Epoch: 013/025 | Batch 850/1000 | Loss: 0.012823\n",
      "Epoch: 013/025 | Batch 900/1000 | Loss: 0.015937\n",
      "Epoch: 013/025 | Batch 950/1000 | Loss: 0.018233\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 014/025 | Batch 000/1000 | Loss: 0.084773\n",
      "Epoch: 014/025 | Batch 050/1000 | Loss: 0.010249\n",
      "Epoch: 014/025 | Batch 100/1000 | Loss: 0.021532\n",
      "Epoch: 014/025 | Batch 150/1000 | Loss: 0.011659\n",
      "Epoch: 014/025 | Batch 200/1000 | Loss: 0.012688\n",
      "Epoch: 014/025 | Batch 250/1000 | Loss: 0.008669\n",
      "Epoch: 014/025 | Batch 300/1000 | Loss: 0.045468\n",
      "Epoch: 014/025 | Batch 350/1000 | Loss: 0.056940\n",
      "Epoch: 014/025 | Batch 400/1000 | Loss: 0.042482\n",
      "Epoch: 014/025 | Batch 450/1000 | Loss: 0.032944\n",
      "Epoch: 014/025 | Batch 500/1000 | Loss: 0.002449\n",
      "Epoch: 014/025 | Batch 550/1000 | Loss: 0.048285\n",
      "Epoch: 014/025 | Batch 600/1000 | Loss: 0.036979\n",
      "Epoch: 014/025 | Batch 650/1000 | Loss: 0.035549\n",
      "Epoch: 014/025 | Batch 700/1000 | Loss: 0.009580\n",
      "Epoch: 014/025 | Batch 750/1000 | Loss: 0.040542\n",
      "Epoch: 014/025 | Batch 800/1000 | Loss: 0.017937\n",
      "Epoch: 014/025 | Batch 850/1000 | Loss: 0.027266\n",
      "Epoch: 014/025 | Batch 900/1000 | Loss: 0.033147\n",
      "Epoch: 014/025 | Batch 950/1000 | Loss: 0.021075\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 015/025 | Batch 000/1000 | Loss: 0.007313\n",
      "Epoch: 015/025 | Batch 050/1000 | Loss: 0.012958\n",
      "Epoch: 015/025 | Batch 100/1000 | Loss: 0.015991\n",
      "Epoch: 015/025 | Batch 150/1000 | Loss: 0.015473\n",
      "Epoch: 015/025 | Batch 200/1000 | Loss: 0.056303\n",
      "Epoch: 015/025 | Batch 250/1000 | Loss: 0.019241\n",
      "Epoch: 015/025 | Batch 300/1000 | Loss: 0.018794\n",
      "Epoch: 015/025 | Batch 350/1000 | Loss: 0.012492\n",
      "Epoch: 015/025 | Batch 400/1000 | Loss: 0.028482\n",
      "Epoch: 015/025 | Batch 450/1000 | Loss: 0.058321\n",
      "Epoch: 015/025 | Batch 500/1000 | Loss: 0.033024\n",
      "Epoch: 015/025 | Batch 550/1000 | Loss: 0.010182\n",
      "Epoch: 015/025 | Batch 600/1000 | Loss: 0.011723\n",
      "Epoch: 015/025 | Batch 650/1000 | Loss: 0.015997\n",
      "Epoch: 015/025 | Batch 700/1000 | Loss: 0.024976\n",
      "Epoch: 015/025 | Batch 750/1000 | Loss: 0.005883\n",
      "Epoch: 015/025 | Batch 800/1000 | Loss: 0.081410\n",
      "Epoch: 015/025 | Batch 850/1000 | Loss: 0.030554\n",
      "Epoch: 015/025 | Batch 900/1000 | Loss: 0.015353\n",
      "Epoch: 015/025 | Batch 950/1000 | Loss: 0.021094\n",
      "Time elapsed: 0.09 min\n",
      "Epoch: 016/025 | Batch 000/1000 | Loss: 0.037286\n",
      "Epoch: 016/025 | Batch 050/1000 | Loss: 0.012720\n",
      "Epoch: 016/025 | Batch 100/1000 | Loss: 0.020876\n",
      "Epoch: 016/025 | Batch 150/1000 | Loss: 0.010557\n",
      "Epoch: 016/025 | Batch 200/1000 | Loss: 0.047518\n",
      "Epoch: 016/025 | Batch 250/1000 | Loss: 0.085383\n",
      "Epoch: 016/025 | Batch 300/1000 | Loss: 0.047363\n",
      "Epoch: 016/025 | Batch 350/1000 | Loss: 0.022747\n",
      "Epoch: 016/025 | Batch 400/1000 | Loss: 0.005400\n",
      "Epoch: 016/025 | Batch 450/1000 | Loss: 0.003361\n",
      "Epoch: 016/025 | Batch 500/1000 | Loss: 0.020197\n",
      "Epoch: 016/025 | Batch 550/1000 | Loss: 0.024055\n",
      "Epoch: 016/025 | Batch 600/1000 | Loss: 0.075143\n",
      "Epoch: 016/025 | Batch 650/1000 | Loss: 0.014673\n",
      "Epoch: 016/025 | Batch 700/1000 | Loss: 0.014635\n",
      "Epoch: 016/025 | Batch 750/1000 | Loss: 0.050754\n",
      "Epoch: 016/025 | Batch 800/1000 | Loss: 0.024039\n",
      "Epoch: 016/025 | Batch 850/1000 | Loss: 0.075865\n",
      "Epoch: 016/025 | Batch 900/1000 | Loss: 0.012399\n",
      "Epoch: 016/025 | Batch 950/1000 | Loss: 0.011515\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 017/025 | Batch 000/1000 | Loss: 0.034737\n",
      "Epoch: 017/025 | Batch 050/1000 | Loss: 0.011945\n",
      "Epoch: 017/025 | Batch 100/1000 | Loss: 0.022080\n",
      "Epoch: 017/025 | Batch 150/1000 | Loss: 0.005303\n",
      "Epoch: 017/025 | Batch 200/1000 | Loss: 0.013605\n",
      "Epoch: 017/025 | Batch 250/1000 | Loss: 0.004302\n",
      "Epoch: 017/025 | Batch 300/1000 | Loss: 0.010710\n",
      "Epoch: 017/025 | Batch 350/1000 | Loss: 0.019582\n",
      "Epoch: 017/025 | Batch 400/1000 | Loss: 0.009458\n",
      "Epoch: 017/025 | Batch 450/1000 | Loss: 0.011053\n",
      "Epoch: 017/025 | Batch 500/1000 | Loss: 0.007430\n",
      "Epoch: 017/025 | Batch 550/1000 | Loss: 0.014253\n",
      "Epoch: 017/025 | Batch 600/1000 | Loss: 0.006841\n",
      "Epoch: 017/025 | Batch 650/1000 | Loss: 0.015709\n",
      "Epoch: 017/025 | Batch 700/1000 | Loss: 0.004575\n",
      "Epoch: 017/025 | Batch 750/1000 | Loss: 0.008832\n",
      "Epoch: 017/025 | Batch 800/1000 | Loss: 0.012827\n",
      "Epoch: 017/025 | Batch 850/1000 | Loss: 0.003671\n",
      "Epoch: 017/025 | Batch 900/1000 | Loss: 0.021445\n",
      "Epoch: 017/025 | Batch 950/1000 | Loss: 0.011474\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 018/025 | Batch 000/1000 | Loss: 0.011276\n",
      "Epoch: 018/025 | Batch 050/1000 | Loss: 0.004730\n",
      "Epoch: 018/025 | Batch 100/1000 | Loss: 0.016558\n",
      "Epoch: 018/025 | Batch 150/1000 | Loss: 0.007618\n",
      "Epoch: 018/025 | Batch 200/1000 | Loss: 0.007807\n",
      "Epoch: 018/025 | Batch 250/1000 | Loss: 0.007573\n",
      "Epoch: 018/025 | Batch 300/1000 | Loss: 0.007543\n",
      "Epoch: 018/025 | Batch 350/1000 | Loss: 0.016912\n",
      "Epoch: 018/025 | Batch 400/1000 | Loss: 0.022339\n",
      "Epoch: 018/025 | Batch 450/1000 | Loss: 0.019755\n",
      "Epoch: 018/025 | Batch 500/1000 | Loss: 0.009579\n",
      "Epoch: 018/025 | Batch 550/1000 | Loss: 0.003613\n",
      "Epoch: 018/025 | Batch 600/1000 | Loss: 0.002054\n",
      "Epoch: 018/025 | Batch 650/1000 | Loss: 0.005512\n",
      "Epoch: 018/025 | Batch 700/1000 | Loss: 0.003647\n",
      "Epoch: 018/025 | Batch 750/1000 | Loss: 0.001921\n",
      "Epoch: 018/025 | Batch 800/1000 | Loss: 0.002017\n",
      "Epoch: 018/025 | Batch 850/1000 | Loss: 0.005790\n",
      "Epoch: 018/025 | Batch 900/1000 | Loss: 0.002755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018/025 | Batch 950/1000 | Loss: 0.006950\n",
      "Time elapsed: 0.11 min\n",
      "Epoch: 019/025 | Batch 000/1000 | Loss: 0.013091\n",
      "Epoch: 019/025 | Batch 050/1000 | Loss: 0.021689\n",
      "Epoch: 019/025 | Batch 100/1000 | Loss: 0.000770\n",
      "Epoch: 019/025 | Batch 150/1000 | Loss: 0.002336\n",
      "Epoch: 019/025 | Batch 200/1000 | Loss: 0.002352\n",
      "Epoch: 019/025 | Batch 250/1000 | Loss: 0.002208\n",
      "Epoch: 019/025 | Batch 300/1000 | Loss: 0.008074\n",
      "Epoch: 019/025 | Batch 350/1000 | Loss: 0.004436\n",
      "Epoch: 019/025 | Batch 400/1000 | Loss: 0.004747\n",
      "Epoch: 019/025 | Batch 450/1000 | Loss: 0.003296\n",
      "Epoch: 019/025 | Batch 500/1000 | Loss: 0.001454\n",
      "Epoch: 019/025 | Batch 550/1000 | Loss: 0.005118\n",
      "Epoch: 019/025 | Batch 600/1000 | Loss: 0.004425\n",
      "Epoch: 019/025 | Batch 650/1000 | Loss: 0.017814\n",
      "Epoch: 019/025 | Batch 700/1000 | Loss: 0.003665\n",
      "Epoch: 019/025 | Batch 750/1000 | Loss: 0.008902\n",
      "Epoch: 019/025 | Batch 800/1000 | Loss: 0.003770\n",
      "Epoch: 019/025 | Batch 850/1000 | Loss: 0.008888\n",
      "Epoch: 019/025 | Batch 900/1000 | Loss: 0.013245\n",
      "Epoch: 019/025 | Batch 950/1000 | Loss: 0.006466\n",
      "Time elapsed: 0.12 min\n",
      "Epoch: 020/025 | Batch 000/1000 | Loss: 0.009339\n",
      "Epoch: 020/025 | Batch 050/1000 | Loss: 0.011967\n",
      "Epoch: 020/025 | Batch 100/1000 | Loss: 0.015157\n",
      "Epoch: 020/025 | Batch 150/1000 | Loss: 0.008920\n",
      "Epoch: 020/025 | Batch 200/1000 | Loss: 0.007405\n",
      "Epoch: 020/025 | Batch 250/1000 | Loss: 0.007394\n",
      "Epoch: 020/025 | Batch 300/1000 | Loss: 0.003439\n",
      "Epoch: 020/025 | Batch 350/1000 | Loss: 0.002662\n",
      "Epoch: 020/025 | Batch 400/1000 | Loss: 0.001103\n",
      "Epoch: 020/025 | Batch 450/1000 | Loss: 0.006170\n",
      "Epoch: 020/025 | Batch 500/1000 | Loss: 0.006004\n",
      "Epoch: 020/025 | Batch 550/1000 | Loss: 0.010560\n",
      "Epoch: 020/025 | Batch 600/1000 | Loss: 0.006550\n",
      "Epoch: 020/025 | Batch 650/1000 | Loss: 0.018974\n",
      "Epoch: 020/025 | Batch 700/1000 | Loss: 0.007656\n",
      "Epoch: 020/025 | Batch 750/1000 | Loss: 0.002279\n",
      "Epoch: 020/025 | Batch 800/1000 | Loss: 0.002051\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "my_model.trainer(train_input, train_target, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output the total number of parameters\n",
    "print(\"The total number of the parameters is: %d\" % (sum(p.numel() for p in my_model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the standard deviation:\n",
    "train_errors=[]\n",
    "test_errors=[]\n",
    "for num in range(10):\n",
    "    N_PAIRS = 1000\n",
    "    train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N_PAIRS)\n",
    "    my_model = CNN_Net()\n",
    "    # train the model\n",
    "    my_model.trainer(train_input, train_target, test_input, test_target)\n",
    "    train_errors.append(my_model.compute_error(train_input, train_target))\n",
    "    test_errors.append(my_model.compute_error(test_input, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the deviation and mean value of the training and testing errors\n",
    "print('The standard deviation of train error:',np.std(train_errors))\n",
    "print('The standard deviation of test error:',np.std(test_errors))\n",
    "print('The mean of train error:',np.mean(train_errors))\n",
    "print('The mean of test error:',np.mean(test_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the standard \n",
    "# Define labels, positions, bar heights and error bar heights\n",
    "labels = ['Training', 'Testing']\n",
    "x_pos = np.arange(len(labels))\n",
    "means = [np.mean(train_errors), np.mean(test_errors)]\n",
    "stds = [np.std(train_errors), np.std(test_errors)]\n",
    "# Build the plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, means,\n",
    "       yerr=stds,\n",
    "       align='center',\n",
    "       alpha=0.5,\n",
    "       ecolor='black',\n",
    "       capsize=10)\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_title('The mean and standard deviation of training and testing data')\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
