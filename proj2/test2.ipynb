{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fetch_MNIST'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-08319a3c884f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvolve2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblock_reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_MNIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fetch_MNIST'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "from skimage.measure import block_reduce\n",
    "import fetch_MNIST\n",
    "\n",
    "\n",
    "class LeNet(object):\n",
    "    #The network is like:\n",
    "    #    conv1 -> pool1 -> conv2 -> pool2 -> fc1 -> relu -> fc2 -> relu -> softmax\n",
    "    # l0      l1       l2       l3        l4     l5      l6     l7      l8        l9\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "        # 6 convolution kernal, each has 1 * 5 * 5 size\n",
    "        self.conv1 = xavier_init(6, 1, 5, 5)\n",
    "        # the size for mean pool is 2 * 2, stride = 2\n",
    "        self.pool1 = [2, 2]\n",
    "        # 16 convolution kernal, each has 6 * 5 * 5 size\n",
    "        self.conv2 = xavier_init(16, 6, 5, 5)\n",
    "        # the size for mean pool is 2 * 2, stride = 2\n",
    "        self.pool2 = [2, 2]\n",
    "        # fully connected layer 256 -> 200\n",
    "        self.fc1 = xavier_init(256, 200, fc=True)\n",
    "        # fully connected layer 200 -> 10\n",
    "        self.fc2 = xavier_init(200, 10, fc=True)\n",
    "\n",
    "    def forward_prop(self, input_data):\n",
    "        self.l0 = np.expand_dims(input_data, axis=1) / 255   # (batch_sz, 1, 28, 28)\n",
    "        self.l1 = self.convolution(self.l0, self.conv1)      # (batch_sz, 6, 24, 24)\n",
    "        self.l2 = self.mean_pool(self.l1, self.pool1)        # (batch_sz, 6, 12, 12)\n",
    "        self.l3 = self.convolution(self.l2, self.conv2)      # (batch_sz, 16, 8, 8)\n",
    "        self.l4 = self.mean_pool(self.l3, self.pool2)        # (batch_sz, 16, 4, 4)\n",
    "        self.l5 = self.fully_connect(self.l4, self.fc1)      # (batch_sz, 200)\n",
    "        self.l6 = self.relu(self.l5)                         # (batch_sz, 200)\n",
    "        self.l7 = self.fully_connect(self.l6, self.fc2)      # (batch_sz, 10)\n",
    "        self.l8 = self.relu(self.l7)                         # (batch_sz, 10)\n",
    "        self.l9 = self.softmax(self.l8)                      # (batch_sz, 10)\n",
    "        return self.l9\n",
    "\n",
    "    def backward_prop(self, softmax_output, output_label):\n",
    "        l8_delta             = (output_label - softmax_output) / softmax_output.shape[0]\n",
    "        l7_delta             = self.relu(self.l8, l8_delta, deriv=True)                     # (batch_sz, 10)\n",
    "        l6_delta, self.fc2   = self.fully_connect(self.l6, self.fc2, l7_delta, deriv=True)  # (batch_sz, 200)\n",
    "        l5_delta             = self.relu(self.l6, l6_delta, deriv=True)                     # (batch_sz, 200)\n",
    "        l4_delta, self.fc1   = self.fully_connect(self.l4, self.fc1, l5_delta, deriv=True)  # (batch_sz, 16, 4, 4)\n",
    "        l3_delta             = self.mean_pool(self.l3, self.pool2, l4_delta, deriv=True)    # (batch_sz, 16, 8, 8)\n",
    "        l2_delta, self.conv2 = self.convolution(self.l2, self.conv2, l3_delta, deriv=True)  # (batch_sz, 6, 12, 12)\n",
    "        l1_delta             = self.mean_pool(self.l1, self.pool1, l2_delta, deriv=True)    # (batch_sz, 6, 24, 24)\n",
    "        l0_delta, self.conv1 = self.convolution(self.l0, self.conv1, l1_delta, deriv=True)  # (batch_sz, 1, 28, 28)\n",
    "\n",
    "    def convolution(self, input_map, kernal, front_delta=None, deriv=False):\n",
    "        N, C, W, H = input_map.shape\n",
    "        K_NUM, K_C, K_W, K_H = kernal.shape\n",
    "        if deriv == False:\n",
    "            feature_map = np.zeros((N, K_NUM, W-K_W+1, H-K_H+1))\n",
    "            for imgId in range(N):\n",
    "                for kId in range(K_NUM):\n",
    "                    for cId in range(C):\n",
    "                        feature_map[imgId][kId] += \\\n",
    "                          convolve2d(input_map[imgId][cId], kernal[kId,cId,:,:], mode='valid')\n",
    "            return feature_map\n",
    "        else :\n",
    "            # front->back (propagate loss)\n",
    "            back_delta = np.zeros((N, C, W, H))\n",
    "            kernal_gradient = np.zeros((K_NUM, K_C, K_W, K_H))\n",
    "            padded_front_delta = \\\n",
    "              np.pad(front_delta, [(0,0), (0,0), (K_W-1, K_H-1), (K_W-1, K_H-1)], mode='constant', constant_values=0)\n",
    "            for imgId in range(N):\n",
    "                for cId in range(C):\n",
    "                    for kId in range(K_NUM):\n",
    "                        back_delta[imgId][cId] += \\\n",
    "                          convolve2d(padded_front_delta[imgId][kId], kernal[kId,cId,::-1,::-1], mode='valid')\n",
    "                        kernal_gradient[kId][cId] += \\\n",
    "                          convolve2d(front_delta[imgId][kId], input_map[imgId,cId,::-1,::-1], mode='valid')\n",
    "            # update weights\n",
    "            kernal += self.lr * kernal_gradient\n",
    "            return back_delta, kernal\n",
    "\n",
    "    def mean_pool(self, input_map, pool, front_delta=None, deriv=False):\n",
    "        N, C, W, H = input_map.shape\n",
    "        P_W, P_H = tuple(pool)\n",
    "        if deriv == False:\n",
    "            feature_map = np.zeros((N, C, W/P_W, H/P_H))\n",
    "            feature_map = block_reduce(input_map, tuple((1, 1, P_W, P_H)), func=np.mean)\n",
    "            return feature_map\n",
    "        else :\n",
    "            # front->back (propagate loss)\n",
    "            back_delta = np.zeros((N, C, W, H))\n",
    "            back_delta = front_delta.repeat(P_W, axis = 2).repeat(P_H, axis = 3)\n",
    "            back_delta /= (P_W * P_H)\n",
    "            return back_delta\n",
    "\n",
    "    def fully_connect(self, input_data, fc, front_delta=None, deriv=False):\n",
    "        N = input_data.shape[0]\n",
    "        if deriv == False:\n",
    "            output_data = np.dot(input_data.reshape(N, -1), fc)\n",
    "            return output_data\n",
    "        else :\n",
    "            # front->back (propagate loss)\n",
    "            back_delta = np.dot(front_delta, fc.T).reshape(input_data.shape)\n",
    "            # update weights\n",
    "            fc += self.lr * np.dot(input_data.reshape(N, -1).T, front_delta)\n",
    "            return back_delta, fc\n",
    "\n",
    "    def relu(self, x, front_delta=None, deriv=False):\n",
    "        if deriv == False:\n",
    "            return x * (x > 0)\n",
    "        else :\n",
    "            # propagate loss\n",
    "            back_delta = front_delta * 1. * (x > 0)\n",
    "            return back_delta\n",
    "\n",
    "    def softmax(self, x):\n",
    "        y = list()\n",
    "        for t in x:\n",
    "            e_t = np.exp(t - np.max(t))\n",
    "            y.append(e_t / e_t.sum())\n",
    "        return np.array(y)\n",
    "\n",
    "\n",
    "def xavier_init(c1, c2, w=1, h=1, fc=False):\n",
    "    fan_1 = c2 * w * h\n",
    "    fan_2 = c1 * w * h\n",
    "    ratio = np.sqrt(6.0 / (fan_1 + fan_2))\n",
    "    params = ratio * (2*np.random.random((c1, c2, w, h)) - 1)\n",
    "    if fc == True:\n",
    "        params = params.reshape(c1, c2)\n",
    "    return params\n",
    "\n",
    "def convertToOneHot(labels):\n",
    "    oneHotLabels = np.zeros((labels.size, labels.max()+1))\n",
    "    oneHotLabels[np.arange(labels.size), labels] = 1\n",
    "    return oneHotLabels\n",
    "\n",
    "def shuffle_dataset(data, label):\n",
    "    N = data.shape[0]\n",
    "    index = np.random.permutation(N)\n",
    "    x = data[index, :, :]; y = label[index, :]\n",
    "    return x, y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_imgs = fetch_MNIST.load_train_images()\n",
    "    train_labs = fetch_MNIST.load_train_labels().astype(int)\n",
    "    # size of data;                  batch size\n",
    "    data_size = train_imgs.shape[0]; batch_sz = 64;\n",
    "    # learning rate; max iteration;    iter % mod (avoid index out of range)\n",
    "    lr = 0.01;     max_iter = 50000; iter_mod = int(data_size/batch_sz)\n",
    "    train_labs = convertToOneHot(train_labs)\n",
    "    my_CNN = LeNet(lr)\n",
    "    for iters in range(max_iter):\n",
    "        # starting index and ending index for input data\n",
    "        st_idx = (iters % iter_mod) * batch_sz\n",
    "        # shuffle the dataset\n",
    "        if st_idx == 0:\n",
    "            train_imgs, train_labs = shuffle_dataset(train_imgs, train_labs)\n",
    "        input_data = train_imgs[st_idx : st_idx + batch_sz]\n",
    "        output_label = train_labs[st_idx : st_idx + batch_sz]\n",
    "        softmax_output = my_CNN.forward_prop(input_data)\n",
    "        if iters % 50 == 0:\n",
    "            # calculate accuracy\n",
    "            correct_list = [ int(np.argmax(softmax_output[i])==np.argmax(output_label[i])) for i in range(batch_sz) ]\n",
    "            accuracy = float(np.array(correct_list).sum()) / batch_sz\n",
    "            # calculate loss\n",
    "            correct_prob = [ softmax_output[i][np.argmax(output_label[i])] for i in range(batch_sz) ]\n",
    "            correct_prob = filter(lambda x: x > 0, correct_prob)\n",
    "            loss = -1.0 * np.sum(np.log(correct_prob))\n",
    "            print(\"The %d iters result:\" % iters)\n",
    "            print(\"The accuracy is %f The loss is %f \" % (accuracy, loss))\n",
    "        my_CNN.backward_prop(softmax_output, output_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
